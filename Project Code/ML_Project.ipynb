{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgQqa8tGMtA0",
        "colab_type": "code",
        "outputId": "f4ff71b0-fdd6-40df-c7c4-18dee05d0ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/fer')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAu2l1Ht1_PA",
        "colab_type": "code",
        "outputId": "0af59afa-152a-4822-84a3-8ec3da1083f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# df=pd.read_csv(\"Train_Data.csv\")\n",
        "df['emotion'].hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f718acf8eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub_X7rV2Z2u9",
        "colab_type": "code",
        "outputId": "a7e163ef-13ca-4718-d7a9-fdeaa9c780b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def loaddataset(filename):\n",
        "  data = pd.read_csv(filename)\n",
        "  pixels = data['pixels'].tolist()\n",
        "  width, height = 48, 48\n",
        "  faces = []\n",
        "  for pixel_sequence in pixels:\n",
        "      face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "      face = np.asarray(face)#.reshape(width, height)\n",
        "  #     face = cv2.resize(face.astype('uint8'),(48,48))\n",
        "      faces.append(face.astype('float32'))\n",
        "  faces = np.asarray(faces)\n",
        "  faces = np.expand_dims(faces, -1)\n",
        "  emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
        "  return faces,emotions\n",
        "trainx,trainy = loaddataset('Train_Data.csv')\n",
        "testx,testy=loaddataset('Test_Data.csv')\n",
        "valx,valy=loaddataset('Validation_Data.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXyMIdLZoh5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# from keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PrLEBdKNaEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainx1=trainx.astype('float32') / 255\n",
        "mean, std = np.mean(trainx1), np.std(trainx1)\n",
        "train_X = np.asarray([(np.array(xi)-mean) for xi in trainx1])\n",
        "\n",
        "\n",
        "testx1=testx.astype('float32') / 255\n",
        "mean, std = np.mean(testx1), np.std(testx1)\n",
        "test_X = np.asarray([(np.array(xi)-mean) for xi in testx1])\n",
        "\n",
        "valx1=valx.astype('float32') / 255\n",
        "mean, std = np.mean(valx1), np.std(valx1)\n",
        "val_X = np.asarray([(np.array(xi)-mean) for xi in valx1])\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48)\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48, 1)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48,1)\n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48)  \n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48, 1) \n",
        "\n",
        "train_Y=trainy\n",
        "val_Y=valy\n",
        "test_Y=testy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w-ke_u2qBNl",
        "colab_type": "code",
        "outputId": "531350d1-44f9-4d4c-ac3c-ea6cec03ad6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3589, 48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYeaIPxV8ZgY",
        "colab_type": "text"
      },
      "source": [
        "My Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOsfwTtKlPu7",
        "colab_type": "code",
        "outputId": "3041faeb-9dc2-440b-963b-6fc20b1b19a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        }
      },
      "source": [
        "def model_generate():\n",
        "    img_rows, img_cols = 48, 48\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(64, 5, 5, border_mode='valid',\n",
        "                            input_shape=(48, 48,1)))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "    model.add(keras.layers.convolutional.ZeroPadding2D(padding=(2, 2), dim_ordering='th'))\n",
        "    model.add(MaxPooling2D(pool_size=(5, 5),strides=(2, 2)))\n",
        "      \n",
        "    model.add(keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='th')) \n",
        "    model.add(Convolution2D(64, 3, 3))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "    model.add(keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='th')) \n",
        "    model.add(Convolution2D(64, 3, 3))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "    model.add(keras.layers.convolutional.AveragePooling2D(pool_size=(3, 3),strides=(2, 2)))\n",
        "     \n",
        "    model.add(keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='th'))\n",
        "    model.add(Convolution2D(128, 3, 3))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "    model.add(keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='th'))\n",
        "    model.add(Convolution2D(128, 3, 3))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "     \n",
        "    model.add(keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='th'))\n",
        "    model.add(keras.layers.convolutional.AveragePooling2D(pool_size=(3, 3),strides=(2, 2)))\n",
        "     \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(4096))\n",
        "    model.add(keras.layers.advanced_activations.PReLU(init='zero', weights=None))\n",
        "    model.add(Dropout(0.2))\n",
        "     \n",
        "      \n",
        "    model.add(Dense(7))  \n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    ada = Adadelta(lr=0.1, rho=0.95, epsilon=1e-08)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=ada,\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = model_generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(48, 48, 1..., padding=\"valid\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D(padding=(2, 2), data_format=\"channels_first\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D(padding=(1, 1), data_format=\"channels_first\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D(padding=(1, 1), data_format=\"channels_first\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D(padding=(1, 1), data_format=\"channels_first\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D(padding=(1, 1), data_format=\"channels_first\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `ZeroPadding2D` call to the Keras 2 API: `ZeroPadding2D(padding=(1, 1), data_format=\"channels_first\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_34 (Conv2D)           (None, 44, 44, 64)        1664      \n",
            "_________________________________________________________________\n",
            "p_re_lu_8 (PReLU)            (None, 44, 44, 64)        123904    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPaddin (None, 44, 48, 68)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 20, 22, 68)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_8 (ZeroPaddin (None, 20, 24, 70)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 18, 22, 64)        40384     \n",
            "_________________________________________________________________\n",
            "p_re_lu_9 (PReLU)            (None, 18, 22, 64)        25344     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_9 (ZeroPaddin (None, 18, 24, 66)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 16, 22, 64)        38080     \n",
            "_________________________________________________________________\n",
            "p_re_lu_10 (PReLU)           (None, 16, 22, 64)        22528     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 7, 10, 64)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPaddi (None, 7, 12, 66)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 5, 10, 128)        76160     \n",
            "_________________________________________________________________\n",
            "p_re_lu_11 (PReLU)           (None, 5, 10, 128)        6400      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPaddi (None, 5, 12, 130)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 3, 10, 128)        149888    \n",
            "_________________________________________________________________\n",
            "p_re_lu_12 (PReLU)           (None, 3, 10, 128)        3840      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPaddi (None, 3, 12, 130)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 1, 5, 130)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 650)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4096)              2666496   \n",
            "_________________________________________________________________\n",
            "p_re_lu_13 (PReLU)           (None, 4096)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "p_re_lu_14 (PReLU)           (None, 4096)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 7)                 28679     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 19,972,871\n",
            "Trainable params: 19,972,871\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdysvHMY5XRA",
        "colab_type": "code",
        "outputId": "7c51f3ee-0e05-4a80-a076-1e3ce848032f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1873
        }
      },
      "source": [
        "img_rows, img_cols = 48, 48\n",
        "batch_size = 128\n",
        "nb_classes = 7\n",
        "nb_epoch = 50\n",
        "img_channels = 1\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "datagen.fit(train_X)\n",
        "\n",
        "train_generator=datagen.flow(train_X, train_Y,batch_size=batch_size)\n",
        "\n",
        "validation_generator = valid_datagen.flow(val_X,val_Y,batch_size=batch_size)\n",
        "\n",
        "test_generator = valid_datagen.flow(test_X,test_Y,batch_size=batch_size)\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    samples_per_epoch=train_X.shape[0],\n",
        "                    nb_epoch=nb_epoch,\n",
        "                    validation_data=(val_X, val_Y))\n",
        "\n",
        "train_loss, test_acc = model.evaluate_generator(generator=train_generator,steps=batch_size)\n",
        "print(\"Train Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "\n",
        "train_loss, test_acc = model.evaluate_generator(generator=validation_generator,steps=batch_size)\n",
        "print(\"Validation Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "\n",
        "train_loss, test_acc = model.evaluate_generator(generator=test_generator,steps=batch_size)\n",
        "print(\"Testing Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Testing Loss: \" + repr(train_loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., steps_per_epoch=224, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "224/224 [==============================] - 28s 124ms/step - loss: 1.7442 - acc: 0.2828 - val_loss: 1.7315 - val_acc: 0.2990\n",
            "Epoch 2/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7445 - acc: 0.2810 - val_loss: 1.7083 - val_acc: 0.3093\n",
            "Epoch 3/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7404 - acc: 0.2848 - val_loss: 1.7146 - val_acc: 0.3096\n",
            "Epoch 4/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7370 - acc: 0.2846 - val_loss: 1.7034 - val_acc: 0.3221\n",
            "Epoch 5/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7357 - acc: 0.2909 - val_loss: 1.6838 - val_acc: 0.3313\n",
            "Epoch 6/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.7289 - acc: 0.2938 - val_loss: 1.7333 - val_acc: 0.3015\n",
            "Epoch 7/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.7276 - acc: 0.2947 - val_loss: 1.6804 - val_acc: 0.3330\n",
            "Epoch 8/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.7218 - acc: 0.2964 - val_loss: 1.7112 - val_acc: 0.3179\n",
            "Epoch 9/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7186 - acc: 0.2989 - val_loss: 1.6674 - val_acc: 0.3435\n",
            "Epoch 10/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.7169 - acc: 0.2987 - val_loss: 1.6820 - val_acc: 0.3355\n",
            "Epoch 11/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7107 - acc: 0.3021 - val_loss: 1.6969 - val_acc: 0.3260\n",
            "Epoch 12/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.7079 - acc: 0.3071 - val_loss: 1.6655 - val_acc: 0.3383\n",
            "Epoch 13/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.7066 - acc: 0.3076 - val_loss: 1.7359 - val_acc: 0.2940\n",
            "Epoch 14/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6976 - acc: 0.3086 - val_loss: 1.6590 - val_acc: 0.3477\n",
            "Epoch 15/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6992 - acc: 0.3115 - val_loss: 1.6629 - val_acc: 0.3419\n",
            "Epoch 16/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6949 - acc: 0.3128 - val_loss: 1.6569 - val_acc: 0.3396\n",
            "Epoch 17/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6884 - acc: 0.3174 - val_loss: 1.6524 - val_acc: 0.3500\n",
            "Epoch 18/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6906 - acc: 0.3157 - val_loss: 1.6623 - val_acc: 0.3455\n",
            "Epoch 19/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6819 - acc: 0.3196 - val_loss: 1.6559 - val_acc: 0.3463\n",
            "Epoch 20/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6788 - acc: 0.3236 - val_loss: 1.6386 - val_acc: 0.3458\n",
            "Epoch 21/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6794 - acc: 0.3225 - val_loss: 1.6592 - val_acc: 0.3383\n",
            "Epoch 22/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6666 - acc: 0.3266 - val_loss: 1.6488 - val_acc: 0.3441\n",
            "Epoch 23/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6726 - acc: 0.3270 - val_loss: 1.6245 - val_acc: 0.3678\n",
            "Epoch 24/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6594 - acc: 0.3354 - val_loss: 1.6252 - val_acc: 0.3572\n",
            "Epoch 25/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6579 - acc: 0.3351 - val_loss: 1.6232 - val_acc: 0.3670\n",
            "Epoch 26/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6574 - acc: 0.3350 - val_loss: 1.6401 - val_acc: 0.3553\n",
            "Epoch 27/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6548 - acc: 0.3381 - val_loss: 1.5795 - val_acc: 0.3764\n",
            "Epoch 28/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6459 - acc: 0.3407 - val_loss: 1.6008 - val_acc: 0.3748\n",
            "Epoch 29/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6470 - acc: 0.3413 - val_loss: 1.6020 - val_acc: 0.3656\n",
            "Epoch 30/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6410 - acc: 0.3468 - val_loss: 1.6375 - val_acc: 0.3644\n",
            "Epoch 31/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6408 - acc: 0.3449 - val_loss: 1.6018 - val_acc: 0.3803\n",
            "Epoch 32/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6334 - acc: 0.3528 - val_loss: 1.6010 - val_acc: 0.3695\n",
            "Epoch 33/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6339 - acc: 0.3499 - val_loss: 1.5573 - val_acc: 0.3957\n",
            "Epoch 34/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6307 - acc: 0.3543 - val_loss: 1.5739 - val_acc: 0.3787\n",
            "Epoch 35/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6221 - acc: 0.3563 - val_loss: 1.5849 - val_acc: 0.3901\n",
            "Epoch 36/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6275 - acc: 0.3540 - val_loss: 1.5859 - val_acc: 0.3881\n",
            "Epoch 37/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6172 - acc: 0.3596 - val_loss: 1.5513 - val_acc: 0.4029\n",
            "Epoch 38/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6178 - acc: 0.3591 - val_loss: 1.5381 - val_acc: 0.3901\n",
            "Epoch 39/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.6084 - acc: 0.3635 - val_loss: 1.5315 - val_acc: 0.4104\n",
            "Epoch 40/50\n",
            "224/224 [==============================] - 28s 123ms/step - loss: 1.6122 - acc: 0.3619 - val_loss: 1.6173 - val_acc: 0.3728\n",
            "Epoch 41/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.6017 - acc: 0.3675 - val_loss: 1.5360 - val_acc: 0.4099\n",
            "Epoch 42/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.5994 - acc: 0.3674 - val_loss: 1.5816 - val_acc: 0.3848\n",
            "Epoch 43/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.5974 - acc: 0.3693 - val_loss: 1.5253 - val_acc: 0.4071\n",
            "Epoch 44/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.5902 - acc: 0.3760 - val_loss: 1.5140 - val_acc: 0.4018\n",
            "Epoch 45/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.5904 - acc: 0.3728 - val_loss: 1.6070 - val_acc: 0.3742\n",
            "Epoch 46/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.5808 - acc: 0.3791 - val_loss: 1.5515 - val_acc: 0.4046\n",
            "Epoch 47/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.5827 - acc: 0.3764 - val_loss: 1.5056 - val_acc: 0.4154\n",
            "Epoch 48/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.5784 - acc: 0.3800 - val_loss: 1.5138 - val_acc: 0.4129\n",
            "Epoch 49/50\n",
            "224/224 [==============================] - 27s 122ms/step - loss: 1.5789 - acc: 0.3810 - val_loss: 1.4894 - val_acc: 0.4277\n",
            "Epoch 50/50\n",
            "224/224 [==============================] - 27s 123ms/step - loss: 1.5700 - acc: 0.3849 - val_loss: 1.4927 - val_acc: 0.4163\n",
            "Train Accuracy: 38.14521573919738%\n",
            "Train Loss: 1.5818835244114093\n",
            "Validation Accuracy: 18.229297759972948%\n",
            "Validation Loss: 2.0180536673361846\n",
            "Testing Accuracy: 16.53032972592942%\n",
            "Testing Loss: 2.024392704549017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnQc_d50pGq-",
        "colab_type": "code",
        "outputId": "eda79110-3be2-480f-cb1a-c4bfca998baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1887
        }
      },
      "source": [
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# gen = ImageDataGenerator()\n",
        "# train_generator = gen.flow(train_X, train_Y, batch_size=32)\n",
        "# model.fit_generator(train_generator, steps_per_epoch=32, epochs=1)\n",
        "\n",
        "history=model.fit(train_X, train_Y, epochs=50, batch_size=128,validation_data=(val_X, val_Y))\n",
        "train_loss, test_acc = model.evaluate(train_X, train_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "train_loss, test_acc = model.evaluate(val_X, val_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "test_loss, test_acc = model.evaluate(test_X, test_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Test Loss: \" + repr(test_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/50\n",
            "28709/28709 [==============================] - 27s 956us/step - loss: 1.4414 - acc: 0.4443 - val_loss: 1.4339 - val_acc: 0.4425\n",
            "Epoch 2/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.4139 - acc: 0.4590 - val_loss: 1.4294 - val_acc: 0.4494\n",
            "Epoch 3/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.3974 - acc: 0.4652 - val_loss: 1.4591 - val_acc: 0.4402\n",
            "Epoch 4/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.3845 - acc: 0.4704 - val_loss: 1.4254 - val_acc: 0.4578\n",
            "Epoch 5/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.3699 - acc: 0.4772 - val_loss: 1.4201 - val_acc: 0.4636\n",
            "Epoch 6/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.3591 - acc: 0.4818 - val_loss: 1.3684 - val_acc: 0.4792\n",
            "Epoch 7/50\n",
            "28709/28709 [==============================] - 27s 957us/step - loss: 1.3473 - acc: 0.4864 - val_loss: 1.6007 - val_acc: 0.4143\n",
            "Epoch 8/50\n",
            "28709/28709 [==============================] - 27s 956us/step - loss: 1.3343 - acc: 0.4925 - val_loss: 1.4058 - val_acc: 0.4620\n",
            "Epoch 9/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.3300 - acc: 0.4938 - val_loss: 1.3722 - val_acc: 0.4792\n",
            "Epoch 10/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.3200 - acc: 0.4992 - val_loss: 1.3428 - val_acc: 0.4921\n",
            "Epoch 11/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.3099 - acc: 0.5021 - val_loss: 1.3935 - val_acc: 0.4692\n",
            "Epoch 12/50\n",
            "28709/28709 [==============================] - 27s 951us/step - loss: 1.3041 - acc: 0.5068 - val_loss: 1.3332 - val_acc: 0.4901\n",
            "Epoch 13/50\n",
            "28709/28709 [==============================] - 27s 952us/step - loss: 1.2954 - acc: 0.5077 - val_loss: 1.3729 - val_acc: 0.4776\n",
            "Epoch 14/50\n",
            "28709/28709 [==============================] - 27s 951us/step - loss: 1.2884 - acc: 0.5130 - val_loss: 1.3256 - val_acc: 0.4954\n",
            "Epoch 15/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.2818 - acc: 0.5150 - val_loss: 1.3945 - val_acc: 0.4583\n",
            "Epoch 16/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.2736 - acc: 0.5189 - val_loss: 1.3453 - val_acc: 0.4948\n",
            "Epoch 17/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.2659 - acc: 0.5205 - val_loss: 1.2981 - val_acc: 0.5102\n",
            "Epoch 18/50\n",
            "28709/28709 [==============================] - 27s 950us/step - loss: 1.2628 - acc: 0.5224 - val_loss: 1.3061 - val_acc: 0.5049\n",
            "Epoch 19/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.2522 - acc: 0.5243 - val_loss: 1.3606 - val_acc: 0.4843\n",
            "Epoch 20/50\n",
            "28709/28709 [==============================] - 27s 952us/step - loss: 1.2471 - acc: 0.5277 - val_loss: 1.2870 - val_acc: 0.5110\n",
            "Epoch 21/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.2412 - acc: 0.5320 - val_loss: 1.3481 - val_acc: 0.4935\n",
            "Epoch 22/50\n",
            "28709/28709 [==============================] - 27s 958us/step - loss: 1.2349 - acc: 0.5333 - val_loss: 1.2907 - val_acc: 0.5118\n",
            "Epoch 23/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.2276 - acc: 0.5365 - val_loss: 1.3406 - val_acc: 0.4960\n",
            "Epoch 24/50\n",
            "28709/28709 [==============================] - 27s 951us/step - loss: 1.2234 - acc: 0.5366 - val_loss: 1.3020 - val_acc: 0.5088\n",
            "Epoch 25/50\n",
            "28709/28709 [==============================] - 27s 949us/step - loss: 1.2156 - acc: 0.5409 - val_loss: 1.3247 - val_acc: 0.4999\n",
            "Epoch 26/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.2137 - acc: 0.5409 - val_loss: 1.3041 - val_acc: 0.5060\n",
            "Epoch 27/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.2051 - acc: 0.5463 - val_loss: 1.2740 - val_acc: 0.5163\n",
            "Epoch 28/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.1975 - acc: 0.5482 - val_loss: 1.2519 - val_acc: 0.5286\n",
            "Epoch 29/50\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 1.1888 - acc: 0.5532 - val_loss: 1.2675 - val_acc: 0.5277\n",
            "Epoch 30/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.1856 - acc: 0.5542 - val_loss: 1.2396 - val_acc: 0.5344\n",
            "Epoch 31/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.1831 - acc: 0.5525 - val_loss: 1.2859 - val_acc: 0.5183\n",
            "Epoch 32/50\n",
            "28709/28709 [==============================] - 27s 956us/step - loss: 1.1775 - acc: 0.5562 - val_loss: 1.2750 - val_acc: 0.5208\n",
            "Epoch 33/50\n",
            "28709/28709 [==============================] - 27s 952us/step - loss: 1.1708 - acc: 0.5580 - val_loss: 1.2524 - val_acc: 0.5341\n",
            "Epoch 34/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.1660 - acc: 0.5611 - val_loss: 1.2302 - val_acc: 0.5433\n",
            "Epoch 35/50\n",
            "28709/28709 [==============================] - 27s 957us/step - loss: 1.1566 - acc: 0.5638 - val_loss: 1.2722 - val_acc: 0.5277\n",
            "Epoch 36/50\n",
            "28709/28709 [==============================] - 27s 957us/step - loss: 1.1546 - acc: 0.5648 - val_loss: 1.4222 - val_acc: 0.4765\n",
            "Epoch 37/50\n",
            "28709/28709 [==============================] - 27s 951us/step - loss: 1.1461 - acc: 0.5704 - val_loss: 1.2699 - val_acc: 0.5199\n",
            "Epoch 38/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.1405 - acc: 0.5702 - val_loss: 1.2617 - val_acc: 0.5286\n",
            "Epoch 39/50\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.1389 - acc: 0.5693 - val_loss: 1.2374 - val_acc: 0.5339\n",
            "Epoch 40/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.1322 - acc: 0.5729 - val_loss: 1.2246 - val_acc: 0.5450\n",
            "Epoch 41/50\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 1.1273 - acc: 0.5738 - val_loss: 1.2438 - val_acc: 0.5411\n",
            "Epoch 42/50\n",
            "28709/28709 [==============================] - 27s 950us/step - loss: 1.1202 - acc: 0.5792 - val_loss: 1.2204 - val_acc: 0.5391\n",
            "Epoch 43/50\n",
            "28709/28709 [==============================] - 27s 947us/step - loss: 1.1170 - acc: 0.5802 - val_loss: 1.2177 - val_acc: 0.5386\n",
            "Epoch 44/50\n",
            "28709/28709 [==============================] - 27s 947us/step - loss: 1.1083 - acc: 0.5829 - val_loss: 1.2629 - val_acc: 0.5425\n",
            "Epoch 45/50\n",
            "28709/28709 [==============================] - 27s 949us/step - loss: 1.1083 - acc: 0.5813 - val_loss: 1.2763 - val_acc: 0.5235\n",
            "Epoch 46/50\n",
            "28709/28709 [==============================] - 27s 949us/step - loss: 1.1015 - acc: 0.5850 - val_loss: 1.2510 - val_acc: 0.5327\n",
            "Epoch 47/50\n",
            "28709/28709 [==============================] - 27s 950us/step - loss: 1.0956 - acc: 0.5864 - val_loss: 1.3468 - val_acc: 0.5138\n",
            "Epoch 48/50\n",
            "28709/28709 [==============================] - 27s 951us/step - loss: 1.0901 - acc: 0.5904 - val_loss: 1.2180 - val_acc: 0.5506\n",
            "Epoch 49/50\n",
            "28709/28709 [==============================] - 27s 948us/step - loss: 1.0863 - acc: 0.5912 - val_loss: 1.3105 - val_acc: 0.5177\n",
            "Epoch 50/50\n",
            "28709/28709 [==============================] - 27s 947us/step - loss: 1.0792 - acc: 0.5917 - val_loss: 1.2688 - val_acc: 0.5308\n",
            "28709/28709 [==============================] - 10s 331us/step\n",
            "Accuracy: 57.43495071242398%\n",
            "Train Loss: 1.1198330312617466\n",
            "3589/3589 [==============================] - 1s 332us/step\n",
            "Accuracy: 53.0788520483394%\n",
            "Validation Loss: 1.2687797554358362\n",
            "3589/3589 [==============================] - 1s 336us/step\n",
            "Accuracy: 53.524658682619375%\n",
            "Test Loss: 1.2113120333668115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goZsKKkIPs4C",
        "colab_type": "code",
        "outputId": "ffa169de-337e-484b-ee40-81cb7476cb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UqShvLMn_t5",
        "colab_type": "text"
      },
      "source": [
        "New Try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ASBz_XBnmU3",
        "colab_type": "code",
        "outputId": "4ea57ee6-c1b1-4a20-d186-0cb96dc8b38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3435
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "number_of_classes = 7\n",
        "dimension = 48\n",
        "number_of_channels = 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(48, 48 ,1), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 50\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "adam = Adam(decay=decay)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "history=model.fit(train_X, train_Y, epochs=epochs, batch_size=128,validation_data=(val_X, val_Y))\n",
        "train_loss, test_acc = model.evaluate(train_X, train_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "train_loss, test_acc = model.evaluate(val_X, val_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "test_loss, test_acc = model.evaluate(test_X, test_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Test Loss: \" + repr(test_loss))\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_49 (Conv2D)           (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4096)              18878464  \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 7)                 28679     \n",
            "=================================================================\n",
            "Total params: 26,545,095\n",
            "Trainable params: 26,543,175\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/50\n",
            "28709/28709 [==============================] - 57s 2ms/step - loss: 3.2621 - acc: 0.2213 - val_loss: 2.5913 - val_acc: 0.1658\n",
            "Epoch 2/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.8002 - acc: 0.2506 - val_loss: 1.7858 - val_acc: 0.2547\n",
            "Epoch 3/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.7821 - acc: 0.2610 - val_loss: 1.7588 - val_acc: 0.2622\n",
            "Epoch 4/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.7533 - acc: 0.2795 - val_loss: 1.7277 - val_acc: 0.3048\n",
            "Epoch 5/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.6906 - acc: 0.3218 - val_loss: 1.6333 - val_acc: 0.3578\n",
            "Epoch 6/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.5984 - acc: 0.3713 - val_loss: 1.5755 - val_acc: 0.3901\n",
            "Epoch 7/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.5038 - acc: 0.4147 - val_loss: 1.5076 - val_acc: 0.4291\n",
            "Epoch 8/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.4203 - acc: 0.4478 - val_loss: 1.4035 - val_acc: 0.4533\n",
            "Epoch 9/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.3393 - acc: 0.4813 - val_loss: 1.3869 - val_acc: 0.4547\n",
            "Epoch 10/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.2630 - acc: 0.5126 - val_loss: 1.3549 - val_acc: 0.5021\n",
            "Epoch 11/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.2006 - acc: 0.5378 - val_loss: 1.3056 - val_acc: 0.5219\n",
            "Epoch 12/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1462 - acc: 0.5611 - val_loss: 1.1911 - val_acc: 0.5539\n",
            "Epoch 13/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1003 - acc: 0.5782 - val_loss: 1.2920 - val_acc: 0.5107\n",
            "Epoch 14/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.0606 - acc: 0.5952 - val_loss: 1.1547 - val_acc: 0.5567\n",
            "Epoch 15/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.0236 - acc: 0.6142 - val_loss: 1.2101 - val_acc: 0.5737\n",
            "Epoch 16/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9695 - acc: 0.6316 - val_loss: 1.1756 - val_acc: 0.5860\n",
            "Epoch 17/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9271 - acc: 0.6488 - val_loss: 1.1820 - val_acc: 0.5740\n",
            "Epoch 18/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8867 - acc: 0.6655 - val_loss: 1.1360 - val_acc: 0.5957\n",
            "Epoch 19/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8384 - acc: 0.6817 - val_loss: 1.0835 - val_acc: 0.5982\n",
            "Epoch 20/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7901 - acc: 0.6998 - val_loss: 1.1343 - val_acc: 0.5999\n",
            "Epoch 21/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7545 - acc: 0.7149 - val_loss: 1.3457 - val_acc: 0.5433\n",
            "Epoch 22/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7015 - acc: 0.7367 - val_loss: 1.1600 - val_acc: 0.6155\n",
            "Epoch 23/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6600 - acc: 0.7512 - val_loss: 1.1855 - val_acc: 0.6174\n",
            "Epoch 24/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6037 - acc: 0.7701 - val_loss: 1.2347 - val_acc: 0.6266\n",
            "Epoch 25/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5602 - acc: 0.7893 - val_loss: 1.2154 - val_acc: 0.6194\n",
            "Epoch 26/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5006 - acc: 0.8128 - val_loss: 1.2413 - val_acc: 0.6147\n",
            "Epoch 27/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4607 - acc: 0.8282 - val_loss: 1.3407 - val_acc: 0.6344\n",
            "Epoch 28/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4170 - acc: 0.8439 - val_loss: 1.4169 - val_acc: 0.6350\n",
            "Epoch 29/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3795 - acc: 0.8594 - val_loss: 1.3614 - val_acc: 0.6339\n",
            "Epoch 30/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3398 - acc: 0.8747 - val_loss: 1.5939 - val_acc: 0.6420\n",
            "Epoch 31/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3090 - acc: 0.8862 - val_loss: 1.5486 - val_acc: 0.6333\n",
            "Epoch 32/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2804 - acc: 0.8985 - val_loss: 1.6384 - val_acc: 0.6297\n",
            "Epoch 33/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2422 - acc: 0.9134 - val_loss: 1.7207 - val_acc: 0.6264\n",
            "Epoch 34/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2228 - acc: 0.9201 - val_loss: 1.7133 - val_acc: 0.6467\n",
            "Epoch 35/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2113 - acc: 0.9245 - val_loss: 1.8682 - val_acc: 0.6386\n",
            "Epoch 36/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1933 - acc: 0.9294 - val_loss: 1.8631 - val_acc: 0.6372\n",
            "Epoch 37/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1749 - acc: 0.9393 - val_loss: 1.7877 - val_acc: 0.6369\n",
            "Epoch 38/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1554 - acc: 0.9460 - val_loss: 1.9673 - val_acc: 0.6475\n",
            "Epoch 39/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1433 - acc: 0.9503 - val_loss: 1.9811 - val_acc: 0.6400\n",
            "Epoch 40/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1426 - acc: 0.9503 - val_loss: 2.0564 - val_acc: 0.6155\n",
            "Epoch 41/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1283 - acc: 0.9558 - val_loss: 2.0032 - val_acc: 0.6375\n",
            "Epoch 42/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1228 - acc: 0.9587 - val_loss: 1.9825 - val_acc: 0.6347\n",
            "Epoch 43/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1154 - acc: 0.9606 - val_loss: 2.0841 - val_acc: 0.6356\n",
            "Epoch 44/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1013 - acc: 0.9649 - val_loss: 2.1196 - val_acc: 0.6383\n",
            "Epoch 45/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0997 - acc: 0.9655 - val_loss: 2.1149 - val_acc: 0.6420\n",
            "Epoch 46/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0955 - acc: 0.9660 - val_loss: 2.1802 - val_acc: 0.6400\n",
            "Epoch 47/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0918 - acc: 0.9689 - val_loss: 2.1248 - val_acc: 0.6372\n",
            "Epoch 48/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0856 - acc: 0.9691 - val_loss: 2.2323 - val_acc: 0.6381\n",
            "Epoch 49/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0870 - acc: 0.9701 - val_loss: 2.2599 - val_acc: 0.6517\n",
            "Epoch 50/50\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0815 - acc: 0.9725 - val_loss: 2.2735 - val_acc: 0.6417\n",
            "28709/28709 [==============================] - 22s 755us/step\n",
            "Accuracy: 99.63077780486955%\n",
            "Train Loss: 0.011544558205509328\n",
            "3589/3589 [==============================] - 3s 756us/step\n",
            "Accuracy: 64.16829200417394%\n",
            "Validation Loss: 2.2735254429083542\n",
            "3589/3589 [==============================] - 3s 753us/step\n",
            "Accuracy: 65.92365561769878%\n",
            "Test Loss: 2.102663086961325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1tRoWkpeDbH",
        "colab_type": "code",
        "outputId": "d61027eb-f18e-48f1-9fb7-e5741673b2d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5135
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "number_of_classes = 7\n",
        "dimension = 48\n",
        "number_of_channels = 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(48, 48 ,1), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 100\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "adam = Adam(decay=decay)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "history=model.fit(train_X, train_Y, epochs=epochs, batch_size=128,validation_data=(val_X, val_Y))\n",
        "train_loss, test_acc = model.evaluate(train_X, train_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "train_loss, test_acc = model.evaluate(val_X, val_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "test_loss, test_acc = model.evaluate(test_X, test_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Test Loss: \" + repr(test_loss))\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_59 (Conv2D)           (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_66 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 4096)              18878464  \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 7)                 28679     \n",
            "=================================================================\n",
            "Total params: 26,545,095\n",
            "Trainable params: 26,543,175\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/100\n",
            "28709/28709 [==============================] - 57s 2ms/step - loss: 4.4185 - acc: 0.2086 - val_loss: 12.7605 - val_acc: 0.1691\n",
            "Epoch 2/100\n",
            "28709/28709 [==============================] - 53s 2ms/step - loss: 1.7960 - acc: 0.2504 - val_loss: 1.7810 - val_acc: 0.2533\n",
            "Epoch 3/100\n",
            "28709/28709 [==============================] - 53s 2ms/step - loss: 1.7366 - acc: 0.2942 - val_loss: 1.7911 - val_acc: 0.3104\n",
            "Epoch 4/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.6304 - acc: 0.3563 - val_loss: 1.6818 - val_acc: 0.3383\n",
            "Epoch 5/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.5174 - acc: 0.4033 - val_loss: 1.4477 - val_acc: 0.4400\n",
            "Epoch 6/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.3894 - acc: 0.4598 - val_loss: 1.7056 - val_acc: 0.4430\n",
            "Epoch 7/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.3119 - acc: 0.4923 - val_loss: 1.4931 - val_acc: 0.4798\n",
            "Epoch 8/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.2435 - acc: 0.5186 - val_loss: 1.3264 - val_acc: 0.5102\n",
            "Epoch 9/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1892 - acc: 0.5474 - val_loss: 1.3133 - val_acc: 0.5166\n",
            "Epoch 10/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1377 - acc: 0.5701 - val_loss: 1.6925 - val_acc: 0.5308\n",
            "Epoch 11/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1071 - acc: 0.5813 - val_loss: 1.1705 - val_acc: 0.5648\n",
            "Epoch 12/100\n",
            "28709/28709 [==============================] - 53s 2ms/step - loss: 1.0556 - acc: 0.5986 - val_loss: 1.1691 - val_acc: 0.5723\n",
            "Epoch 13/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.0235 - acc: 0.6119 - val_loss: 1.1669 - val_acc: 0.5692\n",
            "Epoch 14/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9877 - acc: 0.6265 - val_loss: 1.0947 - val_acc: 0.5968\n",
            "Epoch 15/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9534 - acc: 0.6393 - val_loss: 1.1236 - val_acc: 0.5832\n",
            "Epoch 16/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9163 - acc: 0.6559 - val_loss: 1.0570 - val_acc: 0.6113\n",
            "Epoch 17/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8858 - acc: 0.6653 - val_loss: 1.2099 - val_acc: 0.6041\n",
            "Epoch 18/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8590 - acc: 0.6790 - val_loss: 1.1235 - val_acc: 0.6030\n",
            "Epoch 19/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8339 - acc: 0.6863 - val_loss: 1.1064 - val_acc: 0.6191\n",
            "Epoch 20/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7840 - acc: 0.7046 - val_loss: 1.0823 - val_acc: 0.6110\n",
            "Epoch 21/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7470 - acc: 0.7199 - val_loss: 1.0909 - val_acc: 0.6239\n",
            "Epoch 22/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7081 - acc: 0.7337 - val_loss: 1.1361 - val_acc: 0.6269\n",
            "Epoch 23/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6790 - acc: 0.7466 - val_loss: 1.1426 - val_acc: 0.6236\n",
            "Epoch 24/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6538 - acc: 0.7555 - val_loss: 1.1515 - val_acc: 0.6361\n",
            "Epoch 25/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6198 - acc: 0.7657 - val_loss: 1.3011 - val_acc: 0.6219\n",
            "Epoch 26/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5782 - acc: 0.7831 - val_loss: 1.1255 - val_acc: 0.6230\n",
            "Epoch 27/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5456 - acc: 0.7971 - val_loss: 1.2413 - val_acc: 0.6269\n",
            "Epoch 28/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5199 - acc: 0.8041 - val_loss: 1.1974 - val_acc: 0.6464\n",
            "Epoch 29/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4868 - acc: 0.8201 - val_loss: 1.4663 - val_acc: 0.6191\n",
            "Epoch 30/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4500 - acc: 0.8342 - val_loss: 1.3937 - val_acc: 0.6325\n",
            "Epoch 31/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4407 - acc: 0.8373 - val_loss: 1.2657 - val_acc: 0.6464\n",
            "Epoch 32/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3967 - acc: 0.8523 - val_loss: 1.3333 - val_acc: 0.6305\n",
            "Epoch 33/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3633 - acc: 0.8659 - val_loss: 1.5093 - val_acc: 0.6431\n",
            "Epoch 34/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3419 - acc: 0.8753 - val_loss: 1.4830 - val_acc: 0.6422\n",
            "Epoch 35/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3194 - acc: 0.8823 - val_loss: 1.5315 - val_acc: 0.6414\n",
            "Epoch 36/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2991 - acc: 0.8929 - val_loss: 1.6440 - val_acc: 0.6350\n",
            "Epoch 37/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2825 - acc: 0.8954 - val_loss: 1.5530 - val_acc: 0.6514\n",
            "Epoch 38/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2613 - acc: 0.9067 - val_loss: 1.6486 - val_acc: 0.6358\n",
            "Epoch 39/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2443 - acc: 0.9118 - val_loss: 1.7737 - val_acc: 0.6395\n",
            "Epoch 40/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2414 - acc: 0.9119 - val_loss: 1.6499 - val_acc: 0.6450\n",
            "Epoch 41/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2093 - acc: 0.9254 - val_loss: 1.9799 - val_acc: 0.6250\n",
            "Epoch 42/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2063 - acc: 0.9263 - val_loss: 1.7851 - val_acc: 0.6417\n",
            "Epoch 43/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1959 - acc: 0.9307 - val_loss: 1.7094 - val_acc: 0.6428\n",
            "Epoch 44/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1816 - acc: 0.9346 - val_loss: 1.8608 - val_acc: 0.6431\n",
            "Epoch 45/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1755 - acc: 0.9389 - val_loss: 1.8678 - val_acc: 0.6417\n",
            "Epoch 46/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1603 - acc: 0.9425 - val_loss: 1.7766 - val_acc: 0.6459\n",
            "Epoch 47/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1542 - acc: 0.9454 - val_loss: 2.1186 - val_acc: 0.6375\n",
            "Epoch 48/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1572 - acc: 0.9451 - val_loss: 1.8529 - val_acc: 0.6489\n",
            "Epoch 49/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1402 - acc: 0.9519 - val_loss: 1.9366 - val_acc: 0.6439\n",
            "Epoch 50/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1451 - acc: 0.9499 - val_loss: 2.0437 - val_acc: 0.6447\n",
            "Epoch 51/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1376 - acc: 0.9532 - val_loss: 1.9094 - val_acc: 0.6492\n",
            "Epoch 52/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1229 - acc: 0.9574 - val_loss: 2.1703 - val_acc: 0.6358\n",
            "Epoch 53/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1453 - acc: 0.9513 - val_loss: 2.0257 - val_acc: 0.6420\n",
            "Epoch 54/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1111 - acc: 0.9626 - val_loss: 2.2086 - val_acc: 0.6484\n",
            "Epoch 55/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1116 - acc: 0.9605 - val_loss: 2.0226 - val_acc: 0.6436\n",
            "Epoch 56/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1466 - acc: 0.9508 - val_loss: 2.0162 - val_acc: 0.6537\n",
            "Epoch 57/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1005 - acc: 0.9654 - val_loss: 2.1198 - val_acc: 0.6523\n",
            "Epoch 58/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1050 - acc: 0.9639 - val_loss: 2.1532 - val_acc: 0.6464\n",
            "Epoch 59/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0984 - acc: 0.9668 - val_loss: 1.9516 - val_acc: 0.6478\n",
            "Epoch 60/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0903 - acc: 0.9696 - val_loss: 2.0441 - val_acc: 0.6500\n",
            "Epoch 61/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0867 - acc: 0.9698 - val_loss: 2.1673 - val_acc: 0.6489\n",
            "Epoch 62/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0848 - acc: 0.9700 - val_loss: 2.2493 - val_acc: 0.6403\n",
            "Epoch 63/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0875 - acc: 0.9698 - val_loss: 2.0934 - val_acc: 0.6537\n",
            "Epoch 64/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0800 - acc: 0.9717 - val_loss: 2.2108 - val_acc: 0.6495\n",
            "Epoch 65/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0859 - acc: 0.9703 - val_loss: 2.2756 - val_acc: 0.6539\n",
            "Epoch 66/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0734 - acc: 0.9754 - val_loss: 2.2686 - val_acc: 0.6461\n",
            "Epoch 67/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0773 - acc: 0.9743 - val_loss: 2.2560 - val_acc: 0.6576\n",
            "Epoch 68/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0786 - acc: 0.9726 - val_loss: 2.2102 - val_acc: 0.6648\n",
            "Epoch 69/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0719 - acc: 0.9757 - val_loss: 2.3103 - val_acc: 0.6640\n",
            "Epoch 70/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0670 - acc: 0.9772 - val_loss: 2.4153 - val_acc: 0.6425\n",
            "Epoch 71/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0703 - acc: 0.9759 - val_loss: 2.3461 - val_acc: 0.6509\n",
            "Epoch 72/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0692 - acc: 0.9759 - val_loss: 2.2840 - val_acc: 0.6528\n",
            "Epoch 73/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0656 - acc: 0.9777 - val_loss: 2.2941 - val_acc: 0.6534\n",
            "Epoch 74/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0695 - acc: 0.9767 - val_loss: 2.1249 - val_acc: 0.6548\n",
            "Epoch 75/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0666 - acc: 0.9777 - val_loss: 2.2031 - val_acc: 0.6514\n",
            "Epoch 76/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0655 - acc: 0.9775 - val_loss: 2.3447 - val_acc: 0.6509\n",
            "Epoch 77/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0629 - acc: 0.9781 - val_loss: 2.3000 - val_acc: 0.6503\n",
            "Epoch 78/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0650 - acc: 0.9784 - val_loss: 2.3234 - val_acc: 0.6537\n",
            "Epoch 79/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0609 - acc: 0.9792 - val_loss: 2.3304 - val_acc: 0.6548\n",
            "Epoch 80/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0522 - acc: 0.9825 - val_loss: 2.4815 - val_acc: 0.6445\n",
            "Epoch 81/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0551 - acc: 0.9814 - val_loss: 2.3678 - val_acc: 0.6408\n",
            "Epoch 82/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0542 - acc: 0.9811 - val_loss: 2.4041 - val_acc: 0.6495\n",
            "Epoch 83/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0679 - acc: 0.9765 - val_loss: 2.4555 - val_acc: 0.6565\n",
            "Epoch 84/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0531 - acc: 0.9809 - val_loss: 2.3506 - val_acc: 0.6414\n",
            "Epoch 85/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0501 - acc: 0.9828 - val_loss: 2.4161 - val_acc: 0.6339\n",
            "Epoch 86/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0496 - acc: 0.9821 - val_loss: 2.4753 - val_acc: 0.6551\n",
            "Epoch 87/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0572 - acc: 0.9806 - val_loss: 2.4373 - val_acc: 0.6567\n",
            "Epoch 88/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0506 - acc: 0.9821 - val_loss: 2.4897 - val_acc: 0.6489\n",
            "Epoch 89/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0496 - acc: 0.9831 - val_loss: 2.4615 - val_acc: 0.6400\n",
            "Epoch 90/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0474 - acc: 0.9847 - val_loss: 2.4942 - val_acc: 0.6475\n",
            "Epoch 91/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0469 - acc: 0.9840 - val_loss: 2.3614 - val_acc: 0.6484\n",
            "Epoch 92/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0452 - acc: 0.9839 - val_loss: 2.4405 - val_acc: 0.6551\n",
            "Epoch 93/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0423 - acc: 0.9852 - val_loss: 2.4866 - val_acc: 0.6506\n",
            "Epoch 94/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0452 - acc: 0.9839 - val_loss: 2.3591 - val_acc: 0.6445\n",
            "Epoch 95/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0494 - acc: 0.9829 - val_loss: 2.4245 - val_acc: 0.6484\n",
            "Epoch 96/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0436 - acc: 0.9852 - val_loss: 2.5872 - val_acc: 0.6486\n",
            "Epoch 97/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0413 - acc: 0.9856 - val_loss: 2.4922 - val_acc: 0.6473\n",
            "Epoch 98/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0406 - acc: 0.9864 - val_loss: 2.5686 - val_acc: 0.6375\n",
            "Epoch 99/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0485 - acc: 0.9825 - val_loss: 2.4969 - val_acc: 0.6570\n",
            "Epoch 100/100\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0434 - acc: 0.9853 - val_loss: 2.4525 - val_acc: 0.6567\n",
            "28709/28709 [==============================] - 22s 757us/step\n",
            "Accuracy: 99.77707339161935%\n",
            "Train Loss: 0.00502367771275763\n",
            "3589/3589 [==============================] - 3s 754us/step\n",
            "Accuracy: 65.67288938505997%\n",
            "Validation Loss: 2.4525050183596733\n",
            "3589/3589 [==============================] - 3s 756us/step\n",
            "Accuracy: 67.70688214153259%\n",
            "Test Loss: 2.2811240145890417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9RuSPmnYPAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpImFNBFRNKEArqISJr4/zG2E/FShOygAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC8xNwtn0rmt",
        "colab_type": "code",
        "outputId": "bccaab0d-be15-430f-a4bd-b91de93929c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18718
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "number_of_classes = 7\n",
        "dimension = 48\n",
        "number_of_channels = 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(48, 48 ,1), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 500\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "adam = Adam(decay=decay)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.save('500epochs.h5')\n",
        "history=model.fit(train_X, train_Y, epochs=epochs, batch_size=128,validation_data=(val_X, val_Y))\n",
        "train_loss, test_acc = model.evaluate(train_X, train_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "train_loss, test_acc = model.evaluate(val_X, val_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "test_loss, test_acc = model.evaluate(test_X, test_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Test Loss: \" + repr(test_loss))\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_69 (Conv2D)           (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_70 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_77 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_78 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 4096)              18878464  \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 7)                 28679     \n",
            "=================================================================\n",
            "Total params: 26,545,095\n",
            "Trainable params: 26,543,175\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/500\n",
            "28709/28709 [==============================] - 58s 2ms/step - loss: 3.7674 - acc: 0.2217 - val_loss: 1.8403 - val_acc: 0.2524\n",
            "Epoch 2/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.8156 - acc: 0.2504 - val_loss: 1.7851 - val_acc: 0.2622\n",
            "Epoch 3/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.8032 - acc: 0.2567 - val_loss: 1.7722 - val_acc: 0.2625\n",
            "Epoch 4/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.7840 - acc: 0.2674 - val_loss: 1.7164 - val_acc: 0.2909\n",
            "Epoch 5/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.6730 - acc: 0.3205 - val_loss: 1.6780 - val_acc: 0.3377\n",
            "Epoch 6/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.5253 - acc: 0.3930 - val_loss: 1.5698 - val_acc: 0.4168\n",
            "Epoch 7/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.4246 - acc: 0.4465 - val_loss: 1.4068 - val_acc: 0.4425\n",
            "Epoch 8/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.3381 - acc: 0.4818 - val_loss: 1.3297 - val_acc: 0.4884\n",
            "Epoch 9/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.2805 - acc: 0.5072 - val_loss: 1.4240 - val_acc: 0.4581\n",
            "Epoch 10/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.2252 - acc: 0.5293 - val_loss: 1.3429 - val_acc: 0.4923\n",
            "Epoch 11/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1650 - acc: 0.5553 - val_loss: 1.3764 - val_acc: 0.5032\n",
            "Epoch 12/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.1208 - acc: 0.5774 - val_loss: 1.1409 - val_acc: 0.5626\n",
            "Epoch 13/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.0789 - acc: 0.5939 - val_loss: 1.1847 - val_acc: 0.5731\n",
            "Epoch 14/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 1.0388 - acc: 0.6121 - val_loss: 1.1145 - val_acc: 0.5899\n",
            "Epoch 15/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9954 - acc: 0.6259 - val_loss: 1.1265 - val_acc: 0.5924\n",
            "Epoch 16/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9597 - acc: 0.6429 - val_loss: 1.5419 - val_acc: 0.5631\n",
            "Epoch 17/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9312 - acc: 0.6546 - val_loss: 1.3973 - val_acc: 0.5860\n",
            "Epoch 18/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.9032 - acc: 0.6643 - val_loss: 1.1010 - val_acc: 0.6105\n",
            "Epoch 19/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8538 - acc: 0.6806 - val_loss: 1.1314 - val_acc: 0.5932\n",
            "Epoch 20/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.8134 - acc: 0.6968 - val_loss: 1.1314 - val_acc: 0.6032\n",
            "Epoch 21/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7669 - acc: 0.7160 - val_loss: 1.0856 - val_acc: 0.6219\n",
            "Epoch 22/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.7368 - acc: 0.7264 - val_loss: 1.1541 - val_acc: 0.6096\n",
            "Epoch 23/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6852 - acc: 0.7452 - val_loss: 1.1347 - val_acc: 0.6244\n",
            "Epoch 24/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6470 - acc: 0.7624 - val_loss: 1.1299 - val_acc: 0.6305\n",
            "Epoch 25/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.6073 - acc: 0.7790 - val_loss: 1.1635 - val_acc: 0.6308\n",
            "Epoch 26/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5783 - acc: 0.7878 - val_loss: 1.2859 - val_acc: 0.6096\n",
            "Epoch 27/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.5311 - acc: 0.8078 - val_loss: 1.2647 - val_acc: 0.6361\n",
            "Epoch 28/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4952 - acc: 0.8194 - val_loss: 1.1915 - val_acc: 0.6408\n",
            "Epoch 29/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4519 - acc: 0.8367 - val_loss: 1.4458 - val_acc: 0.6459\n",
            "Epoch 30/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.4210 - acc: 0.8491 - val_loss: 1.3205 - val_acc: 0.6531\n",
            "Epoch 31/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3930 - acc: 0.8581 - val_loss: 1.5410 - val_acc: 0.6392\n",
            "Epoch 32/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3597 - acc: 0.8723 - val_loss: 1.3386 - val_acc: 0.6420\n",
            "Epoch 33/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3241 - acc: 0.8839 - val_loss: 1.5537 - val_acc: 0.6503\n",
            "Epoch 34/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.3177 - acc: 0.8874 - val_loss: 1.4889 - val_acc: 0.6464\n",
            "Epoch 35/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2833 - acc: 0.9000 - val_loss: 1.5198 - val_acc: 0.6590\n",
            "Epoch 36/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2585 - acc: 0.9099 - val_loss: 1.4756 - val_acc: 0.6525\n",
            "Epoch 37/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2453 - acc: 0.9148 - val_loss: 1.6378 - val_acc: 0.6442\n",
            "Epoch 38/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2239 - acc: 0.9202 - val_loss: 1.6846 - val_acc: 0.6408\n",
            "Epoch 39/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2114 - acc: 0.9268 - val_loss: 1.6717 - val_acc: 0.6567\n",
            "Epoch 40/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.2026 - acc: 0.9308 - val_loss: 1.6478 - val_acc: 0.6506\n",
            "Epoch 41/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1867 - acc: 0.9352 - val_loss: 1.6386 - val_acc: 0.6425\n",
            "Epoch 42/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1745 - acc: 0.9405 - val_loss: 1.6824 - val_acc: 0.6442\n",
            "Epoch 43/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1580 - acc: 0.9455 - val_loss: 1.8367 - val_acc: 0.6506\n",
            "Epoch 44/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1542 - acc: 0.9464 - val_loss: 1.8367 - val_acc: 0.6461\n",
            "Epoch 45/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1468 - acc: 0.9495 - val_loss: 1.9298 - val_acc: 0.6339\n",
            "Epoch 46/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1575 - acc: 0.9457 - val_loss: 1.9168 - val_acc: 0.6551\n",
            "Epoch 47/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1336 - acc: 0.9546 - val_loss: 1.8787 - val_acc: 0.6467\n",
            "Epoch 48/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1314 - acc: 0.9552 - val_loss: 1.9748 - val_acc: 0.6567\n",
            "Epoch 49/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1272 - acc: 0.9554 - val_loss: 1.7673 - val_acc: 0.6525\n",
            "Epoch 50/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1230 - acc: 0.9577 - val_loss: 1.9560 - val_acc: 0.6386\n",
            "Epoch 51/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1155 - acc: 0.9612 - val_loss: 1.9514 - val_acc: 0.6481\n",
            "Epoch 52/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1055 - acc: 0.9652 - val_loss: 2.0236 - val_acc: 0.6506\n",
            "Epoch 53/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1050 - acc: 0.9639 - val_loss: 2.0844 - val_acc: 0.6397\n",
            "Epoch 54/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1030 - acc: 0.9638 - val_loss: 2.1265 - val_acc: 0.6539\n",
            "Epoch 55/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1022 - acc: 0.9651 - val_loss: 2.0793 - val_acc: 0.6537\n",
            "Epoch 56/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1006 - acc: 0.9673 - val_loss: 2.1628 - val_acc: 0.6525\n",
            "Epoch 57/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0957 - acc: 0.9673 - val_loss: 2.0531 - val_acc: 0.6411\n",
            "Epoch 58/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0984 - acc: 0.9667 - val_loss: 2.3436 - val_acc: 0.6372\n",
            "Epoch 59/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0955 - acc: 0.9692 - val_loss: 2.0289 - val_acc: 0.6420\n",
            "Epoch 60/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0983 - acc: 0.9671 - val_loss: 2.0755 - val_acc: 0.6612\n",
            "Epoch 61/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0831 - acc: 0.9720 - val_loss: 1.9832 - val_acc: 0.6570\n",
            "Epoch 62/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0827 - acc: 0.9722 - val_loss: 2.0875 - val_acc: 0.6506\n",
            "Epoch 63/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0784 - acc: 0.9731 - val_loss: 2.0846 - val_acc: 0.6478\n",
            "Epoch 64/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0855 - acc: 0.9704 - val_loss: 2.0022 - val_acc: 0.6573\n",
            "Epoch 65/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0812 - acc: 0.9726 - val_loss: 1.9953 - val_acc: 0.6570\n",
            "Epoch 66/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0818 - acc: 0.9729 - val_loss: 2.1553 - val_acc: 0.6570\n",
            "Epoch 67/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0835 - acc: 0.9723 - val_loss: 2.2310 - val_acc: 0.6484\n",
            "Epoch 68/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0804 - acc: 0.9730 - val_loss: 2.0804 - val_acc: 0.6551\n",
            "Epoch 69/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0781 - acc: 0.9730 - val_loss: 2.1885 - val_acc: 0.6601\n",
            "Epoch 70/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0723 - acc: 0.9761 - val_loss: 2.0200 - val_acc: 0.6556\n",
            "Epoch 71/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0730 - acc: 0.9758 - val_loss: 2.1005 - val_acc: 0.6531\n",
            "Epoch 72/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0672 - acc: 0.9769 - val_loss: 2.1332 - val_acc: 0.6420\n",
            "Epoch 73/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0646 - acc: 0.9782 - val_loss: 2.1724 - val_acc: 0.6512\n",
            "Epoch 74/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0666 - acc: 0.9765 - val_loss: 2.1899 - val_acc: 0.6556\n",
            "Epoch 75/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0656 - acc: 0.9786 - val_loss: 2.2210 - val_acc: 0.6461\n",
            "Epoch 76/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0642 - acc: 0.9788 - val_loss: 2.3499 - val_acc: 0.6506\n",
            "Epoch 77/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0661 - acc: 0.9784 - val_loss: 2.1387 - val_acc: 0.6500\n",
            "Epoch 78/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0671 - acc: 0.9786 - val_loss: 2.2381 - val_acc: 0.6617\n",
            "Epoch 79/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0569 - acc: 0.9812 - val_loss: 2.1863 - val_acc: 0.6553\n",
            "Epoch 80/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0574 - acc: 0.9803 - val_loss: 2.1533 - val_acc: 0.6570\n",
            "Epoch 81/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0575 - acc: 0.9809 - val_loss: 2.1266 - val_acc: 0.6467\n",
            "Epoch 82/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0645 - acc: 0.9785 - val_loss: 2.3420 - val_acc: 0.6495\n",
            "Epoch 83/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0746 - acc: 0.9752 - val_loss: 2.0797 - val_acc: 0.6662\n",
            "Epoch 84/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0544 - acc: 0.9813 - val_loss: 2.0327 - val_acc: 0.6553\n",
            "Epoch 85/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0519 - acc: 0.9824 - val_loss: 2.3289 - val_acc: 0.6319\n",
            "Epoch 86/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0586 - acc: 0.9811 - val_loss: 2.1574 - val_acc: 0.6431\n",
            "Epoch 87/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0544 - acc: 0.9816 - val_loss: 2.2602 - val_acc: 0.6517\n",
            "Epoch 88/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0485 - acc: 0.9833 - val_loss: 2.1680 - val_acc: 0.6584\n",
            "Epoch 89/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0553 - acc: 0.9817 - val_loss: 2.1783 - val_acc: 0.6517\n",
            "Epoch 90/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0512 - acc: 0.9827 - val_loss: 2.2429 - val_acc: 0.6461\n",
            "Epoch 91/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0620 - acc: 0.9790 - val_loss: 2.2060 - val_acc: 0.6537\n",
            "Epoch 92/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0597 - acc: 0.9804 - val_loss: 2.2740 - val_acc: 0.6372\n",
            "Epoch 93/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0516 - acc: 0.9823 - val_loss: 2.2429 - val_acc: 0.6651\n",
            "Epoch 94/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0441 - acc: 0.9850 - val_loss: 2.4543 - val_acc: 0.6498\n",
            "Epoch 95/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0593 - acc: 0.9806 - val_loss: 2.4823 - val_acc: 0.6525\n",
            "Epoch 96/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0452 - acc: 0.9842 - val_loss: 2.3835 - val_acc: 0.6509\n",
            "Epoch 97/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0510 - acc: 0.9832 - val_loss: 2.4364 - val_acc: 0.6623\n",
            "Epoch 98/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0514 - acc: 0.9835 - val_loss: 2.3260 - val_acc: 0.6528\n",
            "Epoch 99/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0495 - acc: 0.9829 - val_loss: 2.2378 - val_acc: 0.6576\n",
            "Epoch 100/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0481 - acc: 0.9833 - val_loss: 2.2273 - val_acc: 0.6581\n",
            "Epoch 101/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0468 - acc: 0.9833 - val_loss: 2.4201 - val_acc: 0.6486\n",
            "Epoch 102/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0538 - acc: 0.9815 - val_loss: 2.4621 - val_acc: 0.6523\n",
            "Epoch 103/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0508 - acc: 0.9835 - val_loss: 2.3203 - val_acc: 0.6509\n",
            "Epoch 104/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0414 - acc: 0.9856 - val_loss: 2.3908 - val_acc: 0.6645\n",
            "Epoch 105/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0422 - acc: 0.9856 - val_loss: 2.3525 - val_acc: 0.6525\n",
            "Epoch 106/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0404 - acc: 0.9862 - val_loss: 2.3363 - val_acc: 0.6595\n",
            "Epoch 107/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0388 - acc: 0.9867 - val_loss: 2.3337 - val_acc: 0.6620\n",
            "Epoch 108/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0448 - acc: 0.9848 - val_loss: 2.3406 - val_acc: 0.6512\n",
            "Epoch 109/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0451 - acc: 0.9853 - val_loss: 2.3593 - val_acc: 0.6534\n",
            "Epoch 110/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0409 - acc: 0.9867 - val_loss: 2.4879 - val_acc: 0.6453\n",
            "Epoch 111/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0390 - acc: 0.9864 - val_loss: 2.4961 - val_acc: 0.6584\n",
            "Epoch 112/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0374 - acc: 0.9863 - val_loss: 2.4717 - val_acc: 0.6576\n",
            "Epoch 113/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0396 - acc: 0.9859 - val_loss: 2.2751 - val_acc: 0.6631\n",
            "Epoch 114/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0367 - acc: 0.9868 - val_loss: 2.3807 - val_acc: 0.6606\n",
            "Epoch 115/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0354 - acc: 0.9878 - val_loss: 2.5160 - val_acc: 0.6567\n",
            "Epoch 116/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0367 - acc: 0.9871 - val_loss: 2.3465 - val_acc: 0.6581\n",
            "Epoch 117/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0520 - acc: 0.9830 - val_loss: 2.3686 - val_acc: 0.6587\n",
            "Epoch 118/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0410 - acc: 0.9866 - val_loss: 2.2245 - val_acc: 0.6581\n",
            "Epoch 119/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0379 - acc: 0.9872 - val_loss: 2.4918 - val_acc: 0.6567\n",
            "Epoch 120/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0362 - acc: 0.9878 - val_loss: 2.4690 - val_acc: 0.6500\n",
            "Epoch 121/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0324 - acc: 0.9882 - val_loss: 2.4397 - val_acc: 0.6478\n",
            "Epoch 122/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0349 - acc: 0.9879 - val_loss: 2.4583 - val_acc: 0.6537\n",
            "Epoch 123/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0349 - acc: 0.9883 - val_loss: 2.4043 - val_acc: 0.6503\n",
            "Epoch 124/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0345 - acc: 0.9886 - val_loss: 2.5035 - val_acc: 0.6537\n",
            "Epoch 125/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0327 - acc: 0.9887 - val_loss: 2.4257 - val_acc: 0.6489\n",
            "Epoch 126/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0393 - acc: 0.9869 - val_loss: 2.4811 - val_acc: 0.6601\n",
            "Epoch 127/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0304 - acc: 0.9891 - val_loss: 2.5545 - val_acc: 0.6623\n",
            "Epoch 128/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0352 - acc: 0.9888 - val_loss: 2.4174 - val_acc: 0.6640\n",
            "Epoch 129/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0402 - acc: 0.9866 - val_loss: 2.3492 - val_acc: 0.6525\n",
            "Epoch 130/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0359 - acc: 0.9886 - val_loss: 2.4341 - val_acc: 0.6634\n",
            "Epoch 131/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0305 - acc: 0.9889 - val_loss: 2.6636 - val_acc: 0.6562\n",
            "Epoch 132/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0344 - acc: 0.9876 - val_loss: 2.7756 - val_acc: 0.6595\n",
            "Epoch 133/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0365 - acc: 0.9879 - val_loss: 2.5632 - val_acc: 0.6581\n",
            "Epoch 134/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0348 - acc: 0.9884 - val_loss: 2.4557 - val_acc: 0.6576\n",
            "Epoch 135/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0294 - acc: 0.9893 - val_loss: 2.5782 - val_acc: 0.6609\n",
            "Epoch 136/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0293 - acc: 0.9906 - val_loss: 2.5053 - val_acc: 0.6581\n",
            "Epoch 137/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0325 - acc: 0.9882 - val_loss: 2.4489 - val_acc: 0.6668\n",
            "Epoch 138/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0301 - acc: 0.9896 - val_loss: 2.4991 - val_acc: 0.6565\n",
            "Epoch 139/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0290 - acc: 0.9905 - val_loss: 2.3774 - val_acc: 0.6565\n",
            "Epoch 140/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0314 - acc: 0.9892 - val_loss: 2.5649 - val_acc: 0.6578\n",
            "Epoch 141/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0295 - acc: 0.9896 - val_loss: 2.5432 - val_acc: 0.6573\n",
            "Epoch 142/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0322 - acc: 0.9885 - val_loss: 2.3420 - val_acc: 0.6615\n",
            "Epoch 143/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0327 - acc: 0.9887 - val_loss: 2.2515 - val_acc: 0.6551\n",
            "Epoch 144/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0324 - acc: 0.9892 - val_loss: 2.4202 - val_acc: 0.6604\n",
            "Epoch 145/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0317 - acc: 0.9891 - val_loss: 2.5556 - val_acc: 0.6551\n",
            "Epoch 146/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.1139 - acc: 0.9626 - val_loss: 2.3284 - val_acc: 0.6631\n",
            "Epoch 147/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0382 - acc: 0.9872 - val_loss: 2.2684 - val_acc: 0.6551\n",
            "Epoch 148/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0300 - acc: 0.9892 - val_loss: 2.6649 - val_acc: 0.6631\n",
            "Epoch 149/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0258 - acc: 0.9905 - val_loss: 2.7052 - val_acc: 0.6570\n",
            "Epoch 150/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0226 - acc: 0.9918 - val_loss: 2.6672 - val_acc: 0.6668\n",
            "Epoch 151/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0216 - acc: 0.9923 - val_loss: 2.6444 - val_acc: 0.6684\n",
            "Epoch 152/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0239 - acc: 0.9913 - val_loss: 2.7407 - val_acc: 0.6626\n",
            "Epoch 153/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0244 - acc: 0.9916 - val_loss: 2.5765 - val_acc: 0.6606\n",
            "Epoch 154/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0268 - acc: 0.9904 - val_loss: 2.4798 - val_acc: 0.6673\n",
            "Epoch 155/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0241 - acc: 0.9912 - val_loss: 2.4542 - val_acc: 0.6659\n",
            "Epoch 156/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0204 - acc: 0.9924 - val_loss: 2.7262 - val_acc: 0.6615\n",
            "Epoch 157/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0230 - acc: 0.9915 - val_loss: 2.5218 - val_acc: 0.6528\n",
            "Epoch 158/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0236 - acc: 0.9916 - val_loss: 2.4938 - val_acc: 0.6698\n",
            "Epoch 159/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0223 - acc: 0.9917 - val_loss: 2.4718 - val_acc: 0.6631\n",
            "Epoch 160/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0236 - acc: 0.9917 - val_loss: 2.6291 - val_acc: 0.6606\n",
            "Epoch 161/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0265 - acc: 0.9906 - val_loss: 2.6729 - val_acc: 0.6528\n",
            "Epoch 162/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0249 - acc: 0.9912 - val_loss: 2.5504 - val_acc: 0.6559\n",
            "Epoch 163/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0254 - acc: 0.9914 - val_loss: 2.4704 - val_acc: 0.6567\n",
            "Epoch 164/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0242 - acc: 0.9914 - val_loss: 2.5252 - val_acc: 0.6531\n",
            "Epoch 165/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0252 - acc: 0.9916 - val_loss: 2.5200 - val_acc: 0.6598\n",
            "Epoch 166/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0248 - acc: 0.9907 - val_loss: 2.4988 - val_acc: 0.6612\n",
            "Epoch 167/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0222 - acc: 0.9922 - val_loss: 2.6772 - val_acc: 0.6693\n",
            "Epoch 168/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0229 - acc: 0.9927 - val_loss: 2.5803 - val_acc: 0.6556\n",
            "Epoch 169/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0231 - acc: 0.9922 - val_loss: 2.7670 - val_acc: 0.6623\n",
            "Epoch 170/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0269 - acc: 0.9909 - val_loss: 2.4955 - val_acc: 0.6548\n",
            "Epoch 171/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0245 - acc: 0.9916 - val_loss: 2.3663 - val_acc: 0.6584\n",
            "Epoch 172/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0200 - acc: 0.9928 - val_loss: 2.5803 - val_acc: 0.6654\n",
            "Epoch 173/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0236 - acc: 0.9920 - val_loss: 2.3020 - val_acc: 0.6548\n",
            "Epoch 174/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0262 - acc: 0.9912 - val_loss: 2.3522 - val_acc: 0.6623\n",
            "Epoch 175/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0250 - acc: 0.9912 - val_loss: 2.5675 - val_acc: 0.6609\n",
            "Epoch 176/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0207 - acc: 0.9921 - val_loss: 2.5675 - val_acc: 0.6573\n",
            "Epoch 177/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0213 - acc: 0.9919 - val_loss: 2.9805 - val_acc: 0.6514\n",
            "Epoch 178/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0275 - acc: 0.9905 - val_loss: 2.4229 - val_acc: 0.6651\n",
            "Epoch 179/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0237 - acc: 0.9916 - val_loss: 2.4514 - val_acc: 0.6606\n",
            "Epoch 180/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0287 - acc: 0.9905 - val_loss: 2.3731 - val_acc: 0.6578\n",
            "Epoch 181/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0275 - acc: 0.9902 - val_loss: 2.5374 - val_acc: 0.6637\n",
            "Epoch 182/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0201 - acc: 0.9927 - val_loss: 2.6245 - val_acc: 0.6595\n",
            "Epoch 183/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0203 - acc: 0.9925 - val_loss: 2.5364 - val_acc: 0.6551\n",
            "Epoch 184/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0192 - acc: 0.9933 - val_loss: 2.6539 - val_acc: 0.6556\n",
            "Epoch 185/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0206 - acc: 0.9925 - val_loss: 2.6368 - val_acc: 0.6581\n",
            "Epoch 186/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0212 - acc: 0.9926 - val_loss: 2.7224 - val_acc: 0.6604\n",
            "Epoch 187/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0212 - acc: 0.9924 - val_loss: 2.4506 - val_acc: 0.6506\n",
            "Epoch 188/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0202 - acc: 0.9928 - val_loss: 2.5798 - val_acc: 0.6587\n",
            "Epoch 189/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0204 - acc: 0.9929 - val_loss: 2.6008 - val_acc: 0.6587\n",
            "Epoch 190/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0258 - acc: 0.9908 - val_loss: 2.4663 - val_acc: 0.6631\n",
            "Epoch 191/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0173 - acc: 0.9934 - val_loss: 2.6591 - val_acc: 0.6629\n",
            "Epoch 192/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0189 - acc: 0.9933 - val_loss: 2.4781 - val_acc: 0.6578\n",
            "Epoch 193/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0202 - acc: 0.9923 - val_loss: 2.6706 - val_acc: 0.6567\n",
            "Epoch 194/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0200 - acc: 0.9930 - val_loss: 2.5943 - val_acc: 0.6581\n",
            "Epoch 195/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0196 - acc: 0.9926 - val_loss: 2.8107 - val_acc: 0.6506\n",
            "Epoch 196/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0247 - acc: 0.9912 - val_loss: 2.7160 - val_acc: 0.6617\n",
            "Epoch 197/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0212 - acc: 0.9926 - val_loss: 2.6404 - val_acc: 0.6637\n",
            "Epoch 198/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0196 - acc: 0.9930 - val_loss: 2.6817 - val_acc: 0.6640\n",
            "Epoch 199/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0222 - acc: 0.9920 - val_loss: 2.6147 - val_acc: 0.6551\n",
            "Epoch 200/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0184 - acc: 0.9930 - val_loss: 2.6861 - val_acc: 0.6565\n",
            "Epoch 201/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0238 - acc: 0.9919 - val_loss: 2.5570 - val_acc: 0.6643\n",
            "Epoch 202/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0221 - acc: 0.9925 - val_loss: 2.7799 - val_acc: 0.6551\n",
            "Epoch 203/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0248 - acc: 0.9912 - val_loss: 2.4746 - val_acc: 0.6604\n",
            "Epoch 204/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0177 - acc: 0.9940 - val_loss: 2.7373 - val_acc: 0.6617\n",
            "Epoch 205/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0178 - acc: 0.9936 - val_loss: 2.9660 - val_acc: 0.6573\n",
            "Epoch 206/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0212 - acc: 0.9921 - val_loss: 2.6364 - val_acc: 0.6581\n",
            "Epoch 207/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0182 - acc: 0.9936 - val_loss: 2.7497 - val_acc: 0.6592\n",
            "Epoch 208/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0209 - acc: 0.9925 - val_loss: 2.6637 - val_acc: 0.6559\n",
            "Epoch 209/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0165 - acc: 0.9935 - val_loss: 2.6297 - val_acc: 0.6645\n",
            "Epoch 210/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0186 - acc: 0.9936 - val_loss: 2.8131 - val_acc: 0.6637\n",
            "Epoch 211/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0182 - acc: 0.9932 - val_loss: 2.6342 - val_acc: 0.6651\n",
            "Epoch 212/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0190 - acc: 0.9930 - val_loss: 2.6504 - val_acc: 0.6629\n",
            "Epoch 213/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0210 - acc: 0.9930 - val_loss: 2.6909 - val_acc: 0.6556\n",
            "Epoch 214/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0174 - acc: 0.9932 - val_loss: 2.8345 - val_acc: 0.6584\n",
            "Epoch 215/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0220 - acc: 0.9927 - val_loss: 2.6872 - val_acc: 0.6553\n",
            "Epoch 216/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0185 - acc: 0.9934 - val_loss: 2.6230 - val_acc: 0.6695\n",
            "Epoch 217/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0284 - acc: 0.9909 - val_loss: 2.5484 - val_acc: 0.6531\n",
            "Epoch 218/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0279 - acc: 0.9904 - val_loss: 2.5823 - val_acc: 0.6648\n",
            "Epoch 219/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0188 - acc: 0.9931 - val_loss: 2.6521 - val_acc: 0.6656\n",
            "Epoch 220/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0170 - acc: 0.9944 - val_loss: 2.7037 - val_acc: 0.6668\n",
            "Epoch 221/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0156 - acc: 0.9944 - val_loss: 2.6708 - val_acc: 0.6693\n",
            "Epoch 222/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0128 - acc: 0.9953 - val_loss: 2.7797 - val_acc: 0.6645\n",
            "Epoch 223/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0177 - acc: 0.9935 - val_loss: 2.5668 - val_acc: 0.6601\n",
            "Epoch 224/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0189 - acc: 0.9936 - val_loss: 2.6138 - val_acc: 0.6656\n",
            "Epoch 225/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0163 - acc: 0.9937 - val_loss: 2.7149 - val_acc: 0.6629\n",
            "Epoch 226/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0177 - acc: 0.9936 - val_loss: 2.6296 - val_acc: 0.6704\n",
            "Epoch 227/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0185 - acc: 0.9932 - val_loss: 2.7844 - val_acc: 0.6598\n",
            "Epoch 228/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0219 - acc: 0.9925 - val_loss: 2.6702 - val_acc: 0.6640\n",
            "Epoch 229/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0192 - acc: 0.9930 - val_loss: 2.5686 - val_acc: 0.6609\n",
            "Epoch 230/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0165 - acc: 0.9942 - val_loss: 2.5604 - val_acc: 0.6592\n",
            "Epoch 231/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0162 - acc: 0.9943 - val_loss: 2.6288 - val_acc: 0.6562\n",
            "Epoch 232/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0158 - acc: 0.9944 - val_loss: 2.5855 - val_acc: 0.6637\n",
            "Epoch 233/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0160 - acc: 0.9938 - val_loss: 2.6477 - val_acc: 0.6665\n",
            "Epoch 234/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0197 - acc: 0.9928 - val_loss: 2.4265 - val_acc: 0.6631\n",
            "Epoch 235/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0257 - acc: 0.9916 - val_loss: 2.5841 - val_acc: 0.6659\n",
            "Epoch 236/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0217 - acc: 0.9924 - val_loss: 2.7502 - val_acc: 0.6500\n",
            "Epoch 237/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0181 - acc: 0.9937 - val_loss: 2.6348 - val_acc: 0.6595\n",
            "Epoch 238/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0170 - acc: 0.9939 - val_loss: 2.5629 - val_acc: 0.6682\n",
            "Epoch 239/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0162 - acc: 0.9942 - val_loss: 2.7028 - val_acc: 0.6656\n",
            "Epoch 240/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0191 - acc: 0.9932 - val_loss: 2.5930 - val_acc: 0.6606\n",
            "Epoch 241/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0171 - acc: 0.9931 - val_loss: 2.6207 - val_acc: 0.6626\n",
            "Epoch 242/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0153 - acc: 0.9944 - val_loss: 2.7975 - val_acc: 0.6551\n",
            "Epoch 243/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0135 - acc: 0.9946 - val_loss: 2.8744 - val_acc: 0.6617\n",
            "Epoch 244/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0195 - acc: 0.9933 - val_loss: 2.5976 - val_acc: 0.6556\n",
            "Epoch 245/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0163 - acc: 0.9937 - val_loss: 2.8071 - val_acc: 0.6679\n",
            "Epoch 246/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0140 - acc: 0.9944 - val_loss: 2.8610 - val_acc: 0.6592\n",
            "Epoch 247/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0146 - acc: 0.9945 - val_loss: 2.7112 - val_acc: 0.6587\n",
            "Epoch 248/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0189 - acc: 0.9935 - val_loss: 2.6221 - val_acc: 0.6598\n",
            "Epoch 249/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0167 - acc: 0.9941 - val_loss: 2.9188 - val_acc: 0.6673\n",
            "Epoch 250/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0139 - acc: 0.9947 - val_loss: 2.7148 - val_acc: 0.6617\n",
            "Epoch 251/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0170 - acc: 0.9937 - val_loss: 2.8823 - val_acc: 0.6556\n",
            "Epoch 252/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0159 - acc: 0.9944 - val_loss: 2.4648 - val_acc: 0.6682\n",
            "Epoch 253/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0172 - acc: 0.9937 - val_loss: 2.6637 - val_acc: 0.6651\n",
            "Epoch 254/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0139 - acc: 0.9945 - val_loss: 2.8611 - val_acc: 0.6620\n",
            "Epoch 255/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0135 - acc: 0.9947 - val_loss: 2.9451 - val_acc: 0.6606\n",
            "Epoch 256/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0146 - acc: 0.9948 - val_loss: 2.8808 - val_acc: 0.6615\n",
            "Epoch 257/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0147 - acc: 0.9944 - val_loss: 2.6747 - val_acc: 0.6598\n",
            "Epoch 258/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0144 - acc: 0.9947 - val_loss: 2.7110 - val_acc: 0.6656\n",
            "Epoch 259/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0130 - acc: 0.9949 - val_loss: 2.6498 - val_acc: 0.6690\n",
            "Epoch 260/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0143 - acc: 0.9942 - val_loss: 2.8569 - val_acc: 0.6612\n",
            "Epoch 261/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0172 - acc: 0.9928 - val_loss: 2.9665 - val_acc: 0.6612\n",
            "Epoch 262/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0153 - acc: 0.9943 - val_loss: 2.8215 - val_acc: 0.6578\n",
            "Epoch 263/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0172 - acc: 0.9945 - val_loss: 2.7278 - val_acc: 0.6634\n",
            "Epoch 264/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0168 - acc: 0.9938 - val_loss: 2.7494 - val_acc: 0.6520\n",
            "Epoch 265/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0159 - acc: 0.9942 - val_loss: 2.6739 - val_acc: 0.6601\n",
            "Epoch 266/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0148 - acc: 0.9947 - val_loss: 2.7657 - val_acc: 0.6676\n",
            "Epoch 267/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0169 - acc: 0.9940 - val_loss: 2.7393 - val_acc: 0.6626\n",
            "Epoch 268/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0184 - acc: 0.9933 - val_loss: 2.4519 - val_acc: 0.6662\n",
            "Epoch 269/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0179 - acc: 0.9941 - val_loss: 2.8773 - val_acc: 0.6565\n",
            "Epoch 270/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0150 - acc: 0.9944 - val_loss: 2.7226 - val_acc: 0.6590\n",
            "Epoch 271/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0140 - acc: 0.9951 - val_loss: 2.8315 - val_acc: 0.6623\n",
            "Epoch 272/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0178 - acc: 0.9937 - val_loss: 2.7175 - val_acc: 0.6643\n",
            "Epoch 273/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0148 - acc: 0.9944 - val_loss: 2.6861 - val_acc: 0.6548\n",
            "Epoch 274/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0120 - acc: 0.9954 - val_loss: 2.8872 - val_acc: 0.6612\n",
            "Epoch 275/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0145 - acc: 0.9944 - val_loss: 2.6287 - val_acc: 0.6592\n",
            "Epoch 276/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0166 - acc: 0.9944 - val_loss: 2.6548 - val_acc: 0.6643\n",
            "Epoch 277/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0126 - acc: 0.9949 - val_loss: 2.7489 - val_acc: 0.6634\n",
            "Epoch 278/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0149 - acc: 0.9951 - val_loss: 2.7292 - val_acc: 0.6698\n",
            "Epoch 279/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0148 - acc: 0.9944 - val_loss: 2.6548 - val_acc: 0.6651\n",
            "Epoch 280/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0126 - acc: 0.9949 - val_loss: 2.7060 - val_acc: 0.6631\n",
            "Epoch 281/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0144 - acc: 0.9947 - val_loss: 2.6671 - val_acc: 0.6620\n",
            "Epoch 282/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0103 - acc: 0.9956 - val_loss: 2.9357 - val_acc: 0.6645\n",
            "Epoch 283/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0153 - acc: 0.9948 - val_loss: 2.7467 - val_acc: 0.6626\n",
            "Epoch 284/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0124 - acc: 0.9951 - val_loss: 2.9226 - val_acc: 0.6617\n",
            "Epoch 285/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0149 - acc: 0.9948 - val_loss: 2.8047 - val_acc: 0.6606\n",
            "Epoch 286/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0149 - acc: 0.9945 - val_loss: 2.8649 - val_acc: 0.6617\n",
            "Epoch 287/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0150 - acc: 0.9942 - val_loss: 2.7807 - val_acc: 0.6651\n",
            "Epoch 288/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0137 - acc: 0.9952 - val_loss: 2.9198 - val_acc: 0.6595\n",
            "Epoch 289/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0143 - acc: 0.9950 - val_loss: 3.0689 - val_acc: 0.6682\n",
            "Epoch 290/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0130 - acc: 0.9954 - val_loss: 2.7367 - val_acc: 0.6707\n",
            "Epoch 291/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0133 - acc: 0.9953 - val_loss: 2.6078 - val_acc: 0.6701\n",
            "Epoch 292/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0132 - acc: 0.9949 - val_loss: 2.9390 - val_acc: 0.6676\n",
            "Epoch 293/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0120 - acc: 0.9953 - val_loss: 2.9981 - val_acc: 0.6626\n",
            "Epoch 294/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0141 - acc: 0.9941 - val_loss: 2.7030 - val_acc: 0.6645\n",
            "Epoch 295/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0135 - acc: 0.9950 - val_loss: 2.7221 - val_acc: 0.6698\n",
            "Epoch 296/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0136 - acc: 0.9948 - val_loss: 2.7531 - val_acc: 0.6551\n",
            "Epoch 297/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0126 - acc: 0.9951 - val_loss: 2.7615 - val_acc: 0.6612\n",
            "Epoch 298/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0124 - acc: 0.9952 - val_loss: 2.7846 - val_acc: 0.6637\n",
            "Epoch 299/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0140 - acc: 0.9946 - val_loss: 2.7114 - val_acc: 0.6684\n",
            "Epoch 300/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0122 - acc: 0.9954 - val_loss: 2.9790 - val_acc: 0.6590\n",
            "Epoch 301/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0139 - acc: 0.9949 - val_loss: 2.9492 - val_acc: 0.6595\n",
            "Epoch 302/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0143 - acc: 0.9950 - val_loss: 2.8178 - val_acc: 0.6690\n",
            "Epoch 303/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0134 - acc: 0.9952 - val_loss: 2.7043 - val_acc: 0.6634\n",
            "Epoch 304/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0115 - acc: 0.9958 - val_loss: 2.7540 - val_acc: 0.6623\n",
            "Epoch 305/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0132 - acc: 0.9949 - val_loss: 2.7423 - val_acc: 0.6762\n",
            "Epoch 306/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0139 - acc: 0.9948 - val_loss: 2.7616 - val_acc: 0.6679\n",
            "Epoch 307/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0124 - acc: 0.9953 - val_loss: 2.7102 - val_acc: 0.6695\n",
            "Epoch 308/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0116 - acc: 0.9953 - val_loss: 3.0005 - val_acc: 0.6615\n",
            "Epoch 309/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0119 - acc: 0.9955 - val_loss: 2.8543 - val_acc: 0.6634\n",
            "Epoch 310/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0119 - acc: 0.9956 - val_loss: 2.9262 - val_acc: 0.6570\n",
            "Epoch 311/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0133 - acc: 0.9949 - val_loss: 2.8701 - val_acc: 0.6615\n",
            "Epoch 312/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0141 - acc: 0.9953 - val_loss: 2.7946 - val_acc: 0.6656\n",
            "Epoch 313/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0126 - acc: 0.9948 - val_loss: 2.9991 - val_acc: 0.6648\n",
            "Epoch 314/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0108 - acc: 0.9954 - val_loss: 2.8794 - val_acc: 0.6617\n",
            "Epoch 315/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0141 - acc: 0.9953 - val_loss: 2.5361 - val_acc: 0.6626\n",
            "Epoch 316/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0139 - acc: 0.9952 - val_loss: 2.8054 - val_acc: 0.6620\n",
            "Epoch 317/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0117 - acc: 0.9954 - val_loss: 2.7371 - val_acc: 0.6656\n",
            "Epoch 318/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0114 - acc: 0.9954 - val_loss: 2.9303 - val_acc: 0.6573\n",
            "Epoch 319/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0129 - acc: 0.9948 - val_loss: 2.8326 - val_acc: 0.6626\n",
            "Epoch 320/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0118 - acc: 0.9949 - val_loss: 2.6582 - val_acc: 0.6553\n",
            "Epoch 321/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0151 - acc: 0.9948 - val_loss: 2.7750 - val_acc: 0.6609\n",
            "Epoch 322/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0141 - acc: 0.9951 - val_loss: 2.8789 - val_acc: 0.6570\n",
            "Epoch 323/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0126 - acc: 0.9951 - val_loss: 2.7465 - val_acc: 0.6617\n",
            "Epoch 324/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0119 - acc: 0.9955 - val_loss: 2.9550 - val_acc: 0.6592\n",
            "Epoch 325/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0097 - acc: 0.9961 - val_loss: 2.9772 - val_acc: 0.6640\n",
            "Epoch 326/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0103 - acc: 0.9958 - val_loss: 2.8218 - val_acc: 0.6612\n",
            "Epoch 327/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0100 - acc: 0.9961 - val_loss: 2.8894 - val_acc: 0.6584\n",
            "Epoch 328/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0114 - acc: 0.9952 - val_loss: 2.7593 - val_acc: 0.6679\n",
            "Epoch 329/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0101 - acc: 0.9959 - val_loss: 3.0021 - val_acc: 0.6590\n",
            "Epoch 330/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0143 - acc: 0.9944 - val_loss: 2.6954 - val_acc: 0.6578\n",
            "Epoch 331/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0125 - acc: 0.9953 - val_loss: 2.8265 - val_acc: 0.6662\n",
            "Epoch 332/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0108 - acc: 0.9955 - val_loss: 2.7921 - val_acc: 0.6623\n",
            "Epoch 333/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0114 - acc: 0.9958 - val_loss: 2.9612 - val_acc: 0.6698\n",
            "Epoch 334/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0148 - acc: 0.9948 - val_loss: 2.6830 - val_acc: 0.6679\n",
            "Epoch 335/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0133 - acc: 0.9951 - val_loss: 2.6993 - val_acc: 0.6723\n",
            "Epoch 336/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0105 - acc: 0.9961 - val_loss: 2.7647 - val_acc: 0.6721\n",
            "Epoch 337/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0104 - acc: 0.9958 - val_loss: 2.9186 - val_acc: 0.6654\n",
            "Epoch 338/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0093 - acc: 0.9961 - val_loss: 2.8974 - val_acc: 0.6656\n",
            "Epoch 339/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0091 - acc: 0.9959 - val_loss: 3.0022 - val_acc: 0.6587\n",
            "Epoch 340/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0104 - acc: 0.9964 - val_loss: 2.8771 - val_acc: 0.6604\n",
            "Epoch 341/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0117 - acc: 0.9959 - val_loss: 2.8621 - val_acc: 0.6643\n",
            "Epoch 342/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0115 - acc: 0.9956 - val_loss: 2.8345 - val_acc: 0.6598\n",
            "Epoch 343/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0104 - acc: 0.9960 - val_loss: 2.8149 - val_acc: 0.6679\n",
            "Epoch 344/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0121 - acc: 0.9953 - val_loss: 3.0168 - val_acc: 0.6690\n",
            "Epoch 345/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0122 - acc: 0.9958 - val_loss: 2.7826 - val_acc: 0.6645\n",
            "Epoch 346/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0120 - acc: 0.9951 - val_loss: 2.8158 - val_acc: 0.6617\n",
            "Epoch 347/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0126 - acc: 0.9952 - val_loss: 2.7593 - val_acc: 0.6631\n",
            "Epoch 348/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0134 - acc: 0.9951 - val_loss: 2.7516 - val_acc: 0.6531\n",
            "Epoch 349/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0127 - acc: 0.9950 - val_loss: 2.7658 - val_acc: 0.6623\n",
            "Epoch 350/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0098 - acc: 0.9960 - val_loss: 2.8727 - val_acc: 0.6631\n",
            "Epoch 351/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0143 - acc: 0.9950 - val_loss: 2.8896 - val_acc: 0.6656\n",
            "Epoch 352/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0099 - acc: 0.9965 - val_loss: 2.8410 - val_acc: 0.6712\n",
            "Epoch 353/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0098 - acc: 0.9963 - val_loss: 2.7508 - val_acc: 0.6721\n",
            "Epoch 354/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0090 - acc: 0.9963 - val_loss: 2.9521 - val_acc: 0.6729\n",
            "Epoch 355/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0125 - acc: 0.9957 - val_loss: 2.7468 - val_acc: 0.6576\n",
            "Epoch 356/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0117 - acc: 0.9955 - val_loss: 2.9575 - val_acc: 0.6701\n",
            "Epoch 357/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0157 - acc: 0.9948 - val_loss: 2.9383 - val_acc: 0.6643\n",
            "Epoch 358/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0104 - acc: 0.9958 - val_loss: 3.0207 - val_acc: 0.6634\n",
            "Epoch 359/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0102 - acc: 0.9958 - val_loss: 2.8515 - val_acc: 0.6698\n",
            "Epoch 360/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0095 - acc: 0.9961 - val_loss: 3.0420 - val_acc: 0.6721\n",
            "Epoch 361/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0100 - acc: 0.9962 - val_loss: 2.8366 - val_acc: 0.6637\n",
            "Epoch 362/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0105 - acc: 0.9961 - val_loss: 3.0586 - val_acc: 0.6701\n",
            "Epoch 363/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0119 - acc: 0.9954 - val_loss: 2.8934 - val_acc: 0.6684\n",
            "Epoch 364/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0106 - acc: 0.9958 - val_loss: 2.8187 - val_acc: 0.6612\n",
            "Epoch 365/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0078 - acc: 0.9968 - val_loss: 2.9726 - val_acc: 0.6612\n",
            "Epoch 366/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0102 - acc: 0.9960 - val_loss: 3.0167 - val_acc: 0.6676\n",
            "Epoch 367/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0093 - acc: 0.9960 - val_loss: 2.9376 - val_acc: 0.6665\n",
            "Epoch 368/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0128 - acc: 0.9947 - val_loss: 2.8862 - val_acc: 0.6726\n",
            "Epoch 369/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0116 - acc: 0.9952 - val_loss: 2.8061 - val_acc: 0.6651\n",
            "Epoch 370/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0103 - acc: 0.9962 - val_loss: 2.9151 - val_acc: 0.6734\n",
            "Epoch 371/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0101 - acc: 0.9962 - val_loss: 2.8794 - val_acc: 0.6662\n",
            "Epoch 372/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0090 - acc: 0.9960 - val_loss: 3.1053 - val_acc: 0.6709\n",
            "Epoch 373/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9963 - val_loss: 2.8902 - val_acc: 0.6665\n",
            "Epoch 374/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0103 - acc: 0.9960 - val_loss: 2.9736 - val_acc: 0.6693\n",
            "Epoch 375/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0078 - acc: 0.9967 - val_loss: 3.1217 - val_acc: 0.6687\n",
            "Epoch 376/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0114 - acc: 0.9955 - val_loss: 3.0589 - val_acc: 0.6715\n",
            "Epoch 377/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0111 - acc: 0.9958 - val_loss: 2.9159 - val_acc: 0.6659\n",
            "Epoch 378/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9960 - val_loss: 3.0846 - val_acc: 0.6645\n",
            "Epoch 379/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0096 - acc: 0.9959 - val_loss: 2.8564 - val_acc: 0.6659\n",
            "Epoch 380/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0095 - acc: 0.9961 - val_loss: 3.0144 - val_acc: 0.6698\n",
            "Epoch 381/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0091 - acc: 0.9961 - val_loss: 3.0030 - val_acc: 0.6609\n",
            "Epoch 382/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0110 - acc: 0.9960 - val_loss: 2.9951 - val_acc: 0.6704\n",
            "Epoch 383/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0111 - acc: 0.9958 - val_loss: 2.7362 - val_acc: 0.6687\n",
            "Epoch 384/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0113 - acc: 0.9961 - val_loss: 2.8722 - val_acc: 0.6617\n",
            "Epoch 385/500\n",
            "28672/28709 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9965Epoch 386/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0088 - acc: 0.9963 - val_loss: 3.0669 - val_acc: 0.6682\n",
            "Epoch 387/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0123 - acc: 0.9956 - val_loss: 2.9885 - val_acc: 0.6695\n",
            "Epoch 388/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0120 - acc: 0.9959 - val_loss: 2.7625 - val_acc: 0.6709\n",
            "Epoch 389/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0125 - acc: 0.9956 - val_loss: 2.8445 - val_acc: 0.6631\n",
            "Epoch 390/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0121 - acc: 0.9954 - val_loss: 2.9029 - val_acc: 0.6645\n",
            "Epoch 391/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0104 - acc: 0.9960 - val_loss: 2.8558 - val_acc: 0.6721\n",
            "Epoch 392/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0093 - acc: 0.9961 - val_loss: 2.9065 - val_acc: 0.6673\n",
            "Epoch 393/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0107 - acc: 0.9961 - val_loss: 2.7906 - val_acc: 0.6715\n",
            "Epoch 394/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0117 - acc: 0.9962 - val_loss: 3.0723 - val_acc: 0.6684\n",
            "Epoch 395/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0171 - acc: 0.9944 - val_loss: 2.9228 - val_acc: 0.6668\n",
            "Epoch 396/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0161 - acc: 0.9944 - val_loss: 2.9332 - val_acc: 0.6651\n",
            "Epoch 397/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0100 - acc: 0.9960 - val_loss: 3.0039 - val_acc: 0.6693\n",
            "Epoch 398/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0081 - acc: 0.9963 - val_loss: 3.1344 - val_acc: 0.6698\n",
            "Epoch 399/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0109 - acc: 0.9957 - val_loss: 2.9378 - val_acc: 0.6637\n",
            "Epoch 400/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 3.0252 - val_acc: 0.6659\n",
            "Epoch 401/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 2.9879 - val_acc: 0.6709\n",
            "Epoch 402/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0090 - acc: 0.9962 - val_loss: 3.0211 - val_acc: 0.6654\n",
            "Epoch 403/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0089 - acc: 0.9963 - val_loss: 2.9987 - val_acc: 0.6704\n",
            "Epoch 404/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0066 - acc: 0.9971 - val_loss: 3.0781 - val_acc: 0.6684\n",
            "Epoch 405/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9965 - val_loss: 3.1148 - val_acc: 0.6668\n",
            "Epoch 406/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0085 - acc: 0.9963 - val_loss: 3.0809 - val_acc: 0.6665\n",
            "Epoch 407/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0088 - acc: 0.9962 - val_loss: 3.0825 - val_acc: 0.6643\n",
            "Epoch 408/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 2.8710 - val_acc: 0.6648\n",
            "Epoch 409/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0082 - acc: 0.9968 - val_loss: 3.0808 - val_acc: 0.6676\n",
            "Epoch 410/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0086 - acc: 0.9964 - val_loss: 2.9719 - val_acc: 0.6665\n",
            "Epoch 411/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0081 - acc: 0.9965 - val_loss: 3.0275 - val_acc: 0.6626\n",
            "Epoch 412/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0146 - acc: 0.9945 - val_loss: 3.2726 - val_acc: 0.6684\n",
            "Epoch 413/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0131 - acc: 0.9948 - val_loss: 3.0926 - val_acc: 0.6598\n",
            "Epoch 414/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0118 - acc: 0.9957 - val_loss: 2.9216 - val_acc: 0.6690\n",
            "Epoch 415/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0112 - acc: 0.9960 - val_loss: 2.9408 - val_acc: 0.6721\n",
            "Epoch 416/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0116 - acc: 0.9955 - val_loss: 2.8311 - val_acc: 0.6748\n",
            "Epoch 417/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0097 - acc: 0.9960 - val_loss: 2.9784 - val_acc: 0.6631\n",
            "Epoch 418/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0089 - acc: 0.9964 - val_loss: 2.9577 - val_acc: 0.6676\n",
            "Epoch 419/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9963 - val_loss: 2.8664 - val_acc: 0.6634\n",
            "Epoch 420/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0086 - acc: 0.9961 - val_loss: 2.7337 - val_acc: 0.6695\n",
            "Epoch 421/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0072 - acc: 0.9973 - val_loss: 3.1917 - val_acc: 0.6670\n",
            "Epoch 422/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0065 - acc: 0.9969 - val_loss: 3.0721 - val_acc: 0.6709\n",
            "Epoch 423/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0121 - acc: 0.9960 - val_loss: 2.7681 - val_acc: 0.6673\n",
            "Epoch 424/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0072 - acc: 0.9968 - val_loss: 3.2055 - val_acc: 0.6679\n",
            "Epoch 425/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0098 - acc: 0.9963 - val_loss: 2.9804 - val_acc: 0.6643\n",
            "Epoch 426/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0096 - acc: 0.9963 - val_loss: 3.0725 - val_acc: 0.6684\n",
            "Epoch 427/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0086 - acc: 0.9965 - val_loss: 3.0032 - val_acc: 0.6590\n",
            "Epoch 428/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0098 - acc: 0.9958 - val_loss: 3.0466 - val_acc: 0.6665\n",
            "Epoch 429/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0082 - acc: 0.9964 - val_loss: 3.1515 - val_acc: 0.6648\n",
            "Epoch 430/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0090 - acc: 0.9967 - val_loss: 3.0714 - val_acc: 0.6684\n",
            "Epoch 431/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0089 - acc: 0.9964 - val_loss: 2.9838 - val_acc: 0.6648\n",
            "Epoch 432/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0068 - acc: 0.9968 - val_loss: 3.1416 - val_acc: 0.6682\n",
            "Epoch 433/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0106 - acc: 0.9959 - val_loss: 2.8015 - val_acc: 0.6690\n",
            "Epoch 434/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0088 - acc: 0.9965 - val_loss: 2.8864 - val_acc: 0.6687\n",
            "Epoch 435/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9961 - val_loss: 2.9311 - val_acc: 0.6682\n",
            "Epoch 436/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0086 - acc: 0.9966 - val_loss: 2.9439 - val_acc: 0.6673\n",
            "Epoch 437/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0088 - acc: 0.9963 - val_loss: 2.9307 - val_acc: 0.6587\n",
            "Epoch 438/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0085 - acc: 0.9964 - val_loss: 2.9808 - val_acc: 0.6668\n",
            "Epoch 439/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9964 - val_loss: 2.8572 - val_acc: 0.6643\n",
            "Epoch 440/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0084 - acc: 0.9965 - val_loss: 2.9535 - val_acc: 0.6721\n",
            "Epoch 441/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0085 - acc: 0.9961 - val_loss: 3.0476 - val_acc: 0.6662\n",
            "Epoch 442/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0070 - acc: 0.9968 - val_loss: 3.0657 - val_acc: 0.6629\n",
            "Epoch 443/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0079 - acc: 0.9969 - val_loss: 3.1414 - val_acc: 0.6623\n",
            "Epoch 444/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0105 - acc: 0.9963 - val_loss: 2.8096 - val_acc: 0.6687\n",
            "Epoch 445/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0086 - acc: 0.9963 - val_loss: 3.0016 - val_acc: 0.6676\n",
            "Epoch 446/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0099 - acc: 0.9959 - val_loss: 2.9203 - val_acc: 0.6620\n",
            "Epoch 447/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0090 - acc: 0.9962 - val_loss: 2.9837 - val_acc: 0.6654\n",
            "Epoch 448/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0063 - acc: 0.9970 - val_loss: 3.1573 - val_acc: 0.6690\n",
            "Epoch 449/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0081 - acc: 0.9964 - val_loss: 3.0655 - val_acc: 0.6648\n",
            "Epoch 450/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0087 - acc: 0.9963 - val_loss: 2.9729 - val_acc: 0.6662\n",
            "Epoch 451/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0077 - acc: 0.9969 - val_loss: 3.1171 - val_acc: 0.6620\n",
            "Epoch 452/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0098 - acc: 0.9963 - val_loss: 2.9127 - val_acc: 0.6648\n",
            "Epoch 453/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9963 - val_loss: 3.0809 - val_acc: 0.6648\n",
            "Epoch 454/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0100 - acc: 0.9960 - val_loss: 2.9708 - val_acc: 0.6687\n",
            "Epoch 455/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 2.9550 - val_acc: 0.6648\n",
            "Epoch 456/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0091 - acc: 0.9963 - val_loss: 2.7654 - val_acc: 0.6757\n",
            "Epoch 457/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0086 - acc: 0.9965 - val_loss: 2.8171 - val_acc: 0.6712\n",
            "Epoch 458/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0070 - acc: 0.9970 - val_loss: 2.9376 - val_acc: 0.6687\n",
            "Epoch 459/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0079 - acc: 0.9970 - val_loss: 2.9960 - val_acc: 0.6687\n",
            "Epoch 460/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0073 - acc: 0.9970 - val_loss: 2.9364 - val_acc: 0.6695\n",
            "Epoch 461/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0110 - acc: 0.9958 - val_loss: 2.9086 - val_acc: 0.6693\n",
            "Epoch 462/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0083 - acc: 0.9965 - val_loss: 3.0918 - val_acc: 0.6693\n",
            "Epoch 463/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0092 - acc: 0.9967 - val_loss: 3.0203 - val_acc: 0.6707\n",
            "Epoch 464/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0083 - acc: 0.9967 - val_loss: 3.0187 - val_acc: 0.6665\n",
            "Epoch 465/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0099 - acc: 0.9966 - val_loss: 2.8930 - val_acc: 0.6623\n",
            "Epoch 466/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0081 - acc: 0.9968 - val_loss: 2.9905 - val_acc: 0.6732\n",
            "Epoch 467/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0087 - acc: 0.9963 - val_loss: 2.9002 - val_acc: 0.6648\n",
            "Epoch 468/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0104 - acc: 0.9959 - val_loss: 2.8052 - val_acc: 0.6623\n",
            "Epoch 469/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0083 - acc: 0.9964 - val_loss: 2.9019 - val_acc: 0.6643\n",
            "Epoch 470/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0084 - acc: 0.9963 - val_loss: 3.0020 - val_acc: 0.6645\n",
            "Epoch 471/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0069 - acc: 0.9971 - val_loss: 3.0430 - val_acc: 0.6684\n",
            "Epoch 472/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0076 - acc: 0.9967 - val_loss: 3.0994 - val_acc: 0.6707\n",
            "Epoch 473/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 2.9715 - val_acc: 0.6682\n",
            "Epoch 474/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0190 - acc: 0.9928 - val_loss: 2.9008 - val_acc: 0.6617\n",
            "Epoch 475/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0108 - acc: 0.9960 - val_loss: 2.8647 - val_acc: 0.6654\n",
            "Epoch 476/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0096 - acc: 0.9963 - val_loss: 2.8606 - val_acc: 0.6695\n",
            "Epoch 477/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0073 - acc: 0.9968 - val_loss: 3.0160 - val_acc: 0.6693\n",
            "Epoch 478/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0061 - acc: 0.9974 - val_loss: 2.9601 - val_acc: 0.6684\n",
            "Epoch 479/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0076 - acc: 0.9966 - val_loss: 2.9353 - val_acc: 0.6665\n",
            "Epoch 480/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0075 - acc: 0.9965 - val_loss: 2.9715 - val_acc: 0.6668\n",
            "Epoch 481/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0084 - acc: 0.9961 - val_loss: 3.0541 - val_acc: 0.6693\n",
            "Epoch 482/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0063 - acc: 0.9972 - val_loss: 3.0515 - val_acc: 0.6659\n",
            "Epoch 483/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0067 - acc: 0.9969 - val_loss: 3.0242 - val_acc: 0.6668\n",
            "Epoch 484/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0094 - acc: 0.9965 - val_loss: 2.9042 - val_acc: 0.6676\n",
            "Epoch 485/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0066 - acc: 0.9971 - val_loss: 3.0031 - val_acc: 0.6701\n",
            "Epoch 486/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0077 - acc: 0.9963 - val_loss: 2.9960 - val_acc: 0.6732\n",
            "Epoch 487/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0064 - acc: 0.9972 - val_loss: 3.0929 - val_acc: 0.6682\n",
            "Epoch 488/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0062 - acc: 0.9971 - val_loss: 2.9950 - val_acc: 0.6684\n",
            "Epoch 489/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0075 - acc: 0.9966 - val_loss: 2.9053 - val_acc: 0.6693\n",
            "Epoch 490/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0066 - acc: 0.9972 - val_loss: 3.0171 - val_acc: 0.6687\n",
            "Epoch 491/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0063 - acc: 0.9973 - val_loss: 3.2839 - val_acc: 0.6665\n",
            "Epoch 492/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0091 - acc: 0.9960 - val_loss: 3.0936 - val_acc: 0.6631\n",
            "Epoch 493/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0075 - acc: 0.9967 - val_loss: 3.1888 - val_acc: 0.6576\n",
            "Epoch 494/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0074 - acc: 0.9967 - val_loss: 3.0882 - val_acc: 0.6631\n",
            "Epoch 495/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0078 - acc: 0.9969 - val_loss: 3.0152 - val_acc: 0.6584\n",
            "Epoch 496/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0081 - acc: 0.9965 - val_loss: 3.0781 - val_acc: 0.6640\n",
            "Epoch 497/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 2.9225 - val_acc: 0.6662\n",
            "Epoch 498/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0099 - acc: 0.9969 - val_loss: 3.0246 - val_acc: 0.6598\n",
            "Epoch 499/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0093 - acc: 0.9959 - val_loss: 3.0627 - val_acc: 0.6573\n",
            "Epoch 500/500\n",
            "28709/28709 [==============================] - 54s 2ms/step - loss: 0.0096 - acc: 0.9962 - val_loss: 3.0715 - val_acc: 0.6584\n",
            "28709/28709 [==============================] - 23s 784us/step\n",
            "Accuracy: 99.80842244592289%\n",
            "Train Loss: 0.005556794312042552\n",
            "3589/3589 [==============================] - 3s 786us/step\n",
            "Accuracy: 65.84006687431622%\n",
            "Validation Loss: 3.071509134340964\n",
            "3589/3589 [==============================] - 3s 782us/step\n",
            "Accuracy: 66.78740596432445%\n",
            "Test Loss: 2.981639945895667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlbhaNy70AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z33NTy10oBaM",
        "colab_type": "text"
      },
      "source": [
        "Saving the model for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6DpYhmBoBGH",
        "colab_type": "code",
        "outputId": "5f7a68ed-9e9f-448c-b893-102af4621e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2929
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "number_of_classes = 7\n",
        "dimension = 48\n",
        "number_of_channels = 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(48, 48 ,1), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 30\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "adam = Adam(decay=decay)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "history=model.fit(train_X, train_Y, epochs=epochs, batch_size=128,validation_data=(val_X, val_Y))\n",
        "train_loss, test_acc = model.evaluate(train_X, train_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "train_loss, test_acc = model.evaluate(val_X, val_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "test_loss, test_acc = model.evaluate(test_X, test_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Test Loss: \" + repr(test_loss))\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "model.save('30epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4096)              18878464  \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 28679     \n",
            "=================================================================\n",
            "Total params: 26,545,095\n",
            "Trainable params: 26,543,175\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/30\n",
            "28709/28709 [==============================] - 33s 1ms/step - loss: 2.7265 - acc: 0.2220 - val_loss: 6.7404 - val_acc: 0.1819\n",
            "Epoch 2/30\n",
            "28709/28709 [==============================] - 26s 896us/step - loss: 1.7960 - acc: 0.2506 - val_loss: 1.7593 - val_acc: 0.2850\n",
            "Epoch 3/30\n",
            "28709/28709 [==============================] - 26s 906us/step - loss: 1.7500 - acc: 0.2866 - val_loss: 1.7016 - val_acc: 0.3346\n",
            "Epoch 4/30\n",
            "28709/28709 [==============================] - 26s 916us/step - loss: 1.6923 - acc: 0.3253 - val_loss: 1.7159 - val_acc: 0.3179\n",
            "Epoch 5/30\n",
            "28709/28709 [==============================] - 27s 923us/step - loss: 1.6031 - acc: 0.3659 - val_loss: 1.5293 - val_acc: 0.4032\n",
            "Epoch 6/30\n",
            "28709/28709 [==============================] - 27s 928us/step - loss: 1.5009 - acc: 0.4142 - val_loss: 1.4607 - val_acc: 0.4305\n",
            "Epoch 7/30\n",
            "28709/28709 [==============================] - 27s 934us/step - loss: 1.3995 - acc: 0.4540 - val_loss: 1.6919 - val_acc: 0.3840\n",
            "Epoch 8/30\n",
            "28709/28709 [==============================] - 27s 938us/step - loss: 1.3221 - acc: 0.4914 - val_loss: 1.4895 - val_acc: 0.4787\n",
            "Epoch 9/30\n",
            "28709/28709 [==============================] - 27s 944us/step - loss: 1.2692 - acc: 0.5098 - val_loss: 1.3075 - val_acc: 0.5040\n",
            "Epoch 10/30\n",
            "28709/28709 [==============================] - 27s 949us/step - loss: 1.2026 - acc: 0.5377 - val_loss: 1.4291 - val_acc: 0.4946\n",
            "Epoch 11/30\n",
            "28709/28709 [==============================] - 27s 952us/step - loss: 1.1496 - acc: 0.5627 - val_loss: 1.2129 - val_acc: 0.5366\n",
            "Epoch 12/30\n",
            "28709/28709 [==============================] - 27s 950us/step - loss: 1.0986 - acc: 0.5835 - val_loss: 1.4741 - val_acc: 0.5511\n",
            "Epoch 13/30\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 1.0565 - acc: 0.5973 - val_loss: 1.1557 - val_acc: 0.5762\n",
            "Epoch 14/30\n",
            "28709/28709 [==============================] - 27s 956us/step - loss: 1.0083 - acc: 0.6171 - val_loss: 1.1387 - val_acc: 0.5743\n",
            "Epoch 15/30\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 0.9609 - acc: 0.6376 - val_loss: 1.2239 - val_acc: 0.5690\n",
            "Epoch 16/30\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 0.9127 - acc: 0.6533 - val_loss: 1.1267 - val_acc: 0.5971\n",
            "Epoch 17/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.8686 - acc: 0.6718 - val_loss: 1.0875 - val_acc: 0.5954\n",
            "Epoch 18/30\n",
            "28709/28709 [==============================] - 27s 952us/step - loss: 0.8101 - acc: 0.6959 - val_loss: 1.2951 - val_acc: 0.5940\n",
            "Epoch 19/30\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 0.7614 - acc: 0.7112 - val_loss: 1.1199 - val_acc: 0.6110\n",
            "Epoch 20/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.7062 - acc: 0.7345 - val_loss: 1.2086 - val_acc: 0.5968\n",
            "Epoch 21/30\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 0.6517 - acc: 0.7558 - val_loss: 1.1267 - val_acc: 0.6119\n",
            "Epoch 22/30\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 0.6106 - acc: 0.7730 - val_loss: 1.1485 - val_acc: 0.6305\n",
            "Epoch 23/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.5579 - acc: 0.7938 - val_loss: 1.1934 - val_acc: 0.6314\n",
            "Epoch 24/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.5103 - acc: 0.8098 - val_loss: 1.1625 - val_acc: 0.6205\n",
            "Epoch 25/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.4585 - acc: 0.8334 - val_loss: 1.3158 - val_acc: 0.6314\n",
            "Epoch 26/30\n",
            "28709/28709 [==============================] - 27s 955us/step - loss: 0.4167 - acc: 0.8463 - val_loss: 1.3202 - val_acc: 0.6358\n",
            "Epoch 27/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.3761 - acc: 0.8624 - val_loss: 1.3161 - val_acc: 0.6297\n",
            "Epoch 28/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.3417 - acc: 0.8761 - val_loss: 1.4543 - val_acc: 0.6386\n",
            "Epoch 29/30\n",
            "28709/28709 [==============================] - 27s 954us/step - loss: 0.3126 - acc: 0.8878 - val_loss: 1.3509 - val_acc: 0.6342\n",
            "Epoch 30/30\n",
            "28709/28709 [==============================] - 27s 953us/step - loss: 0.2911 - acc: 0.8954 - val_loss: 1.4441 - val_acc: 0.6364\n",
            "28709/28709 [==============================] - 10s 335us/step\n",
            "Accuracy: 97.18555156919433%\n",
            "Train Loss: 0.11153832957625551\n",
            "3589/3589 [==============================] - 1s 327us/step\n",
            "Accuracy: 63.63889663024811%\n",
            "Validation Loss: 1.4440577629383775\n",
            "3589/3589 [==============================] - 1s 329us/step\n",
            "Accuracy: 64.78127612314306%\n",
            "Test Loss: 1.3965097842823666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk4QsuBtjTBKy4G6MMUnIgrsxxiSh/w8s4NRYyXNJCQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AVa3tW_mvHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model, to_file='vgg.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJq4lKoRlqkM",
        "colab_type": "text"
      },
      "source": [
        "Simple CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYe4vu2ElqWZ",
        "colab_type": "code",
        "outputId": "b7c67aef-35e5-4ca1-e8c0-07467477dbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1078
        }
      },
      "source": [
        "from keras.layers.convolutional import AveragePooling2D\n",
        "model = Sequential()\n",
        "\n",
        "#1st convolution layer\n",
        "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
        "model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
        "\n",
        "#2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "\n",
        "#3rd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#fully connected neural networks\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(train_X, train_Y, epochs=10, batch_size=128,validation_data=(val_X, val_Y))\n",
        "train_loss, test_acc = model.evaluate(train_X, train_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "train_loss, test_acc = model.evaluate(val_X, val_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "test_loss, test_acc = model.evaluate(test_X, test_Y)\n",
        "print(\"Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Test Loss: \" + repr(test_loss))\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "model.save('cnn.h5')\n",
        "plot_model(model, to_file='cnn.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/10\n",
            "28709/28709 [==============================] - 5s 176us/step - loss: 1.7710 - acc: 0.2612 - val_loss: 1.6835 - val_acc: 0.3093\n",
            "Epoch 2/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.5980 - acc: 0.3592 - val_loss: 1.5058 - val_acc: 0.4149\n",
            "Epoch 3/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.4556 - acc: 0.4289 - val_loss: 1.4213 - val_acc: 0.4533\n",
            "Epoch 4/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.3761 - acc: 0.4664 - val_loss: 1.3500 - val_acc: 0.4840\n",
            "Epoch 5/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.3122 - acc: 0.4964 - val_loss: 1.3144 - val_acc: 0.4943\n",
            "Epoch 6/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.2582 - acc: 0.5197 - val_loss: 1.2765 - val_acc: 0.5149\n",
            "Epoch 7/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.2183 - acc: 0.5391 - val_loss: 1.2588 - val_acc: 0.5244\n",
            "Epoch 8/10\n",
            "28709/28709 [==============================] - 4s 135us/step - loss: 1.1803 - acc: 0.5541 - val_loss: 1.2145 - val_acc: 0.5428\n",
            "Epoch 9/10\n",
            "28709/28709 [==============================] - 4s 136us/step - loss: 1.1502 - acc: 0.5631 - val_loss: 1.2021 - val_acc: 0.5444\n",
            "Epoch 10/10\n",
            "28709/28709 [==============================] - 4s 136us/step - loss: 1.1175 - acc: 0.5773 - val_loss: 1.1896 - val_acc: 0.5511\n",
            "28709/28709 [==============================] - 2s 82us/step\n",
            "Accuracy: 59.347243024939225%\n",
            "Train Loss: 1.076187484104675\n",
            "3589/3589 [==============================] - 0s 82us/step\n",
            "Accuracy: 55.11284480439683%\n",
            "Validation Loss: 1.1896375184870922\n",
            "3589/3589 [==============================] - 0s 82us/step\n",
            "Accuracy: 55.39147394859016%\n",
            "Test Loss: 1.1649810864288317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niJK+iEga+f8QFpn+Z4BgsAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VOXZ//HPxSIYQPaqiBCXKvsa\nEUVl0VpUUFGqIrg9KkJda21dcFcqtdQNqYoW0RJBfii17vZRHtGqICiyiIhLghFkE1AMVgPX7497\nEhLIMiGTOcnk+3695jUzZ+4555oJXOee+9yLuTsiIpJaakUdgIiIJJ6Su4hIClJyFxFJQUruIiIp\nSMldRCQFKbmLiKQgJXcplpnVNrMtZtYmkWWjZGYHm1nC+/6a2XFmllXo+XIzOzqesrtxrMfM7Ibd\nfX8p+73TzKYker8SnTpRByCJYWZbCj1NA/4LbIs9v8TdM8uzP3ffBjRMdNmawN0PTcR+zOwiYIS7\n9yu074sSsW9JfUruKcLdC5JrrGZ4kbv/b0nlzayOu+clIzYRST41y9QQsZ/dT5vZNDP7HhhhZkeY\n2XtmtsnMVpvZA2ZWN1a+jpm5maXHnk+Nvf6ymX1vZu+a2QHlLRt7/QQz+9TMNpvZBDP7j5mdX0Lc\n8cR4iZl9ZmYbzeyBQu+tbWb3mtkGM/sCGFjK9zPGzKbvtG2imd0Te3yRmS2LfZ7PY7XqkvaVY2b9\nYo/TzOwfsdiWAj13KnujmX0R2+9SMzs5tr0z8CBwdKzJa32h7/bWQu8fFfvsG8zsn2a2bzzfTVnM\nbEgsnk1m9oaZHVrotRvMbJWZfWdmnxT6rL3N7IPY9jVm9pd4jyeVwN11S7EbkAUct9O2O4GfgMGE\nk/qewGHA4YRfcAcCnwKXxcrXARxIjz2fCqwHMoC6wNPA1N0o+wvge+CU2GtXAz8D55fwWeKJ8Tmg\nMZAOfJv/2YHLgKVAa6A5MCf8ky/2OAcCW4AGhfa9FsiIPR8cK2PAAGAr0CX22nFAVqF95QD9Yo/H\nA/8HNAXaAh/vVPYMYN/Y3+TsWAx7x167CPi/neKcCtwae3x8LMZuQH3gb8Ab8Xw3xXz+O4Epscft\nY3EMiP2NbgCWxx53BLKBfWJlDwAOjD1+HxgWe9wIODzq/ws1+aaae83ytrs/7+7b3X2ru7/v7nPd\nPc/dvwAmAX1Lef9Md5/v7j8DmYSkUt6yg4CF7v5c7LV7CSeCYsUZ413uvtndswiJNP9YZwD3unuO\nu28AxpVynC+AJYSTDsCvgI3uPj/2+vPu/oUHbwCvA8VeNN3JGcCd7r7R3bMJtfHCx53h7qtjf5On\nCCfmjDj2CzAceMzdF7r7j8B1QF8za12oTEnfTWnOAv7l7m/E/kbjCCeIw4E8womkY6xp78vYdwfh\nJP1LM2vu7t+7+9w4P4dUAiX3muWrwk/MrJ2ZvWhm35jZd8DtQItS3v9Noce5lH4RtaSyrQrH4e5O\nqOkWK84Y4zoWocZZmqeAYbHHZ8ee58cxyMzmmtm3ZraJUGsu7bvKt29pMZjZ+Wb2Uaz5YxPQLs79\nQvh8Bftz9++AjcB+hcqU529W0n63E/5G+7n7cuD3hL/D2lgz3z6xohcAHYDlZjbPzE6M83NIJVBy\nr1l27gb4CKG2erC77wXcTGh2qEyrCc0kAJiZUTQZ7awiMa4G9i/0vKyumjOA48xsP0IN/qlYjHsC\nM4G7CE0mTYDX4ozjm5JiMLMDgYeA0UDz2H4/KbTfsrptriI09eTvrxGh+efrOOIqz35rEf5mXwO4\n+1R370NokqlN+F5w9+Xufhah6e2vwDNmVr+CschuUnKv2RoBm4EfzKw9cEkSjvkC0MPMBptZHeBK\noGUlxTgDuMrM9jOz5sC1pRV292+At4EpwHJ3XxF7qR6wB7AO2GZmg4BjyxHDDWbWxMI4gMsKvdaQ\nkMDXEc5zFxNq7vnWAK3zLyAXYxpwoZl1MbN6hCT7lruX+EuoHDGfbGb9Ysf+A+E6yVwza29m/WPH\n2xq7bSd8gHPMrEWspr859tm2VzAW2U1K7jXb74HzCP9xHyFc+KxU7r4GOBO4B9gAHAR8SOiXn+gY\nHyK0jS8mXOybGcd7niJcIC1oknH3TcDvgFmEi5JDCSepeNxC+AWRBbwMPFlov4uACcC8WJlDgcLt\n1P8GVgBrzKxw80r++18hNI/Mir2/DaEdvkLcfSnhO3+IcOIZCJwca3+vB9xNuE7yDeGXwpjYW08E\nllnojTUeONPdf6poPLJ7LDR5ikTDzGoTmgGGuvtbUccjkipUc5ekM7OBsWaKesBNhF4W8yIOSySl\nKLlLFI4CviD85P81MMTdS2qWEZHdUGazjJlNJvRNXuvunYp5vTFhYEUbwsCJ8e7+eCXEKiIicYqn\n5j6FUoZtA5cCH7t7V6Af8Fcz26PioYmIyO4qc+Iwd59jsTlDSioCNIr1V25I6E1Q5oRULVq08PT0\n0nYrIiI7W7BgwXp3L637MJCYWSEfBP5F6PHQiND9qdi+rWY2EhgJ0KZNG+bPn5+Aw4uI1BxmVtZI\nayAxF1R/DSwkDFnuBjxoZnsVV9DdJ7l7hrtntGxZ5olHRER2UyKS+wXAs7EJlT4DvqToKDsREUmy\nRCT3lcSGYpvZ3oRRdl+U+g4REalUZba5m9k0Qi+YFmaWQxhOXRfA3R8G7gCmmNliwoRH17p7iVO4\nikg0fv75Z3Jycvjxxx+jDkXiUL9+fVq3bk3duiVNLVS6eHrLDCvj9VWE6U9FpArLycmhUaNGpKen\nEzq3SVXl7mzYsIGcnBwOOOCAst9QjGo1QjUzE9LToVatcJ9ZriWfRWq2H3/8kebNmyuxVwNmRvPm\nzSv0K6vaLJCdmQkjR0JubnienR2eAwyv8Dx4IjWDEnv1UdG/VbWpuY8ZsyOx58vNDdtFRKSoapPc\nV64s33YRqVo2bNhAt27d6NatG/vssw/77bdfwfOffopv2vcLLriA5cuXl1pm4sSJZCaozfaoo45i\n4cKFCdlXslWbZpk2bUJTTHHbRSTxMjPDL+OVK8P/s7FjK9YE2rx584JEeeutt9KwYUOuueaaImXc\nHXenVq3i652PP172nISXXnrp7geZQqpNzX3sWEhLK7otLS1sF5HEyr/GlZ0N7juucVVGJ4bPPvuM\nDh06MHz4cDp27Mjq1asZOXIkGRkZdOzYkdtvv72gbH5NOi8vjyZNmnDdddfRtWtXjjjiCNauXQvA\njTfeyH333VdQ/rrrrqNXr14ceuihvPPOOwD88MMPnH766XTo0IGhQ4eSkZFRZg196tSpdO7cmU6d\nOnHDDTcAkJeXxznnnFOw/YEHHgDg3nvvpUOHDnTp0oURI0Yk/DuLR7WpuefXGBJZkxCR4pV2jasy\n/s998sknPPnkk2RkZAAwbtw4mjVrRl5eHv3792fo0KF06NChyHs2b95M3759GTduHFdffTWTJ0/m\nuuuu22Xf7s68efP417/+xe23384rr7zChAkT2GeffXjmmWf46KOP6NGjR6nx5eTkcOONNzJ//nwa\nN27McccdxwsvvEDLli1Zv349ixcvBmDTpk0A3H333WRnZ7PHHnsUbEu2alNzh/CPKisLtm8P90rs\nIpUj2de4DjrooILEDjBt2jR69OhBjx49WLZsGR9//PEu79lzzz054YQTAOjZsydZWVnF7vu0007b\npczbb7/NWWedBUDXrl3p2LFjqfHNnTuXAQMG0KJFC+rWrcvZZ5/NnDlzOPjgg1m+fDlXXHEFr776\nKo0bNwagY8eOjBgxgszMzN0ehFRR1Sq5i0hylHQtq7KucTVo0KDg8YoVK7j//vt54403WLRoEQMH\nDiy2v/cee+xYNqJ27drk5RU/03i9evXKLLO7mjdvzqJFizj66KOZOHEil1xyCQCvvvoqo0aN4v33\n36dXr15s27YtoceNh5K7iOwiymtc3333HY0aNWKvvfZi9erVvPrqqwk/Rp8+fZgxYwYAixcvLvaX\nQWGHH344s2fPZsOGDeTl5TF9+nT69u3LunXrcHd+85vfcPvtt/PBBx+wbds2cnJyGDBgAHfffTfr\n168nd+c2riSoNm3uIpI8UV7j6tGjBx06dKBdu3a0bduWPn36JPwYl19+Oeeeey4dOnQouOU3qRSn\ndevW3HHHHfTr1w93Z/DgwZx00kl88MEHXHjhhbg7Zsaf//xn8vLyOPvss/n+++/Zvn0711xzDY0a\nNUr4ZyhLmWuoVpaMjAzXYh0iybNs2TLat28fdRhVQl5eHnl5edSvX58VK1Zw/PHHs2LFCurUqVr1\n3eL+Zma2wN0zSnhLgar1SUREkmDLli0ce+yx5OXl4e488sgjVS6xV1RqfRoRkTg0adKEBQsWRB1G\npdIFVRGRFKTkLiKSgpTcRURSkJK7iEgKUnIXkaTo37//LgOS7rvvPkaPHl3q+xo2bAjAqlWrGDp0\naLFl+vXrR1ldq++7774ig4lOPPHEhMz7cuuttzJ+/PgK7yfRlNxFJCmGDRvG9OnTi2ybPn06w4aV\nukxzgVatWjFz5szdPv7Oyf2ll16iSZMmu72/qk7JXUSSYujQobz44osFC3NkZWWxatUqjj766IJ+\n5z169KBz584899xzu7w/KyuLTp06AbB161bOOuss2rdvz5AhQ9i6dWtBudGjRxdMF3zLLbcA8MAD\nD7Bq1Sr69+9P//79AUhPT2f9+vUA3HPPPXTq1IlOnToVTBeclZVF+/btufjii+nYsSPHH398keMU\nZ+HChfTu3ZsuXbowZMgQNm7cWHD8/CmA8ycse/PNNwsWK+nevTvff//9bn+3xVE/d5Ea6KqrINEL\nDHXrBrG8WKxmzZrRq1cvXn75ZU455RSmT5/OGWecgZlRv359Zs2axV577cX69evp3bs3J598conr\niD700EOkpaWxbNkyFi1aVGTK3rFjx9KsWTO2bdvGsccey6JFi7jiiiu45557mD17Ni1atCiyrwUL\nFvD4448zd+5c3J3DDz+cvn370rRpU1asWMG0adN49NFHOeOMM3jmmWdKnZ/93HPPZcKECfTt25eb\nb76Z2267jfvuu49x48bx5ZdfUq9evYKmoPHjxzNx4kT69OnDli1bqF+/fjm+7bKVWXM3s8lmttbM\nlpTw+h/MbGHstsTMtplZs4RGKSIpoXDTTOEmGXfnhhtuoEuXLhx33HF8/fXXrFmzpsT9zJkzpyDJ\ndunShS5duhS8NmPGDHr06EH37t1ZunRpmZOCvf322wwZMoQGDRrQsGFDTjvtNN566y0ADjjgALp1\n6waUPq0whPnlN23aRN++fQE477zzmDNnTkGMw4cPZ+rUqQUjYfv06cPVV1/NAw88wKZNmxI+Qjae\nvU0BHgSeLO5Fd/8L8BcAMxsM/M7dv01UgCKSeKXVsCvTKaecwu9+9zs++OADcnNz6dmzJwCZmZms\nW7eOBQsWULduXdLT04ud5rcsX375JePHj+f999+nadOmnH/++bu1n3z50wVDmDK4rGaZkrz44ovM\nmTOH559/nrFjx7J48WKuu+46TjrpJF566SX69OnDq6++Srt27XY71p2VWXN39zlAvMl6GDCtQhGJ\nSMpq2LAh/fv353/+53+KXEjdvHkzv/jFL6hbty6zZ88mu7gFkws55phjeOqppwBYsmQJixYtAsJ0\nwQ0aNKBx48asWbOGl19+ueA9jRo1KrZd++ijj+af//wnubm5/PDDD8yaNYujjz663J+tcePGNG3a\ntKDW/49//IO+ffuyfft2vvrqK/r378+f//xnNm/ezJYtW/j888/p3Lkz1157LYcddhiffPJJuY9Z\nmoT9DjCzNGAgcFkpZUYCIwHaaGVrkRpp2LBhDBkypEjPmeHDhzN48GA6d+5MRkZGmTXY0aNHc8EF\nF9C+fXvat29f8Auga9eudO/enXbt2rH//vsXmS545MiRDBw4kFatWjF79uyC7T169OD888+nV69e\nAFx00UV079691CaYkjzxxBOMGjWK3NxcDjzwQB5//HG2bdvGiBEj2Lx5M+7OFVdcQZMmTbjpppuY\nPXs2tWrVomPHjgWrSiVKXFP+mlk68IK7dyqlzJnACHcfHM+BNeWvSHJpyt/qpyJT/iayK+RZqElG\nRKRKSEhyN7PGQF9g186plaCM5jgRkRovnq6Q04B3gUPNLMfMLjSzUWY2qlCxIcBr7v5DZQWa7x//\ngIMPTnwfXZGaIKqV16T8Kvq3KvOCqruXOTbY3acQukxWukGDoFkzuPhieO89qF07GUcVqf7q16/P\nhg0baN68eYmDg6RqcHc2bNhQoYFN1W6EatOmoY/u2WfDxIlwxRVRRyRSPbRu3ZqcnBzWrVsXdSgS\nh/r169O6devdfn+1XCDbHU48Ed5+Gz7+GPbfP8HBiYhUUVH0lkkaM/jb32DbNrj88qijERGpeqpl\ncgc44AC47TZ47jmYNSvqaEREqpZqm9whzGzXtWuovX/3XdTRiIhUHdU6udetC5MmwapVMGZM1NGI\niFQd1Tq5A/TqBZddFnrOzJ0bdTQiIlVDtU/uAHfeCa1ahb7vP/8cdTQiItFLieS+117w4IOweDHc\nc0/U0YiIRC8lkjvAqaeG2223wRdfRB2NiEi0Uia5A0yYAHXqwOjRYaCTiEhNlVLJvXVrGDsWXnsN\npmnyYRGpwVIquQP89rehB81VV8G3WslVRGqolEvutWuHvu/ffgt//GPU0YiIRCPlkjuEUatXXw1/\n/zvMmRN1NCIiyZeSyR3gllsgPR1GjoT//jfqaEREkitlk3uDBvDQQ7B8OYwbF3U0IiLJlbLJHWDg\nQDjrLPjTn+CTT6KORkQkeVI6uUNYtSktDUaNUt93Eak5Uj6577033H03vPkmPP541NGIiCRHyid3\ngAsvhKOOgmuugbVro45GRKTylZnczWyyma01syWllOlnZgvNbKmZvZnYECuuVq3Q933LltBFUkQk\n1cVTc58CDCzpRTNrAvwNONndOwK/SUxoidW+PVx3HWRmwr//XbF9ZWaGbpa1aoX7zMxERCgikjhl\nJnd3nwOUNpD/bOBZd18ZK19lGz5uuAEOOSRcXM3N3b19ZGaGvvPZ2eECbXZ2eK4ELyJVSSLa3A8B\nmprZ/5nZAjM7t6SCZjbSzOab2fx169Yl4NDlU78+PPxwmBL4jjt2bx9jxux6YsjN1TJ/IlK1JCK5\n1wF6AicBvwZuMrNDiivo7pPcPcPdM1q2bJmAQ5df//5w/vkwfnxY3KO8Vq4s33YRkSgkIrnnAK+6\n+w/uvh6YA3RNwH4rzfjx0KRJaE7Zvr18723TpnzbRUSikIjk/hxwlJnVMbM04HBgWQL2W2maNw/L\n8b33XmimKY+xY8OgqMLS0sJ2EZGqIp6ukNOAd4FDzSzHzC40s1FmNgrA3ZcBrwCLgHnAY+5eYrfJ\nqmLECDj2WLj+eli1Kv73DR8eulW2bQtm4X7SpLBdRKSqMI9oTH5GRobPnz8/kmPn++wz6NwZTjoJ\nZs6MNBQRkbiY2QJ3zyirXI0YoVqSgw+Gm2+GZ56B55+POhoRkcSp0ckdwpQEnTrBpZeGEawiIqmg\nxif3unVDm3lODtx0U9TRiIgkRo1P7gBHHBFGrT7wACxYEHU0IiIVp+Qec9ddYXrgiy+GvLyooxER\nqRgl95jGjUPN/cMPw72ISHWm5F7I6afDoEGh7T07O+poRER2n5J7IWYwcWK4v/RSLcsnItWXkvtO\n2rQJM0a++KIGNolI9aXkXozLL4cePeCKK2DTpqijEREpPyX3YtSpA48+GtZbvf76qKMRESk/JfcS\n9OgBV14ZZo18552ooxERKR8l91Lcfntogx85En76KepoRETip+ReioYNQ++ZpUvDAh8iItWFknsZ\nBg2CoUNDLf6zz6KORkQkPkrucbj/fqhXL8w/o77vIlIdKLnHoVUrGDcOXn8dpk6NOhoRkbIpucfp\nkkvC7JFXXw3r10cdjYhI6ZTc41SrVpj3fdMm+MMfoo5GRKR0Su7l0KlTSOxTpsDs2VFHIyJSMiX3\ncrrpJjjooNBM8+OPUUcjIlK8MpO7mU02s7VmtqSE1/uZ2WYzWxi73Zz4MKuOPfcMo1ZXrICxY6OO\nRkSkeHXiKDMFeBB4spQyb7n7oIREVA0cdxycc86O5H7rrVC7dqQhiYgUUWZyd/c5ZpZe+aFUL488\nAnvsAXfeCW+/DU89BfvuG3VUIiJBotrcjzCzj8zsZTPrWFIhMxtpZvPNbP66desSdOho7LknPPZY\nuLg6dy507w5vvBF1VCIiQSKS+wdAW3fvCkwA/llSQXef5O4Z7p7RsmXLBBw6euedB++/D82aheaa\n22+HbduijkpEaroKJ3d3/87dt8QevwTUNbMWFY6sGunYEebNgxEj4JZb4IQTwlzwIiJRqXByN7N9\nzMxij3vF9rmhovutbho2hCeeCIt8vPVWaKaZMyfqqESkpoqnK+Q04F3gUDPLMbMLzWyUmY2KFRkK\nLDGzj4AHgLPca+b0WmZw0UXw3nvQoAEMGBDmpNm+PerIRKSmsajycEZGhs+fPz+SYyfDd9+FRT6e\nfhpOPBGefBKaN486KhGp7sxsgbtnlFVOI1QryV57wbRpYbGP//1f6NZNy/WJSPIouVciM/jtb0NS\n32MP6NsX/vpXzQkvIpVPyT0JevaEBQtg8GC45ho49VTYuDHqqEQklSm5J0mTJvDMM3DfffDSS9Cj\nR+gfLyJSGZTck8gMrrwyTFewfTv06QMTJqiZRkQST8k9AocfDh9+CL/+NVxxBZxxBmzeXP79ZGZC\nenpYSCQ9PTwXEQEl98g0awbPPQd33w2zZoV2+Q8/jP/9mZmhq2V2dqj5Z2eH50rwIgJK7pGqVSus\n7PTmm2HhjyOOCHPFx9NMM2YM5OYW3ZabG7aLiCi5VwF9+oRae79+MHo0DB8O339f+ntWrizfdhGp\nWZTcq4iWLUMvmjvvDKNaMzJg8eKSy7dpU77tIlKzKLlXIbVqhWaV118P0xf06gWTJxffTDN2LKSl\nFd2Wlqal/0QkUHKvgvr1g4UL4cgj4cIL4YIL4IcfipYZPhwmTYK2bUMXy7Ztw/PhwyMJWUSqGE0c\nVoVt2wZ33BEWAGnfHmbODPciUnNp4rAUULt2WHz71Vdh3brQDj91atRRiUh1oOReDfzqV6GZpmdP\nOOccuPhi2Lo16qhEpCpTcq8mWrUKC3Bff31YmLt3b/j006ijEpGqSsm9GqlTB/70J3jxRcjJCTX5\np5+OOioRqYqU3KuhE08MzTSdO8NZZ4VpB3ZnbhoRSV1K7tXU/vuHaQv+8IfQTNOhQ5irRkQElNyr\ntbp1w8Rj770X1mc99VQYOhRWr446MhGJmpJ7CujVK6z0NHYsvPBC6Av/6KNhzngRqZnKTO5mNtnM\n1prZkjLKHWZmeWY2NHHhSbzq1oUbboBFi8Ji3CNHQv/+sHx51JGJSBTiqblPAQaWVsDMagN/Bl5L\nQExSAYccErpMPvpoSPRdu4Ya/U8/RR2ZiCRTmcnd3ecA35ZR7HLgGWBtIoKSiqlVCy66CJYtg5NP\nhhtvDN0m586NOjIRSZYKt7mb2X7AEOChOMqONLP5ZjZ/3bp1FT20lGGffWDGjNCLZuPGsBjIlVeW\nPVe8iFR/ibigeh9wrbuXefnO3Se5e4a7Z7Rs2TIBh5Z4nHwyfPxxWAhkwgTo2DEMhBKR1JWI5J4B\nTDezLGAo8DczOzUB+5UE2msvmDgR3n4bGjWCQYNg2DBYq4Y0kZRU4eTu7ge4e7q7pwMzgd+6+z8r\nHJlUiiOPhA8+gNtug2efDd0mp0yJb91WEak+4ukKOQ14FzjUzHLM7EIzG2Vmoyo/PKkM9erBzTeH\nKQzatw+LgfzqV/D551FHJiKJEk9vmWHuvq+713X31u7+d3d/2N0fLqbs+e4+s3JClURr3x7mzIGH\nHoJ586BTpzDiNS+vfPvJzIT09NBLJz09PBeRaGmEag1XqxaMGhW6Tf7613DttXDYYWHEazwyM8OA\nqezs0LSTnR2eK8GLREvJXQDYbz+YNSss5ffNN2FKg2uu2XXt1p2NGQO5uUW35eaG7SISHSV3KWAG\np58eavEXXQR//WuYVvi1UsYdr1xZvu0ikhxK7rKLJk3gkUfClMJ164bmmnPPhfXrdy3bpk3x+yhp\nu4gkh5K7lOiYY+Cjj8L0BdOmhQuwmZlFu02OHQtpaUXfl5YWtotIdJTcpVT168Mdd4S+8QcdBCNG\nwAknQFZWeH34cJg0Cdq2Dc06bduG58OHRxq2SI2n5C5x6dwZ/vMfeOCBcN+xI9x7L2zbFhJ5VlaY\nPz4rS4ldpCpQcpe41a4Nl18OS5eGueKvvhp69w5NNyJStSi5S7m1aQPPPw/Tp4deMT17wvXX79ol\nUkSio+Quu8UMzjwzdJs891wYNw5+8YuwhutTT8HmzVFHKFKzKblLhTRrBpMnh3b4c88N98OHQ8uW\n4cLrpEmwZk3UUYrUPOYRTQeYkZHh8+fPj+TYUnm2bw8rPj37bBjx+vnnoZbfpw8MGRJuBxwQdZQi\n1ZeZLXD3jDLLKblLZXGHxYtDkp81a8eF127dQpI/7bTQ68Ys2jhFqhMld6lyvvhiR6J/552Q/A8+\neEei79UrTGQmIiVTcpcq7Ztvwtquzz4Lb7wRphlu1QpOPTUk+759w9QHIlKUkrtUG5s2hTVdn30W\nXnkldKls2hQGDw6J/vjjd53iQKSmUnKXaik3N8xCOWtW6Eu/cWNI7AMHhkQ/aFCY2EykplJyl2rv\n55/DzJT57fSrV0OdOjBgQEj0p54K++wTdZQiyRVvctflK6my6taF446DiRMhJwfefTdMefDFFzB6\ndGij79MHxo8PXS613J/IDqq5S7XjHua3mTUrtNMvXBi2mxWdjjgtTTNUSupRs4zUGF9+Gea32bhx\n19datYKvv05+TCKVRc0yUmMccEDocVOcVavCoKm77gpNNyI1RZnJ3cwmm9laM1tSwuunmNkiM1to\nZvPN7KjEhylSupKW9WvaNDTP3HBDGDCVkQF/+QtkZyc3PpFki6fmPgUYWMrrrwNd3b0b8D/AYwmI\nS6RcSlrub8KEMBo2Kysk9VqluPi1AAALQ0lEQVS14I9/DBdce/cOC47k5EQRsUjlKjO5u/sc4NtS\nXt/iOxruGwDRNOJLjVbWcn9t28I118C8eaF55q674L//Db1v9t8fjjoqnAhWr472c4gkSlwXVM0s\nHXjB3TuV8PoQ4C7gF8BJ7v5uCeVGAiMB2rRp0zNbv40lYp9+CjNmwNNPw5Il4cTQty+ccQacfnqY\no16kKklob5myknuhcscAN7v7cWXtU71lpKr5+OMdif6TT0ITzoABIdGfdho0bx51hCIR9ZaJNeEc\naGYtErlfkWTo0AFuvTUk+Y8+CksHZmXByJFhJOwJJ8CUKSX3zBGpSiqc3M3sYLMwI7eZ9QDqARsq\nul+RqJhBly5w552h2WbBAvj970Nt/oILQlPN4MEwdSp8913U0YoUr05ZBcxsGtAPaGFmOcAtQF0A\nd38YOB0418x+BrYCZ3pUI6NEEswMevQIt7vugvffD802M2bACy9AvXqhRn/mmWFSs4YNo45YJNAI\nVZHdsH07vPdeSPT/7/+FXjZ77gknnRQS/YknappiqRyafkAkSbZvh7ffDol+5kxYuzYk+g4doH37\ncGvXLtwffLAWIZGKUXIXiUBeHsyZE5psli6FZcvgq692vF6nTkjw+ck+/3boodCoUXRxS/URb3Iv\ns81dROKXP9/8gAE7tm3ZAsuXh0Sff/vkk3ACyMvbUa51611r+u3bhwu4WkRcykvJXSTBMjNhzBhY\nuTLMeTN2bBgp27Nn0XI//xxGyxZO+MuWweTJ4YSQr2nTosk+/3F6OtSundSPJtWImmVEEigzM/SL\nz83dsa2888q7h/lu8pN94eS/Zs2OcvXqheacnRP/IYeENn9JTWpzF4lAenrxM062bRsGRFXUt9/u\nSPqFk/+XX+5YqMQsTIPcvj107QrnnBOSvqQGJXeRCNSqVXQ1qHxmoVdNZdm6FVas2LWm//HHoV3/\nqKPCL4qhQ1Wrr+6U3EUiUNk19/JaswaeeAIefRQ++wyaNIERI+Dii8MoXKl+tBKTSARKmld+7Nho\n4tl77zB//aefwuzZYTTtpEmhuaZ3b/j734tevJXUoeQukkBlzSsfFTPo1w+eeiosPXjvvfD993DR\nRbDvvnDJJWEOHUkdapYRqaHc4d13w8lnxozQbt+9e2iyOftsaNw46gilOGqWEZFSmcGRR4ZpjFet\ngokTw0Xf3/4WWrUKM2C++27xF4il6lNyFxGaNAlJ/cMPw1KEw4eHeXKOPBI6d4b77w/dMKX6UHIX\nkQJmcNhhoalm1arQyyYtDa66KtTmR4yAN99Ubb46UHIXkWI1ahQuuM6bBwsXhscvvBAuzLZrB3/5\nS5gBU6omJXeRFJSZGfrc16oV7jMzK7a/rl3hwQdDbX7KFGjZMnSxbN06rDH7739X7iAtKT8ld5EU\nkz+/TXZ2aD7Jzg7PK5rgITTRnHdemL9+6VK47DJ4/XU4/vgwlfHYseEEINFTV0iRFJPsUbI//giz\nZoX2+dmzw0yVgwaFLpUDB2rmykRTV0iRGmrlyvJtr6j69WHYMHjjjTAS9ve/D10oBw0KJ5pbboEl\nS+CHHyrn+FI81dxFUkxVmN/mp5/g+edDbf6113b0rmnSBPbfP7TVl3TfoEFyYqyutBKTSA01dmzx\nc8onc36bPfaA008Pt6ys0EafkxNuX30V7ufPh3Xrdn1v06ZlnwC0+HjZykzuZjYZGASsdfdOxbw+\nHLgWMOB7YLS7f5ToQEUkPvnz2BS3GlQU0tPDrTg//ghff70j4e98P28erF+/6/uaNSs9+esEEEez\njJkdA2wBniwhuR8JLHP3jWZ2AnCrux9e1oHVLCMi8di6NZwAdk7+hR9v2LDr+5o3L/sEUB3ntk9Y\ns4y7zzGz9FJef6fQ0/eA1vEEKCISjz33DN0sDz645DJbt+7a7FP4/t13Sz4BlNYEtN9+1fMEAIlv\nc78QeLmkF81sJDASoE2bNgk+tIjUVHvuCb/8ZbiVJDe35Cagr76Cd94pfv6cFi3K/gVQv37lfbbd\nlbDkbmb9Ccn9qJLKuPskYBKEZplEHVtEpCxpafGdAEr6BZCdDf/5T8kngLJ+AST7BJCQ5G5mXYDH\ngBPcvZgfPyIiVV9aGhxySLiV5IcfSv4FkJ0degZt3Ljr+1q23JHszzwzzJlfmSqc3M2sDfAscI67\nf1rxkEQkVWRmVp1eO4nSoEF8J4CSfgF8+SWsXl35ccbTFXIa0A9oYWY5wC1AXQB3fxi4GWgO/M3M\nAPLiuZIrIqktf46b/P72+XPcQPVP8GVp0AAOPTTcoqIRqiJSKarCSNlUpLllRCRSyZ7jRopScheR\nSlFSb2f1gk4OJXcRqRRjx+46BUCy57ipyZTcRaRSDB8e1mJt2zaszdq2bXie6hdTqwrNCikilWb4\ncCXzqKjmLiKSgpTcRSTlJXrB8OpAzTIiktJq6mAq1dxFJKWNGVN0VSoIz8eMiSaeZFFyF5GUVlMH\nUym5i0hKq6mDqZTcRSSl1dTBVEruIpLSaupgKvWWEZGUVxMHU6nmLiKSgpTcRURSkJK7iEiSJHOk\nrNrcRUSSINkjZVVzFxFJgmSPlFVyFxFJgmSPlFVyFxFJgmSPlC0zuZvZZDNba2ZLSni9nZm9a2b/\nNbNrEh+iiEj1l+yRsvHU3KcAA0t5/VvgCmB8IgISEUlFyR4pW2ZvGXefY2bppby+FlhrZiclMC4R\nkZSTzJGySW1zN7ORZjbfzOavW7cumYcWEalRkprc3X2Su2e4e0bLli2TeWgRkRpFvWVERFKQkruI\nSAoq84KqmU0D+gEtzCwHuAWoC+DuD5vZPsB8YC9gu5ldBXRw9+8qLWoRESmVuXs0BzZbB2RHcvDE\naQGsjzqIKkTfR1H6PnbQd1FURb6Ptu5e5kXLyJJ7KjCz+e6eEXUcVYW+j6L0feyg76KoZHwfanMX\nEUlBSu4iIilIyb1iJkUdQBWj76MofR876LsoqtK/D7W5i4ikINXcRURSkJK7iEgKUnLfDWa2v5nN\nNrOPzWypmV0ZdUxRM7PaZvahmb0QdSxRM7MmZjbTzD4xs2VmdkTUMUXJzH4X+3+yxMymmVn9qGNK\npuLWxDCzZmb2bzNbEbtvmujjKrnvnjzg9+7eAegNXGpmHSKOKWpXAsuiDqKKuB94xd3bAV2pwd+L\nme1HWO8hw907AbWBs6KNKummsOuaGNcBr7v7L4HXY88TSsl9N7j7anf/IPb4e8J/3v2ijSo6ZtYa\nOAl4LOpYomZmjYFjgL8DuPtP7r4p2qgiVwfY08zqAGnAqojjSSp3n0NY1KiwU4AnYo+fAE5N9HGV\n3CsotpBJd2ButJFE6j7gj8D2qAOpAg4A1gGPx5qpHjOzBlEHFRV3/5qwSttKYDWw2d1fizaqKmFv\nd18de/wNsHeiD6DkXgFm1hB4Briqpk6UZmaDgLXuviDqWKqIOkAP4CF37w78QCX85K4uYm3JpxBO\neq2ABmY2ItqoqhYP/dET3iddyX03mVldQmLPdPdno44nQn2Ak80sC5gODDCzqdGGFKkcIMfd83/J\nzSQk+5rqOOBLd1/n7j8DzwJHRhxTVbDGzPYFiN2vTfQBlNx3g5kZoU11mbvfE3U8UXL36929tbun\nEy6UveHuNbZm5u7fAF+Z2aGxTccCH0cYUtRWAr3NLC32/+ZYavAF5kL+BZwXe3we8FyiD6Dkvnv6\nAOcQaqkLY7cTow5KqozLgUwzWwR0A/4UcTyRif2CmQl8ACwm5JwaNRVBbE2Md4FDzSzHzC4ExgG/\nMrMVhF834xJ+XE0/ICKSelRzFxFJQUruIiIpSMldRCQFKbmLiKQgJXcRkRSk5C4ikoKU3EVEUtD/\nB6JxVxLz5CRXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xs_ttri_kRF",
        "colab_type": "text"
      },
      "source": [
        "Using Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzy2A8_0CRtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainx=np.asarray(trainx)\n",
        "valx=np.asarray(valx)\n",
        "testx=np.asarray(testx)\n",
        "train_X = trainx.reshape(28709, 48, 48,1)\n",
        "val_X = valx.reshape(3589,48, 48,1)\n",
        "test_X = testx.reshape(3589,48, 48,1)  \n",
        "train_Y=trainy\n",
        "val_Y=valy\n",
        "test_Y=testy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQh4QG5E_obU",
        "colab_type": "code",
        "outputId": "bfdfad24-654a-4a9c-9de9-1b289c98d18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3735
        }
      },
      "source": [
        "img_rows, img_cols = 48, 48\n",
        "batch_size = 64\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.3,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.3,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "datagen.fit(train_X)\n",
        "\n",
        "train_generator=datagen.flow(train_X, train_Y,batch_size=batch_size)\n",
        "\n",
        "validation_generator = valid_datagen.flow(val_X,val_Y,batch_size=batch_size)\n",
        "\n",
        "test_generator = test_datagen.flow(test_X,test_Y,batch_size=batch_size)\n",
        "\n",
        "number_of_classes = 7\n",
        "dimension = 48\n",
        "number_of_channels = 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(48, 48 ,1), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 100\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "adam = Adam(decay=decay)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit_generator(train_generator,\n",
        "                    samples_per_epoch=train_X.shape[0],\n",
        "                    nb_epoch=epochs,\n",
        "                    validation_data=(val_X, val_Y))\n",
        "\n",
        "train_loss, test_acc = model.evaluate_generator(generator=train_generator,steps=batch_size)\n",
        "print(\"Train Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Train Loss: \" + repr(train_loss))\n",
        "\n",
        "train_loss, test_acc = model.evaluate_generator(generator=validation_generator,steps=batch_size)\n",
        "print(\"Validation Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Validation Loss: \" + repr(train_loss))\n",
        "\n",
        "train_loss, test_acc = model.evaluate_generator(generator=test_generator,steps=batch_size)\n",
        "print(\"Testing Accuracy: \"+ repr(test_acc*100) + '%')\n",
        "print(\"Testing Loss: \" + repr(train_loss))\n",
        "\n",
        "# model.save('100epochs.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., steps_per_epoch=448, epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "448/448 [==============================] - 39s 87ms/step - loss: 2.8375 - acc: 0.2267 - val_loss: 3.6475 - val_acc: 0.1736\n",
            "Epoch 2/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.9959 - acc: 0.2421 - val_loss: 1.8537 - val_acc: 0.2563\n",
            "Epoch 3/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.8063 - acc: 0.2486 - val_loss: 1.8759 - val_acc: 0.2513\n",
            "Epoch 4/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.8034 - acc: 0.2485 - val_loss: 1.8517 - val_acc: 0.2577\n",
            "Epoch 5/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7977 - acc: 0.2555 - val_loss: 1.8874 - val_acc: 0.2566\n",
            "Epoch 6/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7939 - acc: 0.2524 - val_loss: 1.8917 - val_acc: 0.2608\n",
            "Epoch 7/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7843 - acc: 0.2604 - val_loss: 1.9745 - val_acc: 0.2722\n",
            "Epoch 8/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7733 - acc: 0.2715 - val_loss: 1.7559 - val_acc: 0.2953\n",
            "Epoch 9/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7580 - acc: 0.2806 - val_loss: 2.2665 - val_acc: 0.1605\n",
            "Epoch 10/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7552 - acc: 0.2853 - val_loss: 1.9324 - val_acc: 0.2193\n",
            "Epoch 11/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7953 - acc: 0.2531 - val_loss: 1.8081 - val_acc: 0.2639\n",
            "Epoch 12/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7804 - acc: 0.2646 - val_loss: 1.8321 - val_acc: 0.2889\n",
            "Epoch 13/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7574 - acc: 0.2837 - val_loss: 1.7684 - val_acc: 0.2842\n",
            "Epoch 14/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.7183 - acc: 0.3044 - val_loss: 1.7502 - val_acc: 0.2914\n",
            "Epoch 15/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.6253 - acc: 0.3495 - val_loss: 1.5445 - val_acc: 0.3904\n",
            "Epoch 16/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.5411 - acc: 0.3889 - val_loss: 1.4572 - val_acc: 0.4274\n",
            "Epoch 17/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.4870 - acc: 0.4146 - val_loss: 1.4350 - val_acc: 0.4324\n",
            "Epoch 18/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.4434 - acc: 0.4344 - val_loss: 1.3661 - val_acc: 0.4547\n",
            "Epoch 19/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.4137 - acc: 0.4455 - val_loss: 1.4929 - val_acc: 0.4288\n",
            "Epoch 20/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.3783 - acc: 0.4662 - val_loss: 1.3142 - val_acc: 0.4876\n",
            "Epoch 21/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.3217 - acc: 0.4926 - val_loss: 1.2483 - val_acc: 0.5252\n",
            "Epoch 22/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.2832 - acc: 0.5122 - val_loss: 1.2223 - val_acc: 0.5366\n",
            "Epoch 23/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.2494 - acc: 0.5283 - val_loss: 1.1591 - val_acc: 0.5475\n",
            "Epoch 24/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.2190 - acc: 0.5374 - val_loss: 1.1704 - val_acc: 0.5489\n",
            "Epoch 25/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.1962 - acc: 0.5450 - val_loss: 1.2476 - val_acc: 0.5567\n",
            "Epoch 26/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.1836 - acc: 0.5536 - val_loss: 1.1521 - val_acc: 0.5606\n",
            "Epoch 27/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.1651 - acc: 0.5622 - val_loss: 1.0875 - val_acc: 0.5765\n",
            "Epoch 28/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.1473 - acc: 0.5687 - val_loss: 1.0998 - val_acc: 0.5910\n",
            "Epoch 29/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.1188 - acc: 0.5786 - val_loss: 1.0756 - val_acc: 0.6013\n",
            "Epoch 30/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.1135 - acc: 0.5845 - val_loss: 1.0759 - val_acc: 0.5935\n",
            "Epoch 31/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0973 - acc: 0.5907 - val_loss: 1.0494 - val_acc: 0.6004\n",
            "Epoch 32/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0713 - acc: 0.5966 - val_loss: 1.0507 - val_acc: 0.6052\n",
            "Epoch 33/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0608 - acc: 0.6056 - val_loss: 1.1721 - val_acc: 0.5991\n",
            "Epoch 34/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 1.0489 - acc: 0.6079 - val_loss: 1.0604 - val_acc: 0.5974\n",
            "Epoch 35/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0414 - acc: 0.6121 - val_loss: 1.0921 - val_acc: 0.5899\n",
            "Epoch 36/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0240 - acc: 0.6183 - val_loss: 1.9293 - val_acc: 0.5748\n",
            "Epoch 37/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0283 - acc: 0.6185 - val_loss: 1.0296 - val_acc: 0.6166\n",
            "Epoch 38/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0070 - acc: 0.6270 - val_loss: 1.0503 - val_acc: 0.6152\n",
            "Epoch 39/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 1.0001 - acc: 0.6286 - val_loss: 1.0457 - val_acc: 0.6085\n",
            "Epoch 40/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9860 - acc: 0.6316 - val_loss: 1.0199 - val_acc: 0.6094\n",
            "Epoch 41/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9868 - acc: 0.6351 - val_loss: 1.0132 - val_acc: 0.6317\n",
            "Epoch 42/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9658 - acc: 0.6399 - val_loss: 1.0176 - val_acc: 0.6208\n",
            "Epoch 43/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9499 - acc: 0.6470 - val_loss: 0.9738 - val_acc: 0.6389\n",
            "Epoch 44/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9567 - acc: 0.6466 - val_loss: 0.9876 - val_acc: 0.6445\n",
            "Epoch 45/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9350 - acc: 0.6499 - val_loss: 0.9817 - val_acc: 0.6464\n",
            "Epoch 46/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9267 - acc: 0.6561 - val_loss: 0.9847 - val_acc: 0.6428\n",
            "Epoch 47/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9309 - acc: 0.6544 - val_loss: 0.9787 - val_acc: 0.6425\n",
            "Epoch 48/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9159 - acc: 0.6625 - val_loss: 1.0655 - val_acc: 0.6174\n",
            "Epoch 49/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.9079 - acc: 0.6618 - val_loss: 1.0733 - val_acc: 0.6216\n",
            "Epoch 50/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8944 - acc: 0.6722 - val_loss: 0.9811 - val_acc: 0.6495\n",
            "Epoch 51/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8856 - acc: 0.6726 - val_loss: 1.0028 - val_acc: 0.6403\n",
            "Epoch 52/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8842 - acc: 0.6725 - val_loss: 1.0179 - val_acc: 0.6303\n",
            "Epoch 53/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8685 - acc: 0.6762 - val_loss: 1.0147 - val_acc: 0.6500\n",
            "Epoch 54/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8835 - acc: 0.6741 - val_loss: 1.0706 - val_acc: 0.6297\n",
            "Epoch 55/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8859 - acc: 0.6706 - val_loss: 1.0170 - val_acc: 0.6447\n",
            "Epoch 56/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8560 - acc: 0.6841 - val_loss: 0.9916 - val_acc: 0.6464\n",
            "Epoch 57/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8594 - acc: 0.6843 - val_loss: 1.0094 - val_acc: 0.6475\n",
            "Epoch 58/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8391 - acc: 0.6877 - val_loss: 0.9882 - val_acc: 0.6548\n",
            "Epoch 59/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8350 - acc: 0.6911 - val_loss: 1.0093 - val_acc: 0.6500\n",
            "Epoch 60/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8310 - acc: 0.6930 - val_loss: 0.9891 - val_acc: 0.6503\n",
            "Epoch 61/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8188 - acc: 0.6961 - val_loss: 1.0287 - val_acc: 0.6498\n",
            "Epoch 62/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8140 - acc: 0.6994 - val_loss: 0.9949 - val_acc: 0.6503\n",
            "Epoch 63/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8072 - acc: 0.7023 - val_loss: 1.0088 - val_acc: 0.6484\n",
            "Epoch 64/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.8060 - acc: 0.7042 - val_loss: 1.0185 - val_acc: 0.6498\n",
            "Epoch 65/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7928 - acc: 0.7089 - val_loss: 0.9870 - val_acc: 0.6567\n",
            "Epoch 66/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.7938 - acc: 0.7088 - val_loss: 1.0047 - val_acc: 0.6528\n",
            "Epoch 67/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7912 - acc: 0.7095 - val_loss: 1.0196 - val_acc: 0.6612\n",
            "Epoch 68/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7825 - acc: 0.7130 - val_loss: 1.0268 - val_acc: 0.6534\n",
            "Epoch 69/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7716 - acc: 0.7163 - val_loss: 1.0047 - val_acc: 0.6662\n",
            "Epoch 70/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7609 - acc: 0.7203 - val_loss: 1.0309 - val_acc: 0.6545\n",
            "Epoch 71/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7639 - acc: 0.7163 - val_loss: 0.9867 - val_acc: 0.6645\n",
            "Epoch 72/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7505 - acc: 0.7221 - val_loss: 1.0444 - val_acc: 0.6578\n",
            "Epoch 73/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7490 - acc: 0.7235 - val_loss: 1.0296 - val_acc: 0.6445\n",
            "Epoch 74/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7408 - acc: 0.7298 - val_loss: 1.0568 - val_acc: 0.6620\n",
            "Epoch 75/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7410 - acc: 0.7291 - val_loss: 1.0053 - val_acc: 0.6617\n",
            "Epoch 76/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7324 - acc: 0.7279 - val_loss: 1.0431 - val_acc: 0.6668\n",
            "Epoch 77/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7248 - acc: 0.7331 - val_loss: 1.0311 - val_acc: 0.6629\n",
            "Epoch 78/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7271 - acc: 0.7344 - val_loss: 1.0097 - val_acc: 0.6626\n",
            "Epoch 79/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7161 - acc: 0.7349 - val_loss: 1.0545 - val_acc: 0.6631\n",
            "Epoch 80/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7100 - acc: 0.7413 - val_loss: 1.0803 - val_acc: 0.6637\n",
            "Epoch 81/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7118 - acc: 0.7375 - val_loss: 1.0675 - val_acc: 0.6679\n",
            "Epoch 82/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.7153 - acc: 0.7367 - val_loss: 1.0619 - val_acc: 0.6612\n",
            "Epoch 83/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6932 - acc: 0.7454 - val_loss: 1.0227 - val_acc: 0.6718\n",
            "Epoch 84/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6977 - acc: 0.7458 - val_loss: 1.0226 - val_acc: 0.6715\n",
            "Epoch 85/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.6839 - acc: 0.7482 - val_loss: 1.0761 - val_acc: 0.6584\n",
            "Epoch 86/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6901 - acc: 0.7471 - val_loss: 1.0335 - val_acc: 0.6673\n",
            "Epoch 87/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6763 - acc: 0.7513 - val_loss: 1.0589 - val_acc: 0.6562\n",
            "Epoch 88/100\n",
            "448/448 [==============================] - 35s 77ms/step - loss: 0.6628 - acc: 0.7567 - val_loss: 1.0661 - val_acc: 0.6701\n",
            "Epoch 89/100\n",
            "448/448 [==============================] - 35s 78ms/step - loss: 0.6739 - acc: 0.7522 - val_loss: 1.1192 - val_acc: 0.6517\n",
            "Epoch 90/100\n",
            "448/448 [==============================] - 35s 78ms/step - loss: 0.6644 - acc: 0.7557 - val_loss: 1.0454 - val_acc: 0.6612\n",
            "Epoch 91/100\n",
            "448/448 [==============================] - 35s 78ms/step - loss: 0.6674 - acc: 0.7536 - val_loss: 1.0971 - val_acc: 0.6684\n",
            "Epoch 92/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.6489 - acc: 0.7631 - val_loss: 1.1184 - val_acc: 0.6612\n",
            "Epoch 93/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6397 - acc: 0.7621 - val_loss: 1.0679 - val_acc: 0.6707\n",
            "Epoch 94/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6438 - acc: 0.7639 - val_loss: 1.0450 - val_acc: 0.6726\n",
            "Epoch 95/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.6438 - acc: 0.7611 - val_loss: 1.0924 - val_acc: 0.6670\n",
            "Epoch 96/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.6270 - acc: 0.7714 - val_loss: 1.0958 - val_acc: 0.6746\n",
            "Epoch 97/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.6294 - acc: 0.7702 - val_loss: 1.0877 - val_acc: 0.6656\n",
            "Epoch 98/100\n",
            "448/448 [==============================] - 34s 77ms/step - loss: 0.6298 - acc: 0.7686 - val_loss: 1.1211 - val_acc: 0.6670\n",
            "Epoch 99/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6284 - acc: 0.7696 - val_loss: 1.0833 - val_acc: 0.6832\n",
            "Epoch 100/100\n",
            "448/448 [==============================] - 34s 76ms/step - loss: 0.6221 - acc: 0.7719 - val_loss: 1.0973 - val_acc: 0.6668\n",
            "Train Accuracy: 81.396484375%\n",
            "Train Loss: 0.5224839365109801\n",
            "Validation Accuracy: 13.0790190765224%\n",
            "Validation Loss: 2.3497751134735894\n",
            "Testing Accuracy: 14.06985385260843%\n",
            "Testing Loss: 2.3406888276065434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGlx1QYehhc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('100epochs_dataaug.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgFDw8cEjsIL",
        "colab_type": "code",
        "outputId": "dfecba42-8fad-4844-fc52-23e2a3a627ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "predictions = model.predict_generator(generator=validation_generator,steps=batch_size)\n",
        "predictions[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5436558 , 0.01235714, 0.03024716, 0.09112439, 0.1732681 ,\n",
              "       0.02195545, 0.12739193], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coxkpjbiqrUS",
        "colab_type": "code",
        "outputId": "cf08d71d-2ce2-4975-8144-f5e1442f19e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('100epochs.h5')\n",
        "\n",
        "predicted_output=model.predict(test_X[0].reshape(1,48,48,1))\n",
        "predicted_output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL6QzAIKiUI3",
        "colab_type": "text"
      },
      "source": [
        "Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KSnZYKsiVxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import os\n",
        "import cv2\n",
        "from numpy import dstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "import pickle\n",
        "\n",
        "\n",
        "def loaddataset(filename):\n",
        "  data = pd.read_csv(filename)\n",
        "  pixels = data['pixels'].tolist()\n",
        "  width, height = 48, 48\n",
        "  faces = []\n",
        "  for pixel_sequence in pixels:\n",
        "      face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "      face = np.asarray(face)#.reshape(width, height)\n",
        "  #     face = cv2.resize(face.astype('uint8'),(48,48))\n",
        "      faces.append(face.astype('float32'))\n",
        "  faces = np.asarray(faces)\n",
        "  faces = np.expand_dims(faces, -1)\n",
        "  emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
        "  return faces,emotions\n",
        "trainx,trainy = loaddataset('Train_Data.csv')\n",
        "testx,testy=loaddataset('Test_Data.csv')\n",
        "valx,valy=loaddataset('Validation_Data.csv')\n",
        "\n",
        "trainx1=trainx.astype('float32') / 255\n",
        "mean, std = np.mean(trainx1), np.std(trainx1)\n",
        "train_X = np.asarray([(np.array(xi)-mean) for xi in trainx1])\n",
        "\n",
        "testx1=testx.astype('float32') / 255\n",
        "mean, std = np.mean(testx1), np.std(testx1)\n",
        "test_X = np.asarray([(np.array(xi)-mean) for xi in testx1])\n",
        "\n",
        "valx1=valx.astype('float32') / 255\n",
        "mean, std = np.mean(valx1), np.std(valx1)\n",
        "val_X = np.asarray([(np.array(xi)-mean) for xi in valx1])\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48)\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48, 1)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48,1)\n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48)\n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48, 1)\n",
        "\n",
        "train_Y=[np.where(r==1)[0][0] for r in trainy]\n",
        "val_Y=[np.where(r==1)[0][0] for r in valy]\n",
        "test_Y=[np.where(r==1)[0][0] for r in testy]\n",
        "\n",
        "vgg_model = load_model('30epochs.h5')\n",
        "ex_model = load_model('traindata_mini_XCEPTION.61-0.63.hdf5')\n",
        "cnn_model=load_model('cnn.h5')\n",
        "# resnet_model=load_model('resnet.h5')\n",
        "# inception_model=load_model('inception.h5')\n",
        "members = [resnet_model]\n",
        "\n",
        "# def get_data(dataset):\n",
        "#     file_stream = file_io.FileIO(dataset, mode='r')\n",
        "#     data = pd.read_csv(file_stream)\n",
        "#     pixels = data['pixels'].tolist()\n",
        "#     images = np.empty((len(data), img_height, img_width, 3))\n",
        "#     i = 0\n",
        "\n",
        "#     for pixel_sequence in pixels:\n",
        "#         single_image = [float(pixel) for pixel in pixel_sequence.split(' ')]  # Extraction of each single\n",
        "#         single_image = np.asarray(single_image).reshape(48, 48) # Dimension: 48x48\n",
        "#         single_image = resize(single_image, (img_height, img_width), order = 3, mode = 'constant') # Dimension: 139x139x3 (Bicubic)\n",
        "#         ret = np.empty((img_height, img_width, 3))  \n",
        "#         ret[:, :, 0] = single_image\n",
        "#         ret[:, :, 1] = single_image\n",
        "#         ret[:, :, 2] = single_image\n",
        "#         images[i, :, :, :] = ret\n",
        "#         i += 1\n",
        "    \n",
        "#     images = preprocess_input(images)\n",
        "    \n",
        "\n",
        "#     return images \n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "from skimage.transform import resize\n",
        "def stacked_dataset(members, inputX):\n",
        "    stackX = None\n",
        "    for model in members:\n",
        "        # make prediction\n",
        "#         if model==resnet_model:\n",
        "#           for i in range(len(inputX)):\n",
        "#             inputX[i]=inputX[i].resize(197,197,1,mode = 'constant')\n",
        "#             inputX1 = np.empty((3589,197, 197))\n",
        "#             inputX1[i] = resize(inputX[i], (197, 197), mode = 'constant')\n",
        "#           yhat = model.predict(inputX1, verbose=0)\n",
        "#         if model==inception_model:\n",
        "#           for i in range(len(inputX)):\n",
        "#             inputX2 = np.empty((3589,197, 197, 3))\n",
        "#             inputX2[i] = resize(inputX[i], (139, 139),order=3, mode = 'constant')\n",
        "#             img2 = np.zeros_like(inputX2)\n",
        "#             img2[i,:,:,0] = inputX2[i]\n",
        "#             img2[i,:,:,1] = inputX2[i]\n",
        "#             img2[i,:,:,2] = inputX2[i]\n",
        "          \n",
        "#           yhat = model.predict(img2, verbose=0)\n",
        "#         else:\n",
        "        yhat = model.predict(inputX, verbose=0)\n",
        "        # stack predictions into [rows, members, prob$abilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "    return stackX\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(members, inputX)\n",
        "    # fit standalone model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(stackedX, inputy)\n",
        "    return model\n",
        "\n",
        "# fit stacked model using the ensemble\n",
        "model = fit_stacked_model(members, test_X, test_Y)\n",
        "\n",
        "# filename=('ensemblefinal.sav')\n",
        "# pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(members, inputX)\n",
        "    # make a prediction\n",
        "    yhat = model.predict(stackedX)\n",
        "    return yhat\n",
        "\n",
        "# # evaluate model on Train set\n",
        "# yhat = stacked_prediction(members, model, train_X)\n",
        "# acc = sklearn.metrics.accuracy_score(train_Y, yhat)\n",
        "# print('Stacked Train Accuracy: %.3f' % acc)\n",
        "\n",
        "# # evaluate model on Validation set\n",
        "# yhat = stacked_prediction(members, model, val_X)\n",
        "# acc = sklearn.metrics.accuracy_score(val_Y, yhat)\n",
        "# print('Stacked Validation Accuracy: %.3f' % acc)\n",
        "  \n",
        "# # evaluate model on Test set\n",
        "# yhat = stacked_prediction(members, model, test_X)\n",
        "# acc = sklearn.metrics.accuracy_score(test_Y, yhat)\n",
        "# print('Stacked Test Accuracy: %.3f' % acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QNw_dbX1_PB",
        "colab_type": "code",
        "outputId": "b126a52c-ae12-47fb-a5fa-125f46f55100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3589, 48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMlWv1ZmEt1F",
        "colab_type": "code",
        "outputId": "54df78a3-e136-4f7a-fac6-d83d5e09a8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import os\n",
        "import cv2\n",
        "from numpy import dstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "import pickle\n",
        "\n",
        "def loaddataset(filename):\n",
        "  data = pd.read_csv(filename)\n",
        "  pixels = data['pixels'].tolist()\n",
        "  width, height = 48, 48\n",
        "  faces = []\n",
        "  for pixel_sequence in pixels:\n",
        "      face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "      face = np.asarray(face)#.reshape(width, height)\n",
        "  #     face = cv2.resize(face.astype('uint8'),(48,48))\n",
        "      faces.append(face.astype('float32'))\n",
        "  faces = np.asarray(faces)\n",
        "  faces = np.expand_dims(faces, -1)\n",
        "  emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
        "  return faces,emotions\n",
        "trainx,trainy = loaddataset('Train_Data.csv')\n",
        "testx,testy=loaddataset('Test_Data.csv')\n",
        "valx,valy=loaddataset('Validation_Data.csv')\n",
        "\n",
        "trainx1=trainx.astype('float32') / 255\n",
        "mean, std = np.mean(trainx1), np.std(trainx1)\n",
        "train_X = np.asarray([(np.array(xi)-mean) for xi in trainx1])\n",
        "\n",
        "testx1=testx.astype('float32') / 255\n",
        "mean, std = np.mean(testx1), np.std(testx1)\n",
        "test_X = np.asarray([(np.array(xi)-mean) for xi in testx1])\n",
        "\n",
        "valx1=valx.astype('float32') / 255\n",
        "mean, std = np.mean(valx1), np.std(valx1)\n",
        "val_X = np.asarray([(np.array(xi)-mean) for xi in valx1])\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48)\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48, 1)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48,1)\n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48)\n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48, 1)\n",
        "\n",
        "train_Y=[np.where(r==1)[0][0] for r in trainy]\n",
        "val_Y=[np.where(r==1)[0][0] for r in valy]\n",
        "test_Y=[np.where(r==1)[0][0] for r in testy]\n",
        "\n",
        "\n",
        "def stacked_dataset(members, inputX):\n",
        "    stackX = None\n",
        "    for model in members:\n",
        "\n",
        "        yhat = model.predict(inputX, verbose=0)\n",
        "        # stack predictions into [rows, members, prob$abilities]\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "    return stackX\n",
        "  \n",
        "def stacked_prediction(members, model, inputX):\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(members, inputX)\n",
        "    # make a prediction\n",
        "    yhat = model.predict(stackedX)\n",
        "    return yhat\n",
        "vgg_model = load_model('30epochs.h5')\n",
        "ex_model = load_model('traindata_mini_XCEPTION.61-0.63.hdf5')\n",
        "cnn_model=load_model('cnn.h5')\n",
        "# resnet_model=load_model('resnet.h5')\n",
        "# inception_model=load_model('inception.h5')\n",
        "members = [vgg_model,ex_model,cnn_model]\n",
        "  \n",
        "model = pickle.load(open('ensembleone.sav', 'rb'))\n",
        "# evaluate model on Train set\n",
        "yhat = stacked_prediction(members, model, train_X)\n",
        "acc = sklearn.metrics.accuracy_score(train_Y, yhat)\n",
        "print('Stacked Train Accuracy: %.3f' % acc)\n",
        "\n",
        "# evaluate model on Validation set\n",
        "yhat = stacked_prediction(members, model, val_X)\n",
        "acc = sklearn.metrics.accuracy_score(val_Y, yhat)\n",
        "print('Stacked Validation Accuracy: %.3f' % acc)\n",
        "  \n",
        "# evaluate model on Test set\n",
        "yhat = stacked_prediction(members, model, test_X)\n",
        "acc = sklearn.metrics.accuracy_score(test_Y, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Stacked Train Accuracy: 0.917\n",
            "Stacked Validation Accuracy: 0.662\n",
            "Stacked Test Accuracy: 0.687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ShDTy80EqRz",
        "colab_type": "text"
      },
      "source": [
        "Inception Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBCCfqf2zlEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.transform import resize\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import os\n",
        "import cv2\n",
        "from numpy import dstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "import pickle\n",
        "\n",
        "def loaddataset(filename):\n",
        "  data = pd.read_csv(filename)\n",
        "  pixels = data['pixels'].tolist()\n",
        "  width, height = 48, 48\n",
        "  faces = []\n",
        "  for pixel_sequence in pixels:\n",
        "      face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "      face = np.asarray(face)#.reshape(width, height)\n",
        "  #     face = cv2.resize(face.astype('uint8'),(48,48))\n",
        "      faces.append(face.astype('float32'))\n",
        "  faces = np.asarray(faces)\n",
        "  faces = np.expand_dims(faces, -1)\n",
        "  emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
        "  return faces,emotions\n",
        "trainx,trainy = loaddataset('Train_Data.csv')\n",
        "testx,testy=loaddataset('Test_Data.csv')\n",
        "valx,valy=loaddataset('Validation_Data.csv')\n",
        "\n",
        "trainx1=trainx.astype('float32') / 255\n",
        "mean, std = np.mean(trainx1), np.std(trainx1)\n",
        "train_X = np.asarray([(np.array(xi)-mean) for xi in trainx1])\n",
        "\n",
        "testx1=testx.astype('float32') / 255\n",
        "mean, std = np.mean(testx1), np.std(testx1)\n",
        "test_X = np.asarray([(np.array(xi)-mean) for xi in testx1])\n",
        "\n",
        "valx1=valx.astype('float32') / 255\n",
        "mean, std = np.mean(valx1), np.std(valx1)\n",
        "val_X = np.asarray([(np.array(xi)-mean) for xi in valx1])\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0], 48, 48)\n",
        "# train_X = train_X.reshape(train_X.shape[0], 48, 48, 1)\n",
        "val_X = val_X.reshape(val_X.shape[0],48, 48)\n",
        "# val_X = val_X.reshape(val_X.shape[0],48, 48,1)\n",
        "test_X = test_X.reshape(test_X.shape[0],48, 48)\n",
        "# test_X = test_X.reshape(test_X.shape[0],48, 48, 1)\n",
        "\n",
        "train_Y=[np.where(r==1)[0][0] for r in trainy]\n",
        "val_Y=[np.where(r==1)[0][0] for r in valy]\n",
        "test_Y=[np.where(r==1)[0][0] for r in testy]\n",
        "\n",
        "img_height, img_width = 197, 197\n",
        "\n",
        "def stacked_dataset(members, inputX):\n",
        "    stackX = None\n",
        "    for model in members:\n",
        "        if model==resnet_model:\n",
        "          inputX = inputX.reshape(inputX.shape[0],48, 48)\n",
        "          images = np.empty((3589, img_height, img_width, 3))\n",
        "          for i in range(3589):\n",
        "            \n",
        "            single_image=inputX[i]\n",
        "            single_image = resize(single_image, (img_height, img_width), order = 3, mode = 'constant') # Dimension: 139x139x3 (Bicubic)\n",
        "            ret = np.empty((img_height, img_width, 3))  \n",
        "            ret[:, :, 0] = single_image\n",
        "            ret[:, :, 1] = single_image\n",
        "            ret[:, :, 2] = single_image\n",
        "            images[i, :, :, :] = ret\n",
        "          \n",
        "          yhat = model.predict(images, verbose=0)\n",
        "        elif model==inception_model:\n",
        "          inputX = inputX.reshape(inputX.shape[0],48, 48)\n",
        "          images1 = np.empty((3589, img_height, img_width, 3))\n",
        "          for i in range(3589):\n",
        "            \n",
        "            single_image=inputX[i]\n",
        "            single_image = resize(single_image, (139, 139), order = 3, mode = 'constant') # Dimension: 139x139x3 (Bicubic)\n",
        "            ret = np.empty((img_height, img_width, 3))  \n",
        "            ret[:, :, 0] = single_image\n",
        "            ret[:, :, 1] = single_image\n",
        "            ret[:, :, 2] = single_image\n",
        "            images1[i, :, :, :] = ret\n",
        "          \n",
        "          yhat = model.predict(images1, verbose=0)\n",
        "        else:\n",
        "          inputX = inputX.reshape(inputX.shape[0],48, 48, 1)\n",
        "          yhat = model.predict(inputX, verbose=0)\n",
        "        # stack predictions into [rows, members, prob$abilities]\n",
        "        inputX = inputX.reshape(inputX.shape[0],48, 48, 1)\n",
        "        if stackX is None:\n",
        "            stackX = yhat\n",
        "        else:\n",
        "            stackX = dstack((stackX, yhat))\n",
        "    # flatten predictions to [rows, members x probabilities]\n",
        "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "    return stackX\n",
        "  \n",
        "def stacked_prediction(members, model, inputX):\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(members, inputX)\n",
        "    # make a prediction\n",
        "    yhat = model.predict(stackedX)\n",
        "    return yhat\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "    # create dataset using ensemble\n",
        "    stackedX = stacked_dataset(members, inputX)\n",
        "    # fit standalone model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(stackedX, inputy)\n",
        "    return model\n",
        "\n",
        "\n",
        "# vgg_model = load_model('30epochs.h5')\n",
        "# ex_model = load_model('traindata_mini_XCEPTION.61-0.63.hdf5')\n",
        "cnn_model=load_model('cnn.h5')\n",
        "resnet_model=load_model('resnet.h5')\n",
        "inception_model=load_model('inception.h5')\n",
        "# inceptionres=load_model('inceptionres.h5')\n",
        "members = [inception_model,resnet_model]\n",
        "\n",
        "\n",
        "# fit stacked model using the ensemble\n",
        "model = fit_stacked_model(members, test_X, test_Y)\n",
        "\n",
        "yhat = stacked_prediction(members, model, train_X)\n",
        "acc = sklearn.metrics.accuracy_score(train_Y, yhat)\n",
        "print('Stacked Train Accuracy: %.3f' % acc)\n",
        "\n",
        "# evaluate model on Validation set\n",
        "yhat = stacked_prediction(members, model, val_X)\n",
        "acc = sklearn.metrics.accuracy_score(val_Y, yhat)\n",
        "print('Stacked Validation Accuracy: %.3f' % acc)\n",
        "  \n",
        "# evaluate model on Test set\n",
        "yhat = stacked_prediction(members, model, test_X)\n",
        "acc = sklearn.metrics.accuracy_score(test_Y, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeQyXRfbtQME",
        "colab_type": "text"
      },
      "source": [
        "Stacking Method 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO6oIizdtXe2",
        "colab_type": "code",
        "outputId": "5277f268-a369-4891-b1a5-e0742a1d93c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import os\n",
        "import cv2\n",
        "from numpy import dstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "from keras.layers import concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# def loaddataset(filename):\n",
        "#   data = pd.read_csv(filename)\n",
        "#   pixels = data['pixels'].tolist()\n",
        "#   width, height = 48, 48\n",
        "#   faces = []\n",
        "#   for pixel_sequence in pixels:\n",
        "#       face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "#       face = np.asarray(face)#.reshape(width, height)\n",
        "#   #     face = cv2.resize(face.astype('uint8'),(48,48))\n",
        "#       faces.append(face.astype('float32'))\n",
        "#   faces = np.asarray(faces)\n",
        "#   faces = np.expand_dims(faces, -1)\n",
        "#   emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
        "#   return faces,emotions\n",
        "# trainx,trainy = loaddataset('Train_Data.csv')\n",
        "# testx,testy=loaddataset('Test_Data.csv')\n",
        "# valx,valy=loaddataset('Validation_Data.csv')\n",
        "\n",
        "# trainx1=trainx.astype('float32') / 255\n",
        "# mean, std = np.mean(trainx1), np.std(trainx1)\n",
        "# train_X = np.asarray([(np.array(xi)-mean) for xi in trainx1])\n",
        "\n",
        "\n",
        "# testx1=testx.astype('float32') / 255\n",
        "# mean, std = np.mean(testx1), np.std(testx1)\n",
        "# test_X = np.asarray([(np.array(xi)-mean) for xi in testx1])\n",
        "\n",
        "# valx1=valx.astype('float32') / 255\n",
        "# mean, std = np.mean(valx1), np.std(valx1)\n",
        "# val_X = np.asarray([(np.array(xi)-mean) for xi in valx1])\n",
        "\n",
        "# train_X = train_X.reshape(train_X.shape[0], 48, 48)\n",
        "# train_X = train_X.reshape(train_X.shape[0], 48, 48, 1)\n",
        "# val_X = val_X.reshape(val_X.shape[0],48, 48)\n",
        "# val_X = val_X.reshape(val_X.shape[0],48, 48,1)\n",
        "# test_X = test_X.reshape(test_X.shape[0],48, 48)\n",
        "# test_X = test_X.reshape(test_X.shape[0],48, 48, 1)\n",
        "\n",
        "# train_Y=[np.where(r==1)[0][0] for r in trainy]\n",
        "# val_Y=[np.where(r==1)[0][0] for r in valy]\n",
        "# test_Y=[np.where(r==1)[0][0] for r in testy]\n",
        "\n",
        "\n",
        "vgg_model = load_model('30epochs.h5')\n",
        "ex_model = load_model('traindata_mini_XCEPTION.61-0.63.hdf5')\n",
        "cnn_model=load_model('cnn.h5')\n",
        "members = [vgg_model,ex_model]\n",
        "\n",
        "# update all layers in all models to not be trainable\n",
        "for i in range(len(members)):\n",
        "\tmodel = members[i]\n",
        "\tfor layer in model.layers:\n",
        "\t\t# make not trainable\n",
        "\t\tlayer.trainable = False\n",
        "\t\t# rename to avoid 'unique layer name' issue\n",
        "\t\tlayer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
        "    \n",
        "# define stacked model from multiple member input models\n",
        "def define_stacked_model(members):\n",
        "\t# update all layers in all models to not be trainable\n",
        "\tfor i in range(len(members)):\n",
        "\t\tmodel = members[i]\n",
        "\t\tfor layer in model.layers:\n",
        "\t\t\t# make not trainable\n",
        "\t\t\tlayer.trainable = False\n",
        "\t\t\t# rename to avoid 'unique layer name' issue\n",
        "\t\t\tlayer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
        "\t# define multi-headed input\n",
        "\tensemble_visible = [model.input for model in members]\n",
        "\t# concatenate merge output from each model\n",
        "\tensemble_outputs = [model.output for model in members]\n",
        "\tmerge = concatenate(ensemble_outputs)\n",
        "\thidden = Dense(10, activation='relu')(merge)\n",
        "\toutput = Dense(7, activation='softmax')(hidden)\n",
        "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
        "\t# plot graph of ensemble\n",
        "\tplot_model(model, show_shapes=True, to_file='ensemble.png')\n",
        "\t# compile\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# define ensemble model\n",
        "stacked_model = define_stacked_model(members)\n",
        "\n",
        "# fit a stacked model\n",
        "def fit_stacked_model(model, inputX, inputy):\n",
        "\t# prepare input data\n",
        "\tX = [inputX for _ in range(len(model.input))]\n",
        "\t# encode output data\n",
        "\tinputy_enc = keras.utils.to_categorical(inputy)\n",
        "\t# fit model\n",
        "\tmodel.fit(X, inputy_enc, epochs=30, verbose=0)\n",
        "  \n",
        "# fit stacked model on test dataset\n",
        "fit_stacked_model(stacked_model, test_X, test_Y)\n",
        "\n",
        "\n",
        "# make a prediction with a stacked model\n",
        "def predict_stacked_model(model, inputX):\n",
        "\t# prepare input data\n",
        "\tX = [inputX for _ in range(len(model.input))]\n",
        "\t# make prediction\n",
        "\treturn model.predict(X, verbose=0)\n",
        "\n",
        "# evaluate model on Train set\n",
        "yhat = stacked_prediction(members, model, train_X)\n",
        "yhat = np.argmax(yhat, axis=1)\n",
        "acc = sklearn.metrics.accuracy_score(train_Y, yhat)\n",
        "print('Stacked Train Accuracy: %.3f' % acc)\n",
        "\n",
        "# evaluate model on Validation set\n",
        "yhat = predict_stacked_model(stacked_model, val_X)\n",
        "yhat = np.argmax(yhat, axis=1)\n",
        "acc = sklearn.metrics.accuracy_score(val_Y, yhat)\n",
        "print('Stacked Validation Accuracy: %.3f' % acc)\n",
        "  \n",
        "# evaluate model on Test set\n",
        "yhat = predict_stacked_model(stacked_model, test_X)\n",
        "yhat = np.argmax(yhat, axis=1)\n",
        "acc = sklearn.metrics.accuracy_score(test_Y, yhat)\n",
        "print('Stacked Test Accuracy: %.3f' % acc)\n",
        "\n",
        "model.save('ensembletwo.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-4f81cd978403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# evaluate model on Train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stacked Train Accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-950cf0973278>\u001b[0m in \u001b[0;36mstacked_prediction\u001b[0;34m(members, model, inputX)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mstackedX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstackedX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (28709, 14)"
          ]
        }
      ]
    }
  ]
}