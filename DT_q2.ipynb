{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DT_q2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashankKamath/DecisionTree_ML/blob/master/DT_q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-JGW9s05T3AB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJQB_6vI2rRk",
        "colab_type": "code",
        "outputId": "4ec65fb7-697f-41f9-b152-f5f92f477827",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2834dae4-48a5-4f1b-afe0-9fc873511c13\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2834dae4-48a5-4f1b-afe0-9fc873511c13\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving hw2_question1.csv to hw2_question1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ayli7_LVL9ew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Splitting the data into Train and Test\n"
      ]
    },
    {
      "metadata": {
        "id": "guMvinOPTvlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv(\"hw2_extra.csv\")\n",
        "train_set=data.values.tolist()\n",
        "header=[\"Temperature\",\"Humidity\",\"Sky_Condition\",\"Rainy\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSillIK0_maA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSFFeB1AHXrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.utils import shuffle\n",
        "# data = pd.read_csv(\"hw2_question1.csv\")\n",
        "# header=[\"Thickness\", \"Cellsize\", \"Cellshape\", \"Adhesion\",\"Ecellsize\",\"Nuclei\",\"Chromatin\",\"Nucleoli\",\"Mitosis\",\"Class\"]\n",
        "# data.columns=header\n",
        "# data=shuffle(data)\n",
        "# data=data.reset_index(drop=True)\n",
        "# #   data = data.sample(frac=1).reset_index(drop=True)\n",
        "# train_data=pd.DataFrame()\n",
        "# test_data=pd.DataFrame()\n",
        "# class_2_count=[295,148]\n",
        "# class_4_count=[159,80]\n",
        "# for i in range(0,data.shape[0]):\n",
        "#     if(data.loc[i,'Class']==2 and class_2_count[0]<=295 and class_2_count[0]>0):\n",
        "#         train_data=train_data.append(data.iloc[i])\n",
        "#         class_2_count[0]-=1\n",
        "#     if(data.loc[i,'Class']==4 and class_4_count[0]<=159 and class_4_count[0]>0):\n",
        "#         train_data=train_data.append(data.iloc[i])\n",
        "#         class_4_count[0]-=1\n",
        "#     if(data.loc[i,'Class']==2 and class_2_count[1]<=148 and class_2_count[1]>0):\n",
        "#         test_data=test_data.append(data.iloc[i])\n",
        "#         class_2_count[1]-=1\n",
        "#     if(data.loc[i,'Class']==4 and class_4_count[1]<=80 and class_4_count[1]>0):\n",
        "#         test_data=test_data.append(data.iloc[i])\n",
        "#         class_4_count[1]-=1\n",
        "# train_data=train_data[header]\n",
        "# test_data=test_data[header]\n",
        "# data_set=data.values.tolist()\n",
        "# train_set=train_data.values.tolist()\n",
        "# test_set=test_data.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlEVPw3t_qbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unique_vals(rows, col):\n",
        "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
        "    return set([row[col] for row in rows])\n",
        "# unique_vals(train_set, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgAaOd0xAEJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def class_counts(rows):\n",
        "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
        "    counts = {}  # a dictionary of label -> count.\n",
        "    for row in rows:\n",
        "        # in our dataset format, the label is always the last column\n",
        "        label = row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label] = 0\n",
        "        counts[label] += 1\n",
        "    return counts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiNMEXJdB-9j",
        "colab_type": "code",
        "outputId": "502223a5-a80a-49ae-b481-300276c62d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "def is_numeric(value):\n",
        "    \"\"\"Test if a value is numeric.\"\"\"\n",
        "    return isinstance(value, int) or isinstance(value, float)\n",
        "is_numeric(7)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "I1Jo7aMYCMut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Question:\n",
        "    \"\"\"A Question is used to partition a dataset.\n",
        "\n",
        "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
        "    'column value' (e.g., Green). The 'match' method is used to compare\n",
        "    the feature value in an example to the feature value stored in the\n",
        "    question. See the demo below.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, column, value):\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "\n",
        "    def match(self, example):\n",
        "        # Compare the feature value in an example to the\n",
        "        # feature value in this question.\n",
        "        val = example[self.column]\n",
        "        if is_numeric(val):\n",
        "            return val >= self.value\n",
        "        else:\n",
        "            return val == self.value\n",
        "\n",
        "    def __repr__(self):\n",
        "        # This is just a helper method to print\n",
        "        # the question in a readable format.\n",
        "        condition = \"==\"\n",
        "        if is_numeric(self.value):\n",
        "            condition = \">=\"\n",
        "        return \"Is %s %s %s?\" % (header[self.column], condition, str(self.value))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERphJAFXCmxE",
        "colab_type": "code",
        "outputId": "8fd7369e-b8f1-4c46-ef59-7b5ddcd1cde5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "q = Question(6,7)\n",
        "example = train_set[0]\n",
        "q.match(example)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ervjNTmtENF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def partition(rows, question):\n",
        "    \"\"\"Partitions a dataset.\n",
        "\n",
        "    For each row in the dataset, check if it matches the question. If\n",
        "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
        "    \"\"\"\n",
        "    true_rows, false_rows = [], []\n",
        "    for row in rows:\n",
        "        if question.match(row):\n",
        "            true_rows.append(row)\n",
        "        else:\n",
        "            false_rows.append(row)\n",
        "    return true_rows, false_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F1chNkv9ESqi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# true_rows, false_rows = partition(train_set, Question(0, 2))\n",
        "# true_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UkITmm5xFZsG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will give all values less than two in the first column"
      ]
    },
    {
      "metadata": {
        "id": "FnL9J7ZKFYnv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gini(rows):\n",
        "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
        "\n",
        "    There are a few different ways to do this, I thought this one was\n",
        "    the most concise. See:\n",
        "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
        "    \"\"\"\n",
        "    counts = class_counts(rows)\n",
        "    impurity = 1\n",
        "    for lbl in counts:\n",
        "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
        "        impurity -= prob_of_lbl**2\n",
        "    return impurity "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvhag6jmaZBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "def entropy(rows):\n",
        "    counts = class_counts(rows)\n",
        "    impurity = 0\n",
        "    for lbl in counts:\n",
        "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
        "        impurity -= prob_of_lbl*math.log2(prob_of_lbl)\n",
        "    return impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNIbuyYtFmeb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def info_gain(left, right, current_uncertainty):\n",
        "    \"\"\"Information Gain.\n",
        "\n",
        "    The uncertainty of the starting node, minus the weighted impurity of\n",
        "    two child nodes.\n",
        "    \"\"\"\n",
        "    p = float(len(left)) / (len(left) + len(right))\n",
        "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPnyHTynFo49",
        "colab_type": "code",
        "outputId": "9e51f57c-7a46-4d64-9cd9-e0c1fcb73bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "current_uncertainty = entropy(train_set)\n",
        "current_uncertainty"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9342646162694885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "htnar3UKF9ER",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_best_split(rows):\n",
        "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
        "    and calculating the information gain.\"\"\"\n",
        "    best_gain = 0  # keep track of the best information gain\n",
        "    best_question = None  # keep train of the feature / value that produced it\n",
        "    current_uncertainty = gini(rows)\n",
        "#     current_uncertainty = entropy(rows)\n",
        "    n_features = len(rows[0]) - 1  # number of columns\n",
        "\n",
        "    for col in range(n_features):  # for each feature\n",
        "\n",
        "        values = set([row[col] for row in rows])  # unique values in the column\n",
        "\n",
        "        for val in values:  # for each value\n",
        "\n",
        "            question = Question(col, val)\n",
        "\n",
        "            # try splitting the dataset\n",
        "            true_rows, false_rows = partition(rows, question)\n",
        "\n",
        "            # Skip this split if it doesn't divide the\n",
        "            # dataset.\n",
        "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate the information gain from this split\n",
        "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
        "\n",
        "            # You actually can use '>' instead of '>=' here\n",
        "            # but I wanted the tree to look a certain way for our\n",
        "            # toy dataset.\n",
        "#             print(\"Question and gain:\",question,gain)\n",
        "            if gain >= best_gain:\n",
        "                best_gain, best_question = gain, question\n",
        "#     print(\"Best Question: \",best_question)\n",
        "#     print(\"Best Gain: \", best_gain)\n",
        "    return best_gain, best_question"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NUkRm9THpkf",
        "colab_type": "code",
        "outputId": "1c11469b-0430-44f0-8351-bbe1534c7b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "best_gain, best_question = find_best_split(train_set)\n",
        "best_question, best_gain"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Is Cellsize >= 4.0?, 0.807353709964154)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "fD0XNKjnH4DZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Leaf:\n",
        "    \"\"\"A Leaf node classifies data.\n",
        "\n",
        "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
        "    it appears in the rows from the training data that reach this leaf.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rows):\n",
        "        self.predictions = class_counts(rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QqVarO3CH5w1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decision_Node:\n",
        "    \"\"\"A Decision Node asks a question.\n",
        "\n",
        "    This holds a reference to the question, and to the two child nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 question,\n",
        "                 true_branch,\n",
        "                 false_branch):\n",
        "        self.question = question\n",
        "        self.true_branch = true_branch\n",
        "        self.false_branch = false_branch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ucq3OZi0MTA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_tree(node, spacing=\"\"):\n",
        "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
        "\n",
        "    # Base case: we've reached a leaf\n",
        "    if isinstance(node, Leaf):\n",
        "        print (spacing + \"Predict\", node.predictions)\n",
        "        return\n",
        "\n",
        "    # Print the question at this node\n",
        "    print (spacing + str(node.question))\n",
        "\n",
        "    # Call this function recursively on the true branch\n",
        "    print (spacing + '--> True:')\n",
        "    print_tree(node.true_branch, spacing + \"  \")\n",
        "\n",
        "    # Call this function recursively on the false branch\n",
        "    print (spacing + '--> False:')\n",
        "    print_tree(node.false_branch, spacing + \"  \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vf8-GdKwFOIl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_tree(rows,max_depth,depth):\n",
        "    \"\"\"Builds the tree.\n",
        "\n",
        "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
        "    for the base case (no further information gain). 3) Prepare for\n",
        "    giant stack traces.\n",
        "    \"\"\"\n",
        "\n",
        "    # Try partitioing the dataset on each of the unique attribute,\n",
        "    # calculate the information gain,\n",
        "    # and return the question that produces the highest gain.\n",
        "#     print(\"Depth: \",depth)\n",
        "    gain, question = find_best_split(rows)\n",
        "  \n",
        "    # Base case: no further info gain\n",
        "    # Since we can ask no further questions,\n",
        "    # we'll return a leaf.\n",
        "    if gain == 0 or gain<0. or depth>=max_depth:\n",
        "#         print(\"Created Leaf Node\")\n",
        "        depth=depth-1\n",
        "        return Leaf(rows)\n",
        "    if depth<max_depth:\n",
        "      true_rows, false_rows = partition(rows, question)\n",
        "#       print(\"Creating True Branch\")\n",
        "      true_branch = build_tree(true_rows,max_depth,depth+1)\n",
        "\n",
        "#       print(\"Creating False Branch\")\n",
        "      false_branch = build_tree(false_rows,max_depth,depth+1)\n",
        "\n",
        "      # Return a Question node.\n",
        "      # This records the best feature / value to ask at this point,\n",
        "      # as well as the branches to follow\n",
        "      # dependingo on the answer.\n",
        "      depth=depth-1\n",
        "      return Decision_Node(question, true_branch, false_branch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0WnIVJkB06a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Question and gain: Is Temperature == hot? 0.03124999999999989\n",
        "Question and gain: Is Temperature == cool? 0.031249999999999917\n",
        "Question and gain: Is Humidity == low? 0.031249999999999917\n",
        "Question and gain: Is Humidity == high? 0.03124999999999989\n",
        "Question and gain: Is Sky_Condition == clear? 0.06124999999999997\n",
        "Question and gain: Is Sky_Condition == cloudy? 0.06124999999999997\n",
        "Best Question:  Is Sky_Condition == cloudy?\n",
        "Best Gain:  0.06124999999999997\n",
        "Creating True Branch\n",
        "Question and gain: Is Temperature == hot? 0.03125\n",
        "Question and gain: Is Temperature == cool? 0.03125\n",
        "Question and gain: Is Humidity == low? 0.06125000000000011\n",
        "Question and gain: Is Humidity == high? 0.06125000000000011\n",
        "Best Question:  Is Humidity == high?\n",
        "Best Gain:  0.06125000000000011\n",
        "Creating True Branch\n",
        "Question and gain: Is Temperature == hot? 0.01999999999999985\n",
        "Question and gain: Is Temperature == cool? 0.01999999999999985\n",
        "Best Question:  Is Temperature == cool?\n",
        "Best Gain:  0.01999999999999985\n",
        "Creating True Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Question and gain: Is Temperature == hot? 0.0449999999999999\n",
        "Question and gain: Is Temperature == cool? 0.04499999999999993\n",
        "Best Question:  Is Temperature == cool?\n",
        "Best Gain:  0.04499999999999993\n",
        "Creating True Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Question and gain: Is Temperature == hot? 0.03124999999999989\n",
        "Question and gain: Is Temperature == cool? 0.03124999999999989\n",
        "Question and gain: Is Humidity == low? 0.01125000000000001\n",
        "Question and gain: Is Humidity == high? 0.01125000000000001\n",
        "Best Question:  Is Temperature == cool?\n",
        "Best Gain:  0.03124999999999989\n",
        "Creating True Branch\n",
        "Question and gain: Is Humidity == low? 0.0050000000000002265\n",
        "Question and gain: Is Humidity == high? 0.0050000000000002265\n",
        "Best Question:  Is Humidity == high?\n",
        "Best Gain:  0.0050000000000002265\n",
        "Creating True Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Question and gain: Is Humidity == low? 0.019999999999999907\n",
        "Question and gain: Is Humidity == high? 0.019999999999999934\n",
        "Best Question:  Is Humidity == high?\n",
        "Best Gain:  0.019999999999999934\n",
        "Creating True Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Creating False Branch\n",
        "Best Question:  None\n",
        "Best Gain:  0\n",
        "Created Leaf Node\n",
        "Is Sky_Condition == cloudy?\n",
        "--> True:\n",
        "  Is Humidity == high?\n",
        "  --> True:\n",
        "    Is Temperature == cool?\n",
        "    --> True:\n",
        "      Predict {1: 7, 0: 3}\n",
        "    --> False:\n",
        "      Predict {1: 9, 0: 1}\n",
        "  --> False:\n",
        "    Is Temperature == cool?\n",
        "    --> True:\n",
        "      Predict {1: 3, 0: 7}\n",
        "    --> False:\n",
        "      Predict {1: 6, 0: 4}\n",
        "--> False:\n",
        "  Is Temperature == cool?\n",
        "  --> True:\n",
        "    Is Humidity == high?\n",
        "    --> True:\n",
        "      Predict {1: 2, 0: 8}\n",
        "    --> False:\n",
        "      Predict {1: 1, 0: 9}\n",
        "  --> False:\n",
        "    Is Humidity == high?\n",
        "    --> True:\n",
        "      Predict {1: 5, 0: 5}\n",
        "    --> False:\n",
        "      Predict {1: 3, 0: 7}"
      ]
    },
    {
      "metadata": {
        "id": "ppdZanNYI-bR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Predicting the Class"
      ]
    },
    {
      "metadata": {
        "id": "IZNRIv33I91h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def classify(row, node):\n",
        "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
        "\n",
        "    # Base case: we've reached a leaf\n",
        "    if isinstance(node, Leaf):\n",
        "        return node.predictions\n",
        "\n",
        "    # Decide whether to follow the true-branch or the false-branch.\n",
        "    # Compare the feature / value stored in the node,\n",
        "    # to the example we're considering.\n",
        "    if node.question.match(row):\n",
        "        return classify(row, node.true_branch)\n",
        "    else:\n",
        "        return classify(row, node.false_branch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KIZBuPTJQb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def print_leaf(counts):\n",
        "#     \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
        "#     total = sum(counts.values()) * 1.0\n",
        "#     probs = {}\n",
        "#     for lbl in counts.keys():\n",
        "#         probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
        "#     return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1WDeXodcJY84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print_leaf(classify(train_set[], my_tree))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RpFSffSUVXSm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "def test_correct(counts):\n",
        "    total = sum(counts.values()) * 1.0\n",
        "    probs = {}\n",
        "    \n",
        "    for lbl in counts.keys():\n",
        "        probs[lbl] = int(counts[lbl] / total * 100)\n",
        "    predicted_class=max(probs.items(), key=operator.itemgetter(1))[0] \n",
        "    return predicted_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q2-q34TAGWLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "def data_generate(data):\n",
        "  data=shuffle(data)\n",
        "  data=data.reset_index(drop=True)\n",
        "  #   data = data.sample(frac=1).reset_index(drop=True)\n",
        "  train_data=pd.DataFrame()\n",
        "  test_data=pd.DataFrame()\n",
        "  class_2_count=[295,148]\n",
        "  class_4_count=[159,80]\n",
        "  for i in range(0,data.shape[0]):\n",
        "      if(data.loc[i,'Class']==2 and class_2_count[0]<=295 and class_2_count[0]>0):\n",
        "          train_data=train_data.append(data.iloc[i])\n",
        "          class_2_count[0]-=1\n",
        "      if(data.loc[i,'Class']==4 and class_4_count[0]<=159 and class_4_count[0]>0):\n",
        "          train_data=train_data.append(data.iloc[i])\n",
        "          class_4_count[0]-=1\n",
        "      if(data.loc[i,'Class']==2 and class_2_count[1]<=148 and class_2_count[1]>0):\n",
        "          test_data=test_data.append(data.iloc[i])\n",
        "          class_2_count[1]-=1\n",
        "      if(data.loc[i,'Class']==4 and class_4_count[1]<=80 and class_4_count[1]>0):\n",
        "          test_data=test_data.append(data.iloc[i])\n",
        "          class_4_count[1]-=1\n",
        "  train_data=train_data[header]\n",
        "  test_data=test_data[header]\n",
        "  data_set=data.values.tolist()\n",
        "  train_set=train_data.values.tolist()\n",
        "  test_set=test_data.values.tolist()\n",
        "  return data_set,train_set,test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jGDOsPXJepz",
        "colab_type": "code",
        "outputId": "777c3b5a-c8ed-4dba-c5ce-5a886db561d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from random import seed\n",
        "accuracy=[]\n",
        "max_depth=5\n",
        "num_of_simulations=5\n",
        "data = pd.read_csv(\"hw2_question1.csv\")\n",
        "header=[\"Thickness\", \"Cellsize\", \"Cellshape\", \"Adhesion\",\"Ecellsize\",\"Nuclei\",\"Chromatin\",\"Nucleoli\",\"Mitosis\",\"Class\"]\n",
        "data.columns=header\n",
        "gain_threshold=list(np.arange(0.01,1,0.001))\n",
        "for i in range(0,num_of_simulations):\n",
        "  accuracy.append([])\n",
        "  seed(i)\n",
        "  data_set,train_set,test_set=data_generate(data)\n",
        "  for depth in range(0,max_depth):\n",
        "    my_tree=build_tree(train_set,depth,0)\n",
        "    # print_tree(my_tree)\n",
        "    accuracy[i].append(0)\n",
        "    for row in test_set:\n",
        "      if row[-1]==test_correct(classify(row, my_tree)):\n",
        "         accuracy[i][depth]+=1\n",
        "    accuracy[i][depth]=(accuracy[i][depth]/len(test_set))*100\n",
        "# print(\"Accuracy: \",accuracy)\n",
        "\n",
        "final_accuracy=[]\n",
        "for depth in range(max_depth):\n",
        "  score=0\n",
        "  for i in range(num_of_simulations):\n",
        "    score+=accuracy[i][depth]\n",
        "  score=score/num_of_simulations\n",
        "  final_accuracy.append(score)\n",
        "  \n",
        "print(\"Final Accuracy:\",final_accuracy)\n",
        "print_tree(my_tree)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy: [64.91228070175438, 92.36842105263159, 93.6842105263158, 95.52631578947368, 95.87719298245615]\n",
            "Is Nuclei >= 4.0?\n",
            "--> True:\n",
            "  Predict {4.0: 139, 2.0: 12}\n",
            "--> False:\n",
            "  Is Cellsize >= 4.0?\n",
            "  --> True:\n",
            "    Is Cellsize >= 5.0?\n",
            "    --> True:\n",
            "      Predict {4.0: 12}\n",
            "    --> False:\n",
            "      Is Nuclei >= 3.0?\n",
            "      --> True:\n",
            "        Predict {2.0: 2}\n",
            "      --> False:\n",
            "        Predict {2.0: 1, 4.0: 3}\n",
            "  --> False:\n",
            "    Predict {2.0: 280, 4.0: 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q1zG6uXIJfaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test Set \n",
        "\n",
        "\n",
        "**Entropy**\n",
        "\n",
        "\n",
        "num_of_sim =10\n",
        "Final Accuracy: [64.9122807017544, 92.4561403508772, 95.48245614035088, 97.19298245614034, 98.24561403508771, 98.85964912280701, 99.56140350877195, 99.64912280701756, 99.91228070175438, 100.0]\n",
        "\n",
        "num_of_sim =20\n",
        "Final Accuracy: [64.91228070175438, 93.70614035087719, 95.8991228070175, 97.23684210526315, 98.02631578947371, 98.81578947368423, 99.3421052631579, 99.69298245614036, 99.9342105263158, 99.95614035087719]\n",
        "\n",
        "num_of_sim =30\n",
        "Final Accuracy: [64.91228070175436, 92.6608187134503, 95.14619883040938, 97.10526315789475, 97.8508771929825, 98.65497076023395, 99.3421052631579, 99.67836257309942, 99.89766081871345, 99.97076023391813]\n",
        "\n",
        "num_of_sim =40\n",
        "Final Accuracy: [64.91228070175434, 93.08114035087718, 95.48245614035093, 96.88596491228077, 97.92763157894741, 98.67324561403511, 99.37500000000003, 99.79166666666667, 99.9342105263158, 99.9780701754386]\n",
        "\n",
        "num_of_sim =50\n",
        "Final Accuracy: [64.91228070175434, 93.41228070175441, 95.82456140350884, 97.27192982456145, 98.03508771929828, 98.85964912280704, 99.33333333333336, 99.640350877193, 99.92105263157895, 99.98245614035088]\n",
        "\n",
        "\n",
        "\n",
        "**Gini**\n",
        "\n",
        "\n",
        "num_of_sim =10\n",
        "Final Accuracy: [64.9122807017544, 93.0263157894737, 95.39473684210527, 97.23684210526315, 98.15789473684211, 98.99122807017544, 99.60526315789474, 99.82456140350878, 99.91228070175438, 99.95614035087719]\n",
        "\n",
        "num_of_sim =20\n",
        "Final Accuracy: [64.91228070175438, 92.89473684210529, 95.7456140350877, 97.19298245614034, 98.15789473684214, 99.01315789473686, 99.51754385964914, 99.75877192982458, 99.95614035087719, 100.0]\n",
        "\n",
        "num_of_sim =30\n",
        "Final Accuracy: [64.91228070175436, 93.18713450292398, 95.40935672514621, 96.827485380117, 97.86549707602343, 98.65497076023395, 99.32748538011697, 99.64912280701756, 99.83918128654972, 99.95614035087719]\n",
        "\n",
        "num_of_sim =40\n",
        "Final Accuracy: [64.91228070175434, 93.28947368421049, 95.88815789473689, 97.30263157894743, 98.25657894736847, 98.92543859649126, 99.41885964912282, 99.81359649122808, 99.91228070175438, 99.95614035087719]\n",
        "\n",
        "num_of_sim =50\n",
        "Final Accuracy: [64.91228070175434, 92.90350877192981, 95.6491228070176, 97.23684210526322, 98.14035087719303, 98.74561403508777, 99.33333333333336, 99.7280701754386, 99.87719298245615, 99.94736842105263]\n",
        "\n",
        "\n",
        "Train Set \n",
        "\n",
        "\n",
        "Entropy\n",
        "\n",
        "\n",
        "num_of_sim =5\n",
        "Final Accuracy: [64.97797356828194, 92.81938325991189, 94.84581497797356, 97.04845814977973, 98.06167400881057, 98.81057268722466, 99.33920704845816, 99.77973568281936, 99.91189427312774, 100.0]\n",
        "\n",
        "\n",
        "num_of_sim =10\n",
        "Final Accuracy: [64.97797356828195, 92.75330396475769, 95.30837004405285, 96.93832599118943, 98.03964757709251, 98.76651982378854, 99.38325991189427, 99.80176211453742, 99.9339207048458, 99.97797356828194]\n",
        "\n",
        "num_of_sim =20\n",
        "Final Accuracy: [64.97797356828194, 93.32599118942733, 95.67180616740089, 97.11453744493393, 97.95154185022028, 98.77753303964758, 99.35022026431716, 99.73568281938324, 99.90088105726872, 99.9669603524229]\n",
        "\n",
        "\n",
        "num_of_sim =30\n",
        "Final Accuracy: [64.97797356828191, 93.04698972099852, 95.50660792951541, 97.1145374449339, 98.0029368575624, 98.7885462555066, 99.28046989720998, 99.66226138032307, 99.8898678414097, 99.95594713656389]\n",
        "\n",
        "\n",
        "num_of_sim =40\n",
        "Final Accuracy: [64.9779735682819, 93.36453744493393, 95.82048458149778, 97.20264317180617, 98.08370044052862, 98.77202643171806, 99.3502202643172, 99.71365638766524, 99.9229074889868, 99.97246696035242]\n",
        "\n",
        "num_of_sim =50\n",
        "Final Accuracy: [64.9779735682819, 93.18061674008811, 95.66079295154188, 97.05726872246694, 97.94713656387667, 98.73568281938327, 99.28634361233485, 99.70044052863439, 99.87224669603528, 99.94713656387667]\n",
        "\n",
        "Gini\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_of_sim =10\n",
        "Final Accuracy: [64.97797356828195, 93.45814977973569, 95.83700440528635, 96.80616740088105, 98.01762114537443, 98.8986784140969, 99.51541850220264, 99.75770925110132, 99.97797356828194, 100.0]\n",
        "\n",
        "num_of_sim =20\n",
        "Final Accuracy: [64.97797356828194, 93.18281938325994, 95.7819383259912, 96.97136563876651, 98.20484581497799, 98.89867841409693, 99.46035242290749, 99.7907488986784, 99.91189427312774, 100.0]\n",
        "\n",
        "num_of_sim =30\n",
        "Final Accuracy: [64.97797356828191, 93.10572687224672, 95.56534508076359, 97.09251101321588, 98.00293685756239, 98.80323054331863, 99.40528634361233, 99.79441997063145, 99.95594713656388, 99.99265785609398]\n",
        "\n",
        "num_of_sim =40\n",
        "Final Accuracy: [64.9779735682819, 93.27643171806167, 95.66079295154182, 97.20814977973569, 98.10022026431716, 98.81057268722466, 99.40528634361236, 99.70264317180619, 99.88986784140971, 99.97246696035242]\n",
        "\n",
        "num_of_sim =50\n",
        "Final Accuracy: [64.9779735682819, 93.08810572687226, 95.73568281938327, 96.99559471365639, 97.93832599118942, 98.7444933920705, 99.24669603524234, 99.64317180616743, 99.8546255506608, 99.93832599118944]\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JXTvP1DaHLm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "deb2e46c-2bea-4cf9-b696-fb2388e8b5b8"
      },
      "cell_type": "code",
      "source": [
        "print(\"Dataset Count\",class_counts(data_set))  \n",
        "print(\"Train Count\",class_counts(train_set))\n",
        "print(\"Test Count\",class_counts(test_set))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Count {2: 443, 4: 239}\n",
            "Train Count {2.0: 295, 4.0: 159}\n",
            "Test Count {2.0: 148, 4.0: 80}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}